{
  "hash": "5b6802e8a436b762ed0ac1ae23f62c0b",
  "result": {
    "markdown": "---\ntitle: A Bottom-Up Introduction to Neural Networks\nsubtitle: From 1-D Forward propagation to vectorized Backprop and everything inbetween\nauthor: Diego Mesa\ndate: Feb. 2023\nimage: nn_train.png\ncategories:\n  - neural networks\n  - linear algebra\nformat:\n  html:\n    toc: true\n    toc-title: Contents\n    html-math-method: katex\n    anchor-sections: true\n    smooth-scroll: true\n---\n\n# NN Walkthrough\n\nIn this notebook we will walk through the forward and backward direction of neural networks.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport sklearn\nimport sklearn.datasets\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport seaborn as sns\n\n# for creating animations\nimport matplotlib.animation\nfrom IPython.display import HTML\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# styling additions\nfrom IPython.display import HTML\n# style = \"<style>div.warn{background-color: #fcf2f2;border-color: #dFb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;}</style>\"\nstyle = \"<style>div.info{padding: 15px; border: 1px solid transparent; border-left: 5px solid #dfb5b4; border-color: transparent; margin-bottom: 10px; border-radius: 4px; background-color: #fcf8e3; border-color: #faebcc;}</style>\"\nHTML(style)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<style>div.info{padding: 15px; border: 1px solid transparent; border-left: 5px solid #dfb5b4; border-color: transparent; margin-bottom: 10px; border-radius: 4px; background-color: #fcf8e3; border-color: #faebcc;}</style>\n```\n:::\n:::\n\n\n# Forward Direction\n\nLets start with the simplest network we can imagine, and first understand how a neural network calculates its output for a particular input. This is known as the \"forward\" direction (we will see why later). \n\n## Dataset\n\nLets generate a simple 1-D toy dataset:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef toy():\n    N = 1000\n    inputs = np.linspace(-5,5,N).reshape(N,1)\n    labels = ((inputs>0).astype(int)*2-1).reshape(N,1)\n    return inputs,labels\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ninputs,labels = toy()\ninputs[0:5],labels[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n(array([[-5.        ],\n        [-4.98998999],\n        [-4.97997998],\n        [-4.96996997],\n        [-4.95995996]]),\n array([[-1],\n        [-1],\n        [-1],\n        [-1],\n        [-1]]))\n```\n:::\n:::\n\n\n## 1-D Network\n\nLets give the simplest network we can imagine. One that consists of a few 1-D \"layers\" of size 1! We have an:\n* input node/layer, \n* hidden node/layer:$h_1$, \n* another hidden node/layer $h_2$\n* output node/layer. \n\nNetworks are typically drawn with the **weights** on the wires. Our simple network can be given as:\n![](figures/simple_network.png)\n\n<div class=\"info\">\n\n**Note:** This is sometimes confusing, as technically its the **activations** of the previous layer that \"flow\" into the next layer. However, the weights are what we are tying to **learn**, and their relative **strengths** tells us something about the structure of the network.\n    \n</div>\n\n\nEach node has an associated:\n* weight\n* bias\n* activation function\n\nIn our example, we have $w_1,b_1,w_2,b_2,w_3,b_3$ as our **parameters** and we are using the **sigmoid** as our activation function.\n\nThe function of each node, is to apply its own weight and bias to a previous layers activation value, and then pass it through its activation function to produce its own activation. For example, $h_1$ is doing:\n\n$$\na_1 = \\sigma(w\\cdot x + b)\n$$\n\nThe *input* to a nodes activation function is useful to think about separately, so we can introduce an additional variable to denote it as:\n$$\nz_i = w\\cdot x + b, \\quad\\quad a_1 = \\sigma(z_i)\n$$\n\nSo, we can describe the behavior of each of our nodes as:\n\n![](figures/nodes.png)\n\n### Implementation\n\nNow we can actually implement this simple network. Lets start by recalling the sigmoid function:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nplt.plot(np.linspace(-5,5),sigmoid(np.linspace(-5,5)));\n```\n\n::: {.cell-output .cell-output-display}\n![](nn_files/figure-html/cell-6-output-1.png){width=571 height=411}\n:::\n:::\n\n\nLets define the function of a **node**, being careful to match our notation:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef node(w,b,a):\n    z = w*a + b\n    return sigmoid(z)\n```\n:::\n\n\nNow we are ready to define our **actual network**! Lets initialize our random weights and biases:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nw1,b1,w2,b2,w3,b3 = np.random.rand(6)\n```\n:::\n\n\nNow, lets pick a training example and run it through our network:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ninput,label = inputs[0],labels[0]\n\n# h1 node - operates on our input\na1 = node(w1,b1,input)\n\n# h2 node - operates on a1\na2 = node(w2,b2,a1)\n\n# output node - operates on a2 - produces our output\noutput = node(w3,b3,a2)\ny_hat = output\n```\n:::\n\n\nLets see how we did!\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nprint(f\"for input: {input} with label: {label}, this network calculated: {y_hat}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfor input: [-5.] with label: [-1], this network calculated: [0.74906357]\n```\n:::\n:::\n\n\nAs we know, to actually see how we did, we need to **define a cost**! Lets proceed with the usual average-MSE:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndef mse(y_hat,y,average=True):\n    if average:\n        return np.mean((y_hat-y)**2)\n    else:\n        return ((y_hat-y)**2)\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndef cost_mse(y_hat,y):\n    return 0.5*np.linalg.norm(y-y_hat)**2\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nprint(f\"for input: {input} with label: {label}, this network calculated: {output}, giving us a cost of: {mse(output,label)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfor input: [-5.] with label: [-1], this network calculated: [0.74906357], giving us a cost of: 3.0592233770969504\n```\n:::\n:::\n\n\nOk - so we've demonstrated the entire forward direction, for one sample. Lets define a function for this simple network, so we can run our entire training set through!\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndef simple_network(inputs):\n    outputs = []\n    N = inputs.shape[0]\n    \n    # initialized weights\n    w1,b1,w2,b2,w3,b3 = np.random.rand(6)\n\n    for i in range(N):\n        input = inputs[i,:]\n        \n        # h1 node\n        a1 = node(w1,b1,input)\n\n        # h2 node\n        a2 = node(w2,b2,a1)\n\n        # output node\n        output = node(w3,b3,a2)\n        \n        # append to form output\n        outputs.append(output)\n    \n    return np.asarray(outputs)\n```\n:::\n\n\nWe can now calculate our **average loss** over the entire training set:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nmse(simple_network(inputs),labels)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n1.5262278676793288\n```\n:::\n:::\n\n\n--- \n\n## Multidimensional Network\n\n\nWe've looked at a very simple example above which captures the essence of what we are doing. However, our networks are really never actually composed of layers with one element each. Instead, **each** layer has **multiple** nodes. \n\nTechnically, we could continue in the same way as above and individually number our weights and biases but this quickly gets out of hand! As an exercise, try repeating the above analysis and implementation where each hidden layer is now of size 2!\n\nIronically, to avoid this notational complexity, it seems like we must introduce additional notation, and rewrite our problem in the language of **linear algebra**. \n\nTo introduce this notation, lets imagine a network the following structure:\n![](figures/md-1.png)\n\nTo make our analysis simpler, lets zoom in on node node, and all of its input weights:\n\n![](figures/md-2.png)\n\nWe've highlighted the 0th node in our last hidden layer and all of its inputs from the previous layer in $\\color{orange}{\\text{orange}}$. We've also numbered each of it input nodes with their layer-specific numbering and shown them in $\\color{blue}{\\text{blue}}$.\n\nWe've also named each weight according to the following format:\n\n$$\nw_{\\;\\color{orange}{\\text{current node #}} \\;,\\; \\color{blue}{\\text{incoming node #}}}\n$$\n\nThis may seem a bit counter intuitive at first, as the tendency when reading from left to write is to want to write our weights as:\n\n$$\nw_{\\;\\color{blue}{\\text{incoming node #}} \\;,\\; \\color{orange}{\\text{current node #}} }\n$$\n\nYou absolutely can, but that will result in a bunch of transposes in later equations. To get rid of them now, we will number our weights as we did above. As is typically the case, people make seemingly weird/arbitrary decisions at the front to result in simplifications down the line. \n\nRecall, the function of a node is to apply its weight to the activation of its input/previous layer. In this case, we have three previous nodes/input nodes, so we will also write them in $\\color{blue}{\\text{blue}}$ to make it clear that they are coming from the previous layer. \n\nSo our orange node is performing:\n\n$$\n\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{0}} \\cdot \\color{blue}{a_0} + \n\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{1}} \\cdot \\color{blue}{a_1} + \n\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{2}} \\cdot \\color{blue}{a_2} + \n\\color{orange}{b_0}\n$$\n\nAlready, our eyes should be screaming **dot-product**! \n\n### Notation\n\nIndeed, we can form a **vector** of the <font color='blue'>**previous layer's**</font> activations as:\n\n$$\n\\color{blue}{\\mathbf{a}_{prev}} = \n\\begin{bmatrix}\n\\color{blue}{a_0 \\\\\na_1 \\\\\na_2}\n\\end{bmatrix}\n$$\n\nand a **vector** of the 0-th neurons **weights** in the <font color=\"orange\">**current layer**</font> as:\n\n$$\n\\color{orange}{\\mathbf{w}_0} = \n\\begin{bmatrix}\n\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{0}} \\\\\n\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{1}} \\\\\n\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{2}}\n\\end{bmatrix}\n$$\n\nwhere we have used color again to make it clear what layer we are talking about: the <font color='orange'>current</font> or <font color='blue'>previous</font> layer.\n\nThen we can rewrite what our orange node is calculating as:\n$$\n\\begin{align}\n\\color{orange}{z_0} &= \\color{orange}{\\mathbf{w}_0} \\cdot \\color{blue}{\\mathbf{a}_{prev}} + \\color{orange}{b_0} \\\\\n\\color{orange}{a_0} &= \\sigma(\\color{orange}{z_0})\n\\end{align}\n$$\n\nWell, we've managed to rewrite the activation of **one-node** in slightly better notation. But we can we do better! Lets now reason about the entire layer!\n\nRecall, we already have a vector of the <font color='blue'>**previous layer's** </font>activations in $\\color{blue}{\\mathbf{a}_{prev}}$, although we never actually gave a formula for it. Based on the formula for the <font color='orange'>0th-node</font> in the <font color='orange'>current layer</font> we just gave, lets try to give a **vector** of activations for the entire <font color='orange'>**current layer**</font>. \n\n(Note: to prevent a color explosion, since we're talking about the current layer, I will drop orange to refer to it most places. I will keep blue to refer to the previous layer).\n\nTo simplify our analysis, lets first note that:\n$$\n\\mathbf{a}_{curr} = \n\\begin{bmatrix}\n\\sigma (z_0) \\\\\n\\sigma (z_1) \\\\\n\\sigma (z_2) \\\\\n\\end{bmatrix} = \n\\sigma\\left(\\;\n\\begin{bmatrix}\nz_0 \\\\\nz_1 \\\\\nz_2 \\\\\n\\end{bmatrix}\\;\n\\right) = \n\\sigma (\\mathbf{z}_{curr})\n$$\n\nSo, lets focus on writing a formula for the vector $\\mathbf{z}_{curr}$:\n\n$$\n\\mathbf{z}_{curr} = \n\\begin{bmatrix}\n\\mathbf{w}_0 \\cdot \\color{blue}{\\mathbf{a}_{prev}} + b_0 \\\\\n\\mathbf{w}_1 \\cdot \\color{blue}{\\mathbf{a}_{prev}} + b_1 \\\\\n\\mathbf{w}_2 \\cdot \\color{blue}{\\mathbf{a}_{prev}} + b_2 \\\\\n\\end{bmatrix}\n$$\n\nLets make it a bit clearer by writing our biases for the entire layer as a separate **vector** $\\mathbf{b}$:\n\n$$\n\\mathbf{z}_{curr} = \n\\begin{bmatrix}\n\\mathbf{w}_0 \\cdot \\color{blue}{\\mathbf{a}_{prev}} \\\\\n\\mathbf{w}_1 \\cdot \\color{blue}{\\mathbf{a}_{prev}} \\\\\n\\mathbf{w}_2 \\cdot \\color{blue}{\\mathbf{a}_{prev}}  \\\\\n\\end{bmatrix}\n+\n\\mathbf{b}\n$$\n\nJust like we saw when we discussed linear regression, this vector of dot products is exactly the **matrix-vector** product of the **weight matrix** and the previous layers **activation vector**!\n\n<div class=\"info\">\n\n**Definition**: The current layers **weight matrix**: $\\mathbf{W}$  is a matrix of $k$-many rows, and $j$-many columns, where $k$ is the number of nodes in the current layer, and $j$ is the number of nodes in the previous layer:\n\n$$\n\\mathbf{W} \\in \\mathbb{R}^{k,j}= \n\\begin{bmatrix}\n\\mathbf{w}_0^T \\\\\n\\mathbf{w}_1^T \\\\\n\\ldots \\\\\n\\mathbf{w}_k^T\n\\end{bmatrix} = \n\\begin{bmatrix}\nw_{0,0} & w_{0,1} & \\ldots & w_{0,j} \\\\\n\\ldots \\\\\nw_{k,0} & w_{0,1} & \\ldots & w_{k,j} \\\\\n\\end{bmatrix}\n$$\n\n**Note**: each row of the weight matrix represents all inputs to a specific node in the current layer.\n    \n</div>\n\nNow, we can finally write a complete linear algebraic equation for the function of a <font color=\"orange\">current layer</font> on a <font color=\"blue\">previous layer</font>:\n\n$$\n\\begin{align}\n\\mathbf{z}_{curr} &= \\mathbf{W}\\color{blue}{\\mathbf{a}_{prev}}+\\mathbf{b} \\\\\n\\mathbf{a}_{curr} &= \\sigma(\\mathbf{z}_{curr})\n\\end{align}\n$$\n\nNow, neural networks do this **sequentially**, so the last piece of the puzzle is to be able to refer to a *specific layer* by number. We now introduce the final piece of notation to let us do this: a **superscript** to designate the layer number:\n\n<div class=\"info\">\n\nThe activation of layer $L$ is given by:\n    \n$$\n\\begin{align}\n\\mathbf{z}^L &= \\mathbf{W}^L \\mathbf{a}^{L-1}+\\mathbf{b}^L \\\\\n\\mathbf{a}^L &= \\sigma(\\mathbf{z}^L)\n\\end{align}\n$$\n\nThis is often written succinctly as:\n\n$$\n\\boxed{\\mathbf{a}^L = \\sigma(\\mathbf{W}\\mathbf{a}^{L-1} + \\mathbf{b})}\n$$\n\nwhere the specific $\\mathbf{W},\\mathbf{b}$ we are talking about is implied.\n</div>\n\n<br/>\nWow we've come a long way! We've given a very clear and succinct linear algebraic equation for the entire forward direction for a network of any number of layers and size of each layer!\n\nLets perform a **size sanity check**: \n* $\\mathbf{W}$ is of size $k \\times j$, where $j$ is the number of neurons in the previous layer. \n* $\\mathbf{a}^{L-1}$ is a vector of size $j \\times 1$, the activations of the previous layer. \n* Their multiplication results in a vector of size $k \\times 1$, where $k$ is the number of neurons in the current layer. \n* Our bias vector is also $k \\times 1$ (as we expect!). \n\nSo everything works as expected! \n\nThis is why we decided to write our weight matrix to be of size $k \\times j$ or $\\text{# neurons in prev layer} \\times \\text{# neurons in current layer}$ instead of the other way around. If we had, we'd need a transpose in the equation above.\n\n---\n\n### Implementation\n\nArmed with our new notation, lets write an implementation of the network we gave above:\n\n![](figures/md-1.png)\n\n<div class=\"info\">\n\nüßê**Pause-and-ponder**: What should the sizes of each $W$ be for this network? Lets go through it together! \n    \n</div>\n\nOk! Now lets implement it! \n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ninputs,labels = toy()\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndef simple_network2(inputs):\n    outputs = []\n    N = inputs.shape[0]\n    \n    # initialize weight matrices - notice the dimensions\n    W_1 = np.random.randn(3,1)\n    W_2 = np.random.randn(2,3)\n    W_3 = np.random.randn(1,2)\n    \n    # and our biases\n    b_1 = np.random.randn(3,1)\n    b_2 = np.random.randn(2,1)\n    b_3 = np.random.randn(1,1)\n    \n    # loop through training data\n    for i in range(N):\n        \n        # correct size for current input\n        input = inputs[i,:]\n        \n        # layer 1\n        a_1 = sigmoid(W_1*input + b_1)\n\n        # layer 2\n        a_2 = sigmoid(W_2.dot(a_1)+b_2)\n\n        # output layer\n        output = sigmoid(W_3.dot(a_2)+b_3)\n        \n        # append to form output\n        outputs.append(output)\n    \n    return np.squeeze(np.asarray(outputs)).reshape(-1,1)\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\noutputs = simple_network2(inputs)\nplt.scatter(inputs,outputs,c=labels);\n```\n\n::: {.cell-output .cell-output-display}\n![](nn_files/figure-html/cell-18-output-1.png){width=588 height=411}\n:::\n:::\n\n\n## Mini-Batch Notation\n\nNote, this implementation runs an example through the network **one-by-one**! Instead, we can imagine feeding our entire dataset through **at once**! This formalism will pave the way for us later to feed in a mini-batch at a time.\n\nLets just reason our way through this one from **first-principles** (fancy way to say: **lets match our matrix sizes!**), seeing how we get our entire dataset through the first hidden layer. \n\nThe first weight matrix is of size:\n\n$$\n\\mathbf{W}_1: (\\text{size of hidden layer} \\times \\text{dimension of input})\n$$\n\nwhich in this case is: $3 \\times 1$. If our input was 2-D, it would be $3 \\times 2$. So what we *dot* it with, needs to be of size: $\\text{dimension of input} \\times \\text{??}$. \n\n**Its in that dimension that we can place our entire dataset!**\n\nSo, now we're going to be shuffling **activation matrices** around! In these activation matrices, each **column** is an activation for the last layer on a different training example! \n\n![](figures/activation-matrix.png)\n\nSo we expect the **first activation matrix** to be of size: $\\text{dimension of input} \\times \\text{number of samples}$. This means this must also be the size of the initial input matrix for the first hidden layer. \n\n\nSo, we can rewrite our layer propagation equation above for our entire dataset:\n\n$$\n\\boxed{\\mathbf{A}^L = \\sigma(\\mathbf{W}\\mathbf{A}^{L-1} + \\mathbf{b})}\n$$\n\nwhere we use **broadcasting rules** to let us add a vector $\\mathbf{b}$ to a matrix. \n\n---\n\nLets make a special note about the **first hidden layer**, and how it processes our **input matrix**. \n\nTypically, we imagine our data matrix such that the first dimension is the `batch_size`:\n\n![](figures/X.png)\n\nThis means **each row** of this matrix, corresponds to **one sample.**\n\nSo if our data matrix is of size $\\text{batch_size} \\times \\text{dimension}$, in order for our first layer to calculate correctly, **we have to make the sizes work**! Meaning, our first layer should be: \n\n$$\n\\mathbf{A}^1 = \\sigma(\\mathbf{W}^1\\mathbf{X}^T + \\mathbf{b})\n$$\n\nwhere $X^T$ is:\n\n\n![](figures/X-T.png)\n\n<div class=\"info\">\n    \n**Note:** Here we **define** the input matrix $X$ to be of size: $N \\times d$. That is why we transpose it in the first layer. This is by no means universal, and different numerical libraries do it differently. You might come across libraries or papers to talks, where the input matrix is defined to be of size $d \\times N$. If that is the case, the first layer does **not** need an $X^T$!\n\n</div>\n\nNow we can implement this simple network, using this notation! \n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ndef simple_network2_batch(inputs):\n    # assume inputs is of shape Nxd\n    \n    # initialize weight matrices - notice the dimensions\n    W_1 = np.random.randn(3,1)\n    W_2 = np.random.randn(2,3)\n    W_3 = np.random.randn(1,2)\n    \n    # and our biases\n    b_1 = np.random.randn(3,1)\n    b_2 = np.random.randn(2,1)\n    b_3 = np.random.randn(1,1)\n    \n    # NOTE - there is no for loop here! \n    \n    # layer 1\n    a_1 = sigmoid(W_1.dot(X.T) + b_1)\n\n    # layer 2\n    a_2 = sigmoid(W_2.dot(a_1)+b_2)\n\n    # output layer\n    output = sigmoid(W_3.dot(a_2)+b_3)\n    \n    return np.squeeze(np.asarray(outputs)).reshape(-1,1)\n```\n:::\n\n\nNow lets actually run our **entire dataset** through (since we aren't yet dealing with batches), to generate outputs:\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ninputs,labels = toy()\ny_hat = outputs = simple_network2(inputs)\n```\n:::\n\n\nJust for fun, lets plot it! \n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,7))\nax1.set_title('Original poits and labels')\nax1.scatter(inputs,labels);\n\nax1.set_title('Original poits and Y_Hat')\nax2.scatter(inputs,outputs,color='orange');\n```\n\n::: {.cell-output .cell-output-display}\n![](nn_files/figure-html/cell-21-output-1.png){width=1185 height=579}\n:::\n:::\n\n\nNote the huge difference in our y-axis! This is **without** and training so its bound to be bad! \n\n<div class=\"info\">\n\n**Pause-and-ponder**: Go back and re run the network above and re-plot the final representation. Notice how random it is! This is because we initialize with random weights, but never actually do any training! \n    \n</div>\n\n\n## Dealing with Biases\n\nAnother very common notational trick people do, as we saw in **linear regression**, was to **add \"another dimension\"** to \"automatically\" deal with our **bias**. \n\nHere,  that means adding **another \"node\" to each layer**. This node has **no input, only outputs**, and its **activation is always the value 1**.\n\n![](figures/biases.png)\n\n<div class=\"info\">\n\n**Question**: How does this act as a bias?\n    \n</div>\n\nWell, lets look at what this means for a particular node. Lets once again highlight the orange node:\n\n![](figures/bias-weights.png)\n\nWhat **effect** does this have for this orange nodes update? Well, lets write out what it is:\n\n![](figures/bias-weights-eqn.png)\n\nSo, $\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{3}}$ is **always** being multiplied by the value $1$. This is exactly the role of the bias! \n\n![](figures/bias-weights-eqn-simp.png)\n\nAs we can see, the addition of a constant node in a layer **gives an extra weight to each node in the next layer**. This weight, multiplied by 1, **acts as the bias for each node**. \n\nSo all we have to do, is **add an extra <font color=\"red\">column</font> to each weight matrix** in our network. (Note: often, this bias is omitted from the final layer).\n\nNow, for a layer $L$, the **weight matrix** is of size: $k \\times j+1$, where $k$ is the number of *actual*/*real* hidden nodes in layer $L$, and $j$ is the number of *actual/real* hidden nodes in layer $L-1$. \n\n**Note**: often this is drawn with the bias nodes on top of the others, not below. \n    \n![](figures/bias-on-top.png)\n    \nSo we would think of the 0-th weight acting as the bias, so we would add an extra column to the left/right of each weight matrix. Its ultimately the same thing\n \n\nAs a sanity check that this works, lets compare the output of the previous implementation with this new notation. To do so, we need to make sure they use the same initialization. Lets take it out of the function and \"make it flat\" so we can easily compare:\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n# W and b network\n# initialize weight matrices - notice the dimensions\nW_1 = np.random.randn(3,1)\nW_2 = np.random.randn(2,3)\nW_3 = np.random.randn(1,2)\n\n# and our biases\nb_1 = np.random.randn(3,1)\nb_2 = np.random.randn(2,1)\nb_3 = np.random.randn(1,1)\n```\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n# W network - adding biases to the right:\nW_1_b = np.hstack((W_1,b_1))\nW_2_b = np.hstack((W_2,b_2))\nW_3_b = np.hstack((W_3,b_3))\n```\n:::\n\n\nLets run the \"old network\" notation:\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\n# layer 1\na_1_old = sigmoid(W_1.dot(inputs.T) + b_1)\n\n# layer 2\na_2_old = sigmoid(W_2.dot(a_1_old)+b_2)\n\n# output layer\noutput_old = sigmoid(W_3.dot(a_2_old)+b_3)\noutput_old = np.squeeze(np.asarray(output_old)).reshape(-1,1)\n```\n:::\n\n\nAnd now the \"new\" network notation. Note: in order to run the inputs through, we need to add the \"extra dimension of 1's\", as we've done many times before!\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\ninputs_new = np.c_[inputs,np.ones(inputs.shape[0])]\n```\n:::\n\n\nNow we can go ahead and \"feed it in\"\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n# layer 1\na_1_new = sigmoid(W_1_b.dot(inputs_new.T))\n# append the 1 to the end of the previous layers activations:\na_1_new = np.r_[a_1_new, np.ones((1,a_1_new.shape[1]))]\n\n# layer 2\na_2_new = sigmoid(W_2_b.dot(a_1_new))\n# append the 1 to the end of the previous layers activations:\na_2_new = np.r_[a_2_new, np.ones((1,a_2_new.shape[1]))]\n\n# output layer\noutput_new = sigmoid(W_3_b.dot(a_2_new))\noutput_new = np.squeeze(np.asarray(output_new)).reshape(-1,1)\n```\n:::\n\n\nLets verify they are both equal:\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nnp.equal(output_new,output_old).all()\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\nFalse\n```\n:::\n:::\n\n\n<div class=\"info\">\n\n**Note**: This might seem like **a strange hack** - We have to reshape each layers weight matrix, and each layers activation matrix to account for this extra \"1\" flying around everywhere. \n\nI will not try to convince you one way or the other which makes the most sense. I'm just explaining it here in case it is useful to you to think of the bias as \"being wrapped up\" in our weight matrix, as it was when we discussed linear regression. \n    \nMoving forward, we will **not** be using this notation in the rest of the notebook.\n\n</div>\n\n\n## Activation Functions\n\nLets imagine revisiting the previous network structure under different activation functions:\n\n### Linear\n\nLets start with a **linear activation** function:\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ndef linear(z):\n    return z\n```\n:::\n\n\nThis is almost always called a linear activation, but is better thought of as the identity activation - that is, it just returns what it was given, unaltered.\n\nWhat does this mean for the computation our network can perform? \n\nOnce again, lets flatten our implementation to take a look:\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\ninputs,labels = toy()\n```\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\n# initialize weight matrices - notice the dimensions\nW_1 = np.random.randn(3,1)\nW_2 = np.random.randn(2,3)\nW_3 = np.random.randn(1,2)\n\n# and our biases\nb_1 = np.random.randn(3,1)\nb_2 = np.random.randn(2,1)\nb_3 = np.random.randn(1,1)\n```\n:::\n\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\n# layer 1\na_1 = linear(W_1.dot(inputs.T) + b_1)\n\n# layer 2\na_2 = linear(W_2.dot(a_1)+b_2)\n\n# output layer\noutputs = linear(W_3.dot(a_2)+b_3)\noutputs = np.squeeze(np.asarray(outputs)).reshape(-1,1)\n```\n:::\n\n\nLets plot it:\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,7))\nax1.set_title('Original poits and labels')\nax1.scatter(inputs,labels);\n\nax1.set_title('Original poits and Y_Hat')\nax2.scatter(inputs,outputs,color='orange');\n```\n\n::: {.cell-output .cell-output-display}\n![](nn_files/figure-html/cell-32-output-1.png){width=1185 height=579}\n:::\n:::\n\n\n<div class=\"info\">\n    \nüßê **Pause-and-ponder**: Go back and re-run the above a few times. What does this mean?\n    \n</div>\n\nRemember, what we are visualizing is the inputs and the corresponding output value - what our network has been able to map to.\n\nYou might be noticing that this is strangely linear! Indeed! With linear activation functions, all we are able to do is calculate some linear combination of our inputs! \n\nLets dig in a bit more, and show what we are actually calculating at Layer $L$:\n\n$$\nA^L = W_3((W_2(W_1X^T + b_1) + b_2) + b_3\n$$\n\nWell, lets distribute this out:\n$$\nA^L = W_3W_2W_1X^T + W_3W_2b_1 + W_3b_2 + b_3\n$$\n\nOk, wait a second... this is getting confusing - lets check to make sure the sizes work out! \n\n![](figures/linear-sizes.png)\n\n<div class=\"info\">\n\nLets go through this part together in class!\n\n</div>\n\nOk! We can see that the sizes do in fact work out!\n\n\n<div class=\"info\">\n\nüßê**Pause-and-ponder:** In your resulting equation, perform some clarifying substitutions. What do we discover? \n    \n</div>\n\nAfter some substitutions, we can write an equation like the following:\n\n$$\n\\hat{\\mathbf{Y}} = \\mathbf{W}^*X^T + \\mathbf{b}^*\n$$\n\nWhich tells us that a NN with only linear activations, is ultimately just another linear function of its inputs! It doesn't matter how deep or wide it is! \n\n\n### A Final Sigmoid\n\nGiven our understanding above, what would happen if we only add a sigmoid at the end? Something like:\n\n![](figures/final-sig.png)\n\n<div class=\"info\">\n\nüßê**Pause-and-ponder**: What do you think it represents? What is its **representational capacity**? \n    \n</div>\n\n\n# Backprop\n\nNow that we have a good idea about **forward propagation** we're ready to learn by **backpropagating an error** signal through the network, allowing us to **attribute** \"blame\"/\"error\" to each specific weight/bias in our network. \n\nIn other words, one way to think of what the result of backprop is as a large vector which tells us which parameters are the most responsible for our error - or phrased another way - which ones we should change, and by how much, and in what direction, in order to get the biggest reduction of our error. \n\n## Latex definitions\nThis cell just creates some latex definitions we will need later. \n\n$$\n\\newcommand{\\bx}[1]{\\color{purple}{b^{#1}}}\n\\newcommand{\\bl}{\\color{purple}{b^L}}\n\\newcommand{\\wx}[1]{\\color{blue}{w^{#1}}}\n\\newcommand{\\wl}{\\color{blue}{w^L}}\n\\newcommand{\\wone}{\\color{blue}{w_1}}\n\\newcommand{\\ax}[1]{\\color{green}{a^{#1}}}\n\\newcommand{\\al}{\\color{green}{a^L}}\n\\newcommand{\\zx}[1]{\\color{orange}{z^{#1}}}\n\\newcommand{\\zl}{\\color{orange}{z^L}}\n\\newcommand{\\ap}{\\color{green}{a^{L-1}}}\n\\newcommand{\\czero}{\\color{red}{C_0}}\n\\newcommand{\\dc}{\\color{red}{\\partial C_0}}\n\\newcommand{\\dw}[1]{\\color{blue}{\\partial \\wx{#1}}}\n\\newcommand{\\dz}[1]{\\color{orange}{\\partial \\zx{#1}}}\n\\newcommand{\\da}[1]{\\color{green}{\\partial \\ax{#1}}}\n\\newcommand{\\db}[1]{\\color{purple}{\\partial \\bx{#1}}}\n\\newcommand{\\dap}{\\color{green}{\\partial \\ax{L-1}}}\n\\newcommand{\\dcdw}[1]{\\frac{\\dc}{\\dw{#1}}}\n\\newcommand{\\dcdz}[1]{\\frac{\\dc}{\\dz{#1}}}\n\\newcommand{\\dzdb}[1]{\\frac{\\dz{#1}}{\\db{#1}}}\n\\newcommand{\\dzdw}[1]{\\frac{\\dz{#1}}{\\dw{#1}}}\n\\newcommand{\\dadz}[1]{\\frac{\\da{#1}}{\\dz{#1}}}\n\\newcommand{\\dcda}[1]{\\frac{\\dc}{\\da{#1}}}\n\\newcommand{\\dcdb}[1]{\\frac{\\dc}{\\db{#1}}}\n\\newcommand{\\dcdap}{\\frac{\\dc}{\\dap}}\n\\newcommand{\\deltal}{\\delta^L}\n\\newcommand{\\deltax}[1]{\\delta^{#1}}\n$$\n\n**Note: Make sure you run this cell!**\n\n\n## 1-D Case\n\n**Note**: This presentation follows **very closely** these two **fantastic** resources\n* [3blue1brown - Backpropagation calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n* [Michael Nielsen - Neural Networks and Deep Learning - Chapter 2](http://neuralnetworksanddeeplearning.com/chap2.html)\n\nLets revisit the simplest network we started with at the beginning of this notebook:\n\n![](figures/1d-bp-net.png)\n\n**Notation Note**: we're introducing a new notation $\\color{blue}{w_3 = w^L, w_2 = w^{L-1}}, \\ldots$ and we will be using them interchangeably. \n\nTo focus our discussion, lets just focus on the last two levels, and label their activation values:\n\n![](figures/1d-bp-end.png)\n\nThe output layer defines the \"representation\" our network has created for a specific input: \n\n$$\n\\begin{align}\n\\zl &= \\wl\\ap+\\bl \\\\\n\\al &= \\sigma(\\zl)\n\\end{align}\n$$\n\nWhere now we are using color to denote the *kind* of variable we are talking about. For example, activations for any layer are <font color=\"green\">green</font>.\n\nSo, its with this layers activation that we want to measure our **cost**, on a **specific example** $x_0$ run through our network:\n\n$$\n\\czero = (\\al - y)^2\n$$\n\nAnother way to think about this process, is as this **computational graph**:\n\n![](figures/1d-bp-graph.png)\n\nThis tells a \"causal\" story, about what variables are needed to compute other variables. Note: this could be carried even further back through the network, all the way to the inputs! \n\n<div class=\"info\">\n\n**Note:** This is a \"light weight\"/conceptual **computational graph**. Its a way to introduce the concept of backpropagating partial derivatives through a graph using the chain rule. \n\n</div>\n\nLets try to understand exactly what a \"partial derivative\" like $\\dcdw{L}$ is telling us, by associating **a little number line** with each of these variables:\n\n![](figures/1d-bp-graph-num-line.png)\n\nPictorially, this is telling us that a little nudge to a weight, results in a nudge to the \"activity\"/$\\zl$ of the neuron (Q: how big/how small of a nudge?), which then results in a nudge to the activation/$\\al$ of the neuron (Q: how big/how small of a nudge?) which then results in a nudge to the total cost of the network (how big/how small of a nudge?).\n\nSo conceptually, the partial $\\dcdw{L}$ is capturing:\n\n![](figures/1d-bp-dcdw.png)\n\n### Simulation\nWe actually go ahead and simulate this to better understand what its telling us! Lets start by running an example through our network:\n\n::: {.cell tags='[]' execution_count=32}\n``` {.python .cell-code}\ninputs,outputs = toy()\ninput,output = inputs[0],outputs[0]\n```\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\n# initialize weights\nw_1 = np.random.randn()\nw_2 = np.random.randn()\nw_3 = 1 #exagerating for plotting purposes\n\n# and our biases\nb_1 = np.random.randn()\nb_2 = np.random.randn()\nb_3 = np.random.randn()\n\n\n# layer 1\nz_1 = w_1*input+b_1\na_1 = sigmoid(z_1)\n\n# layer 2\nz_2 = w_2*a_1+b_2\na_2 = sigmoid(z_2)\n\n# output layer\nz_3 = w_3*a_2+b_3\na_3 = sigmoid(z_3)\n```\n:::\n\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\ncost_0 = (output - a_3)**2\n```\n:::\n\n\n### Nudge $\\color{blue}{w^L}$\n\nNow lets give $\\wl$ a \"little nudge\", and see how it propagates through the network! \n\n**Note**: This is not really a little nudge. Ive greatly exaggerated it to provide a good looking plot.\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nnudge = np.linspace(-5,5,51)\nz_3_nudged = (w_3 + nudge)*a_2 + b_3\na_3_nudged = sigmoid(z_3_nudged)\ncost_0_nudged = (a_3_nudged - output)**2\n```\n:::\n\n\nLets plot it!\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nfig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,7),sharex=True)\nax1.plot(w_3+nudge,z_3_nudged,c=\"orange\");\nax1.scatter(w_3,w_3*a_2+b_3,s=100,c='orange');\nax1.set_xlabel('w_3 + nudge');\nax1.set_ylabel('z_3');\nax1.set_title(\"How a nudge in w_3 affects z_3\")\n\nax2.plot(w_3+nudge, a_3_nudged,c='green');\nax2.scatter(w_3,sigmoid(w_3*a_2+b_3),s=100,c='green');\nax2.set_xlabel('w_3 + nudge');\nax2.set_ylabel('a_3');\nax2.set_title(\"How a nudge in a_3 affects z_3\")\n\nax3.plot(w_3+nudge, cost_0_nudged,c='red');\nax3.scatter(w_3,(sigmoid(w_3*a_2+b_3)-output)**2,s=100,c='red');\nax3.set_xlabel('w_3 + nudge');\nax3.set_ylabel('C_0');\nax3.set_title(\"How a nudge in w_3 affects C_0\");\n```\n\n::: {.cell-output .cell-output-display}\n![](nn_files/figure-html/cell-37-output-1.png){width=1567 height=597}\n:::\n:::\n\n\nWhat is this telling us? That a little nudge in $w_3$ results in very different changes to each the variables down our computational graph! \n\n### Nudge $\\color{blue}{w_1}$\n\nLets repeat this simulation, except go **more back/backer** to see how a nudge at $w_1$ affects our output layer and our cost!\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nnudge = np.linspace(-5,5,51)\nz_1_nudged = (w_1 + nudge)*input + b_1\na_1_nudged = sigmoid(z_1_nudged)\n\nz_2_nudged = w_2*a_1_nudged + b_2\na_2_nudged = sigmoid(z_2_nudged)\n\nz_3_nudged = w_3*a_2_nudged + b_3\na_3_nudged = sigmoid(z_3_nudged)\n\ncost_0_nudged = (a_3_nudged - output)**2\n```\n:::\n\n\nPlot it!\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nfig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,7),sharex=True)\n\nax1.plot(w_1+nudge,z_1_nudged,c=\"orange\");\nax1.scatter(w_1,w_1*input+b_1,s=100,c='orange');\nax1.set_xlabel('W+nudge');\nax1.set_ylabel('z_1');\nax1.set_title(\"How a nudge in w_1 affects z_1\")\n\nax2.plot(w_1+nudge, a_3_nudged,c='green');\nax2.scatter(w_1,sigmoid(w_3*(sigmoid(w_2*sigmoid(w_1*input+b_1)+b_2))+b_3),s=100,c='green');\nax2.set_xlabel('W+nudge');\nax2.set_ylabel('a_3');\nax2.set_title(\"How a nudge in w_1 affects a_3\")\n\nax3.plot(w_1+nudge, cost_0_nudged,c='red');\nax3.scatter(w_1,(sigmoid(w_3*(sigmoid(w_2*sigmoid(w_1*input+b_1)+b_2))+b_3)-output)**2,s=100,c='red');\nax3.set_xlabel('W+nudge');\nax3.set_ylabel('C_0');\nax3.set_title(\"How a nudge in w_1 affects our cost\");\n```\n\n::: {.cell-output .cell-output-display}\n![](nn_files/figure-html/cell-39-output-1.png){width=1563 height=597}\n:::\n:::\n\n\nAh, these graphs have are a bit more interesting, so we can point out the following:\n\n**Implicit** in all of these graphs are the following idea:\n\n$$\n\\frac{\\text{the amount of change in a dependent variable}}{\\text{the amount of change in a variable that it depends on}}\n$$\n\nWell, this is just the **rate of change** of the graph! Which we might remember! Lets restate this idea for each subplot:\n\nThe specific rate of change for the first subplot is:\n\n$$\n\\frac{\\text{resulting change in }\\color{orange}{z_1}}{\\text{some amount of change in } \\color{blue}{w_1}} = \\frac{\\color{orange}{\\Delta z_1}}{\\color{blue}{\\Delta w_1}}\n$$\n\nThe specific rate of change for the second subplot is:\n\n$$\n\\frac{\\text{resulting change in }\\color{green}{a_3}}{\\text{some amount of change in } \\color{blue}{w_1}} = \\frac{\\color{green}{\\Delta a_3}}{\\color{blue}{\\Delta w_1}}\n$$\n\n\nThe specific rate of change for the third subplot is:\n\n$$\n\\frac{\\text{resulting change in } \\color{red}{C_0}}{\\text{some amount of change in } \\color{blue}{w_1}} = \\frac{\\color{red}{\\Delta C_0}}{\\color{blue}{\\Delta w_1}}\n$$\n\n\nAha! That last subplot is telling us something about how **sensitive** the cost $\\czero$ is to changes in $\\wone$. This is what we want! \n\nWe can see that a little change to $\\wone$ to the left/right (depends on random vals), results in a big change to our final cost on this example! \n\nThis **rate of change**/**derivative** tells us the direction we need to change $\\wone$ in order to reduce our costs!\n\n---\n\n### We could do better\n\nNow, from the plotting code for the last subplot up there you might notice that to generate that last subplot, we basically had to run the entire nudged $\\wone + \\Delta$ all the way through our entire network!\n\nWe are also reasoning about the derivative by looking at a graph, instead of actually calculating it so that we can use it. \n\nLets see if we can think of a clever scheme of **actually calculating** our partials with respect to our weights: $\\ldots,\\dcdw{L-1},\\dcdw{L}$, by starting with \n\n$$\n\\dcdw{L} = ?\n$$\n\nLets **bring back our computational graph**:\n\n![](figures/1d-bp-graph.png)\n\nLets use the following idea to **decompose our graph** starting from the end/bottom:\n\n$$\n\\frac{\\text{the amount of change in a dependent variable}}{\\text{the amount of change in a variable that it depends on}}\n$$\n\n---\n\nStarting at $\\czero$, we have:\n\n$$\n\\dcda{L}\n$$\n\nThis object tells us how the cost changes with respect to the final layers activation. Lets calculate it:\n\n$$\n\\begin{align}\n\\czero &= (\\al - y)^2 \\\\\n\\dcda{L}  &= 2(\\al -y)\n\\end{align}\n$$\n\nWhat happens if $\\al$ is small? Well then it contributes less to our error! \n\nSo we should focus on making sure that when $\\al$ is large, its correct!\n\n\n<div class=\"info\">\n\nüßê**Pause-and-ponder**: What about $\\frac{\\czero}{\\partial y}$ ? What can we say about this?\n    \n</div>\n\n---\n\nAccording to our computational graph, thats all of the **immediate** \"independent\" variables $\\czero$ has. So lets now switch our focus to $\\al$:\n\n$$\n\\begin{align}\n\\al &= \\sigma (\\zl) \\\\\n\\dadz{L} &= \\sigma ^{\\color{red}{\\prime}}(\\zl)\n\\end{align}\n$$\n\nwhere $\\sigma^{\\color{red}{\\prime}}(\\cdot)$ is the *derivative* of our sigmoid activation function:\n\n$$\n\\sigma^{\\color{red}{\\prime}}(\\cdot) = \\sigma(\\cdot)(1-\\sigma(\\cdot))\n$$\n\nThis lets us rewrite the above as:\n\n$$\n\\begin{align}\n\\dadz{L} &= \\sigma(\\zl)(1-\\sigma(\\zl))\n\\end{align}\n$$\n\n---\n\nWe now have the following partials:\n\n$$\n\\begin{align}\n\\dcda{L} &: \\text{how changing } \\al \\text{ changes } \\czero \\\\ \\\\\n\\dadz{L} &: \\text{how changing } \\zl \\text{ changes } \\al\n\\end{align}\n$$\n\nIt shouldn't require too much convincing that we can **multiply** these two objects together to create a new partial:\n\n$$\n\\begin{align}\n\\dcda{L} \\cdot \\dadz{L} &= \\dcdz{L} \\\\\n                        &= \\text{how changing } \\zl \\text{ changes } \\czero\n\\end{align}\n$$\n\nIndeed, the notation itself suggests this by allowing us to \"cancel out\" partials that appear on top and on bottom:\n\n\n<div style=\"text-align:center\">\n<img src=\"figures/dcdz.png\">\n</div>\n\nThe last step is to actually write this out to get an expression for $\\dcdz{L}$:\n\n$$\n\\begin{align}\n\\dcdz{L} &= \\dadz{L} \\cdot \\dcda{L} \\\\\n         &= \\underbrace{\\sigma(\\zl)(1-\\sigma(\\zl))}_{\\text{how changing }\\zl\\text{ affects }\\al} \\cdot  \\underbrace{2(\\al -y)}_{\\text{how changing }\\al\\text{ affects }\\czero}\n\\end{align}\n$$\n\n<div class=\"info\">\n\nüßê**Pause-and-ponder**: We have just discovered and applied the **chain rule**! We have used it to **backpropagate** a change at the output of our last layer: $\\dcda{L}$, to a change at the input of the activation function of our last layer: $\\dadz{L}$.\n\n</div>\n\n<div class=\"info\">\n    \nüìñ**Semi-Definition**: **Backpropagation** can be thought of as applying the chain rule **back** across our computational graph!\n    \n</div>\n\n\n---\n\n### Lets keep going!\n\nSo far we've backpropagated once. Lets look at our map so far:\n\n![](figures/1d-bp-graph-current.png)\n\nLets keep **backpropagating** (chain-ruling)! We can now look at the inputs $\\zl$ has. Lets focus on one of the most interesting for right now, $\\wl$:\n\n$$\n\\begin{align}\n\\zl &= \\wl\\ax{L-1}+\\bl \\\\\n\\dzdw{L} &= \\ax{L-1}\n\\end{align}\n$$\n\n<div class=\"info\">\n\nüßê**Pause-and-ponder**: This derivative has a particularly interesting interpretation. Its saying that the amount a nudge to our weight in the last layer influences the \"activity\" of the last layer, depends on how strong the previous neuron was firing! In other words, if the previous neuron was very active, even a small nudge could cause big changes! But if the previous neuron was always low activation/low firing, then this weight doesn't really matter!\n    \n**Note:** This observation is often stated as:\n    \n* \"Neurons that fire together, wire together\",\n* \"Weights between low activation neurons learn very slowly\"\n\n</div>\n\nWith this, we can **chain it** together with our other partials to write an expression for $\\dcdw{L}$:\n\n$$\n\\begin{align}\n\\dcdw{L} &= \\dzdw{L}\\dcdz{L}\\\\ \\\\\n         &= \\dzdw{L}\\dadz{L}\\dcda{L} \\\\ \\\\\n         &= \\underbrace{\\ax{L-1}}_{\\text{how changing }\\wl\\text{ affects } \\zl} \\quad \\underbrace{\\sigma(\\zl)(1-\\sigma(\\zl))}_{\\text{how changing }\\zl\\text{ affects }\\al} \\quad \\underbrace{2(\\al -y)}_{\\text{how changing }\\al\\text{ affects }\\czero}\n\\end{align}\n$$\n\nOr simply:\n\n$$\n\\begin{align}\n\\dcdw{L} &= \\dzdw{L}\\dadz{L}\\dcda{L} \\\\ \n         &= \\left(\\ax{L-1}\\right) \\left(\\sigma(\\zl)(1-\\sigma(\\zl))\\right) \\left(2(\\al -y)\\right)\n\\end{align}\n$$\n\n<div class=\"info\">\n\nüßê**Pause-and-ponder**: From here, can you guess the appropriate equation for $\\dcdb{L}$? \n    \n**Hint**:\n    \n$$\n\\frac{\\dc}{?} = \\frac{\\dz{L}}{?}\\dadz{L}\\dcda{L}\n$$\n    \n</div>\n\n---\n\nWow! We've come a long way! We finally have an actual expression for how a change to **one of our weights** ($\\wl$) causes a change to our cost **for a single example**.\n\n![](figures/1d-bp-graph-wl.png)\n\nLets finish up by using the **hint** I gave above \n    \n$$\n\\frac{\\dc}{?} = \\frac{\\dz{L}}{?}\\dadz{L}\\dcda{L}\n$$\n\nto find an expression for how the cost changes with respect to the **previous layers activity** $\\dcda{L-1}$:\n\n$$\n\\dcda{L-1} = \\underbrace{\\frac{\\dz{L}}{\\da{L-1}}}_{\\text{need to calculate}}\\underbrace{\\dadz{L}\\dcda{L}}_{\\text{already have this}}\n$$\n\nSo all we need is an expression for $\\frac{\\dz{L}}{\\da{L-1}}$:\n\n$$\n\\begin{align}\n\\zl &= \\wl\\ax{L-1}+\\bl \\\\ \n\\frac{\\dz{L}}{\\da{L-1}} &= \\wl\n\\end{align}\n$$\n\nLetting us write:\n\n$$\n\\dcda{L-1} = \\left(\\wl\\right) \\left(\\sigma(\\zl)(1-\\sigma(\\zl))\\right) \\left(2(\\al -y)\\right)\n$$\n\nFor completion, and as an answer to a previous pause-and-ponder, lets write out the expression for $\\dcdb{L}$:\n\n$$\n\\dcdb{L} = \\dzdb{L}\\dadz{L}\\dcda{L}\n$$\n\nWe can get an expression for $\\dzdb{L}$:\n\n$$\n\\begin{align}\n\\zl      &= \\wl\\ax{L-1}+\\bl \\\\\n\\dzdb{L} &= 1\n\\end{align}\n$$\n\nAnd so\n$$\n\\begin{align}\n\\dcdb{L} &= 1\\cdot\\dadz{L}\\dcda{L}\\\\\n         &= \\left(\\sigma(\\zl)(1-\\sigma(\\zl))\\right) \\left(2(\\al -y)\\right)\n\\end{align}\n$$\n\nLets group this level all together to write all partials for this level:\n\n$$\n\\begin{align}\n\\dcdw{L}   &= \\left(\\ax{L-1}\\right) &&\\cdot&\\left(\\sigma(\\zl)(1-\\sigma(\\zl))\\right) \\left(2(\\al -y)\\right) \\\\\n\\dcdb{L}   &= 1 &&\\cdot&\\left(\\sigma(\\zl)(1-\\sigma(\\zl))\\right) \\left(2(\\al -y)\\right) \\\\\n\\dcda{L-1} &=\\left(\\wl\\right) &&\\cdot&\\left(\\sigma(\\zl)(1-\\sigma(\\zl))\\right) \\left(2(\\al -y)\\right)\n\\end{align}\n$$\n\n--- \n\n### Speaking of Error...\nAha! We can notice they all have something in common: \n\n$$\n\\dcdz{L}=\\dadz{L}\\dcda{L}\n$$\n\n<div class=\"info\">\n\n**Definition**: The **error** associated with layer $L$ is given by:\n    \n$$\n\\deltal = \\dcdz{L}=\\dadz{L}\\dcda{L}\n$$\n    \noften stated simply as:\n    \n$$\n\\deltal = \\dcdz{L}\n$$\n    \nand completely as:\n    \n$$\n\\begin{align}\n\\deltal &= \\sigma(\\zl)(1-\\sigma(\\zl)) (\\al -y) \\\\\n        &= \\sigma^{\\color{red}{\\prime}}(\\zl) (\\al-y)\n\\end{align}\n$$\n\n**Note:** We've discarded that $2$ from the cost as is often done.\n    \n</div>\n\n<div class=\"info\">\n\nüßê**Pause-and-ponder**: Why is this called the error?\n    \n</div>\n\nThis lets rewrite our partials as:\n\n$$\n\\begin{align}\n\\dcdw{L}   &= \\ax{L-1} \\deltal \\\\\n\\dcdb{L}   &= \\deltal \\\\\n\\dcda{L-1} &= \\wl \\deltal\n\\end{align}\n$$\n\n--- \n\n### What about the rest?\n\nAnd thats it! We've **finally** finished out how our cost function changes with respect to one layer: $L$. But we've only done one layer! Not to worry! We've discovered backpropagation and the chain rule! So the rest is easy! \n\nLets look at our map\n\n![](figures/1d-bp-map-lminus1.png)\n\nWe've boxed in what we know how to do so far. This also tells us its very straight forward to now give the equations for layer $L-1$:\n\n$$\n\\begin{align}\n\\dcdw{L-1} &= \\dzdw{L-1} \\dadz{L-1} \\dcda{L-1} \\\\\n\\dcdb{L-1} &= \\dzdb{L-1} \\dadz{L-1} \\dcda{L-1} \\\\\n\\dcda{L-2} &= \\frac{\\dz{L-1}}{\\da{L-2}} \\dadz{L-1} \\dcda{L-1}\n\\end{align}\n$$\n\n--- \n\n### Fully recursive definition \nWow! We again see they have something in common!\n\n<div class=\"info\">\n\n**Definition**: The error for layer $L-1$ is:\n    \n$$\n\\deltax{L-1} = \\dcdz{L-1}=\\dadz{L-1}\\dcda{L-1}\n$$\n    \noften stated simply as:\n    \n$$\n\\deltax{L-1} = \\dcdz{L-1}\n$$\n    \n</div>\n\nWe can now restate the above as:\n\n$$\n\\begin{align}\n\\dcdw{L-1} &= \\ax{L-2} \\deltax{L-1} \\\\\n\\dcdb{L-1} &=  \\deltax{L-1} \\\\\n\\dcda{L-2} &= \\wx{L-1} \\deltax{L-1}\n\\end{align}\n$$\n\nThat is a **recursive definition** that serves for the rest of the graph!\n\n--- \n\n### But wait...\n\nTheres more! Lets bring back the definitions of error for layer $L-1$, and the partials for layer $L$:\n\nPartials at layer $L$:\n\n$$\n\\begin{align}\n\\dcdw{L}   &= \\ax{L-1} \\deltal \\\\\n\\dcdb{L}   &= \\deltal \\\\\n\\dcda{L-1} &= \\wl \\deltal\n\\end{align}\n$$\n\nError at layer $L-1$:\n\n$$\n\\deltax{L-1} = \\dcdz{L-1}=\\dadz{L-1}\\dcda{L-1}\n$$\n\nAha! We see something in common again! \n\n$$\n\\boxed{\\dcda{L-1} = \\wl \\deltal}\n$$\n\n$$\n\\deltax{L-1} = \\dcdz{L-1}=\\dadz{L-1}\\boxed{\\dcda{L-1}}\n$$\n\nLets sub that in and write:\n\n$$\n\\deltax{L-1} = \\dcdz{L-1}=\\dadz{L-1}\\wl \\deltal\n$$\n\nWe've discovered another **major equation!**:\n\n<div class=\"info\">\n\nüìñ **Definition**: The error at **any** layer $L-1$ can be written as a function of the **next layer** $L$:\n    \n$$\n\\deltax{L-1} = \\wl \\deltal \\cdot \\sigma^{\\color{red}{\\prime}}(\\zx{L-1})\n$$\n    \n</div>\n\nFor example, for layer $L-2$:\n\n$$\n\\begin{align}\n\\deltax{L-2} &= \\wx{L-1}\\deltax{L-1}\\sigma^{\\color{red}{\\prime}}(\\zx{L-2}) \\\\\n             &= \\wx{L-1}\\left[  \\wl \\deltal \\cdot \\sigma^{\\color{red}{\\prime}}(\\zx{L-1}) \\right]\\sigma^{\\color{red}{\\prime}}(\\zx{L-2})\n\\end{align}\n$$\n\n--- \n\n### The Four Fundamental Backpropagation Equations‚Ñ¢\n\nWe did it! We've completed backprop on this simple 1D network!\n\n![](figures/1d-bp-net.png)\n\nLets cap this off by rewriting **scalar versions** of the four fundamental backpropagation equations:\n\n<div class=\"info\">\n    \n**üìñDefinition**: **Scalar** versions of the **Four Fundamental Backpropagation Equations‚Ñ¢** are given by:\n    \n$$\n\\begin{align}\n\\delta^L &= (\\al-y) \\cdot \\sigma^{\\color{red}{\\prime}}(\\zl)  \\tag{BP1} \\\\\n\\deltax{\\ell} &= \\wx{l+1} \\delta^{\\ell+1} \\cdot \\sigma^{\\color{red}{\\prime}}(\\zx{\\ell}) \\tag{BP2} \\\\\n\\dcdb{\\ell} &= \\delta^{\\ell} \\tag{BP3} \\\\\n\\dcdw{\\ell} &= \\ax{\\ell-1}\\delta^{\\ell} \\tag{BP4}\n\\end{align}\n$$\n    \nwhere $\\ell$ is any layer.\n\n</div>\n\n\n\nSome explanation of each:\n\n* BP1 defines the error for the **last layer**. This is the first thing we calculate to perform backprop.\n* BP2 defines the error for **any layer** $\\ell$ in terms of the error at the next level $\\ell+1$. \n* BP3 defines the bias update at **any layer** $\\ell$\n* BP4 defines the weight update at **any layer** $\\ell$\n\nBP2 and BP4 have **interesting interpretations** which will become more salient when we get to matrices/vectors, but that we can first describe here:\n\n--- \n\nLets start with BP2:\n\n$$\n\\deltax{\\ell} = \\wx{l+1} \\delta^{\\ell+1} \\cdot \\sigma^{\\color{red}{\\prime}}(\\zx{\\ell})\n$$\n\nWe can think of this as **the** backprop equation, as it clearly captures the backward flow of errors from outputs back through the network! \n\nYou can also think of BP2 as saying the following: Just as $\\wx{\\ell+1}$ acted on the **activation** of the **previous** layer $\\ax{l}$ to **bring it forward** to the current layer, it acts on the **error** of the **current layer** to **bring it back** to the previous layer! \n \n**Note:** This will become a bit more intutive when we get to matrices, as we will see this equation will have a weight matrix in the forward direction, and a **transpose** in the reverse direction\n\n--- \n\nLets rewrite BP4 as:\n\n$$\n\\dcdw{} = \\color{green}{a_{in}}\\delta_{out}\n$$\n\n\nwhere it's understood that $\\color{green}{a_{in}}$ is the activation of the neuron input to the weight $w$, and $\\delta_{out}$ is the **error** of the neuron output from the weight $w$.\n\nClearly, when $\\color{green}{a_{in}}$ is small, then $\\dcdw{}$ is small. This is another way to say that this weight **learns slowly**, meaning that it's not changing much during gradient descent. In other words, one consequence of BP4 is that weights output from low-activation neurons learn slowly.\n\n<div class=\"info\">\n\nüßê**Pause-and-ponder**: What else can we say? Think about what these equations are telling us! \n    \n</div>\n\n\n### Implementation\n\nWe're now ready to go ahead and implement backprop ourselves! \n\n<div class=\"info\">\n\nüí™üèΩ**Exercise**: Implement this procedure on our toy example!\n    \n</div>\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\ndef backprop_1D():\n    # EDIT HERE\n    return\n```\n:::\n\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\n# Step 0: fix all FP values: a1, a2, a3 and all parameters\n# Step 1: calculate delta_L\n```\n:::\n\n\n<div class=\"info\">\n\nüßê**Pause-and-ponder**: Once we've implemented these equations and calculated our partials all the way back through our network, what do we do?! \n    \n</div>\n\n\n## Multidimensional case\n\n\nNow things are going to get **interesting**! In our simple toy example, we only had one neuron per layer. This means in our resulting equations everything was just scalar values. \n\n### Latex definitions\nThis cell contains more latex definitions. $\n\\newcommand{\\j}{{j}}\n\\newcommand{\\k}{{k}}\n\\newcommand{\\wjkl}{\\color{blue}{w_{\\j,\\k}^L}}\n\\newcommand{\\ajl}{\\color{green}{a_j^L}}\n\\newcommand{\\ajx}[1]{\\color{green}{a_j^{#1}}}\n\\newcommand{\\akx}[1]{\\color{green}{a_k^{#1}}}\n\\newcommand{\\zjl}{\\color{orange}{z_j^L}}\n\\newcommand{\\va}{\\color{green}{\\mathbf{a}}}\n\\newcommand{\\vy}{\\mathbf{y}}\n\\newcommand{\\vz}{\\color{orange}{\\mathbf{z}}}\n\\newcommand{\\dwjkl}{\\color{blue}{\\partial w_{\\j,\\k}^L}}\n\\newcommand{\\dcdwjk}{\\frac{\\dc}{\\dwjkl}}\n\\newcommand{\\dzjdwjk}{\\frac{\\color{orange}{\\partial z_j^L}}{\\dwjkl}}\n\\newcommand{\\dajdzj}{\\frac{\\color{green}{\\partial \\ajl}}{\\color{orange}{\\partial z_j^L}}}\n\\newcommand{\\dcdaj}{\\frac{\\dc}{\\color{green}{\\partial \\ajl}}}\n\\newcommand{\\dcdakx}[1]{\\frac{\\dc}{\\color{green}{\\partial \\akx{#1}}}}\n$**Note**: remember to run it!\n\n### Notation\n\nNow, we have to be a bit more careful! Lets remind ourselves of the notation we used above:\n\n![](figures/nd-net.png)\n\nHere, we've labeled each neuron by its activation, layer number, and number in layer. For example, $\\ajl$ is neuron $\\j$, in layer $L$, and $\\wjkl$ is the weight that connects neuron $\\k$ in layer $L-1$ to neuron $\\j$ in layer $L$.\n\nNow, the **cost** of a single example $\\czero$ is a sum over the activations of all neurons in the last layer:\n\n$$\n\\czero = \\frac{1}{2}\\sum_{\\j} (\\ajl - y_{\\j})^2\n$$\n\nwhich in vector notation is:\n\n$$\n\\czero = \\frac{1}{2}\\|\\va^L - \\vy\\|^2\n$$\n\nBefore going into vector/matrix notation, lets still deal with an **individual weight** in this network: $\\wjkl$, and write out our backprop equation for this single weight:\n\n$$\n\\dcdwjk = \\dzjdwjk \\dajdzj \\dcdaj\n$$\n\nFor completion, lets write expressions for each of its pieces.\n\nFor $\\dcdaj$:\n\n$$\n\\begin{align}\n\\czero &= \\frac{1}{2}\\sum_{\\j} (\\ajl - y_{\\j})^2 \\\\\n\\dcdaj &= (\\ajl - y_j)\n\\end{align}\n$$\n\nFor $\\dajdzj$:\n$$\n\\begin{align}\n\\dajdzj = \\sigma(\\zjl)(1-\\zjl)\n\\end{align}\n$$\n\nFor $\\dzjdwjk$:\n$$\n\\begin{align}\n\\zjl     &= \\ldots + \\wjkl \\color{green}{a_k^{L-1}} + \\ldots \\\\\n\\dzjdwjk & =  \\color{green}{a_k^{L-1}}\n\\end{align}\n$$\n\n**Note:** Pay particular attention to the $k$ index in the equation above! Remember, our weights are applied to the activations of the previous layer, which we are using $k$ to index!\n\nJust as we did above, we can define a **neuron specific error measure** for neuron $j$ in layer $L$ as:\n\n$$\n\\begin{align}\n\\delta_{j}^L &= \\frac{\\dc}{\\color{orange}{\\partial}\\zjl} \\\\\n             &= \\dajdzj \\dcdaj \\\\\n             &= \\sigma(\\zjl)\\sigma(1-\\zjl) (\\ajl - y_j)\n\\end{align}\n$$\n\n**Note**: pay particular attention to the $j$ index above! This is with respect to the current layer, which we are using $j$ to index!\n\nThis lets us rewrite the above as:\n\n$$\n\\dcdwjk = \\color{green}{a_k^{L-1}} \\delta_{j}^L\n$$\n\nAbove we've given the equation for how the cost changes with a specific weight in our multi dimensional network. Notice how close it is to the 1D case! \n\n<div class=\"info\">\n    \nüí™üèΩ**Exercise**: Give the equation for $\\frac{\\dc}{\\color{purple}{\\partial b^L_j}}$\n    \n</div>\n\n\nNow, lets look at $\\dcdakx{L-1}$. This is asking how the cost varies when we change the activation of a neuron $k$ in layer $L-1$. \n\nJust using pattern matching, we might **want** to write:\n\n$$\n\\dcdakx{L-1} \\stackrel{\\color{red}{?}}{=} \\wjkl \\delta_j^L\n$$\n\nBut **this is incorrect!** To see why, lets draw a picture showing how nueron $k$ in layer $L-1$ affects the current layer (and therefore the cost):\n\n![](figures/2d-ak.png)\n\nAha! The activation of neuron $k$ in layer $L-1: \\akx{L-1}$ does **not** just flow through neuron $j$ in layer $L$, it flows through **every single neuron in layer $L$!**\n\nSo if we want to account for the effect a small nudge to this neuron's activation value has on our cost, we need to account for **each neuron in layer $L$**, and each associated weight! \n\nOur correct equation is then:\n\n$$\n\\dcdakx{L-1} = \\sum_j \\left[ \\wjkl \\delta_j^L \\right]\n$$\n\n\n### Implementation\n\nOk! We've gone to the multidimensional case, but still given equations for each individual parameter. So really, its almost exactly as it was in the 1D case!\n\n<div class=\"info\">\n    \nüí™üèΩ**Exercise**: Implement the backprop equations above! \n\n</div>\n\nAs a hint, some **pseudo-code** for the implementation would be:\n\n```\n# skipping setup\n# skipping forward pass\n\nfor sample in inputs:\n    # last layer\n    for neuron_j in layer[-1].neurons:\n        delta_j = # calculate according to formula above\n        dc_dwj = # calculate according to formula above\n        dc_dbj = # calculate according to formula above\n        dc_dak^{L-1} = # calculate according to formula above\n    \n    # other layers\n    for layer in layers-1:\n        for neuron_j in layer[l].neurons:\n            delta_j = # calculate according to formula above\n            dc_dwj = # calculate according to formula above\n            dc_dbj = # calculate according to formula above\n            dc_dak^{L-1} = # calculate according to formula above\n    \n```\n\n\n\n## N-D Revisited - Vector Notation\n\nAbove, we gave specific equations for each parameter in a multi dimensional network. This will work, as your implementation should prove! \n\nHowever, it leaves much to be desired in terms of efficiency, and conciseness, and it doesn't allow us to make full use of the magic of our numerical linear algebra libraries! \n\nLets begin our analysis with our error vector $\\delta_{j}^L$:\n\n$$\n\\begin{align}\n\\delta_{j}^L &= \\frac{\\dc}{\\color{orange}{\\partial}\\zjl} \\\\\n             &= \\dajdzj \\dcdaj \\\\\n             &= \\sigma(\\zjl)\\sigma(1-\\zjl) (\\ajl - y_j) \\\\\n             &= \\sigma^{\\color{red}{\\prime}}(\\zjl) (\\ajl - y_j)\n\\end{align}\n$$\nwhere we brought back the $\\sigma^{\\color{red}{\\prime}}(\\cdot)$ notation.\n\n\nWe would like to write a **vector** for our error $\\delta^L$, where each component is:\n\n$$\n\\delta^L = \\begin{bmatrix}\n\\sigma^{\\color{red}{\\prime}}(\\color{orange}{z_1^L}) (\\color{green}{a_1^L} - y_1) \\\\\n\\cdots \\\\\n\\sigma^{\\color{red}{\\prime}}(\\zjl) (\\color{green}{a_j^L} - y_j)\\\\\n\\cdots\n\\end{bmatrix}\n$$\n\nThis is exactly the definition of element-wise product of the following vectors: $\\color{orange}{\\mathbf{z}^L},\\color{green}{\\mathbf{a}^L}$ and $\\vy$:\n\n$$\n\\delta^L = \\sigma^{\\color{red}{\\prime}}(\\color{orange}{\\mathbf{z}^L}) \\odot (\\color{green}{\\mathbf{a}^L} - \\vy)\n$$\n\nwhere we introduce another piece of notation:\n\n<div class=\"info\">\n\nüìñ**Definition**: The **Hadamard product** between two vectors is the element-wise product:\n    \n$$\n\\mathbf{a} \\odot \\mathbf{b} = \n\\begin{bmatrix}\na_1 \\cdot b_1 \\\\\n\\cdots \\\\\na_n \\cdot b_n\n\\end{bmatrix}\n$$\n    \n</div>\n\nLets define a **function** to implement this element-wise product:\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\ndef hadamard(a,b):\n    result = np.zeros_like(a)\n    for i in range(a.shape[0]):\n        result[i] = a[i] * b[i]\n    \n    return result\n```\n:::\n\n\nLets test it!\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\na,b = np.random.randint(1,10,(3,1)),np.random.randint(1,10,(3,1))\na,b\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\n(array([[9],\n        [9],\n        [8]]),\n array([[8],\n        [5],\n        [1]]))\n```\n:::\n:::\n\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\nhadamard(a,b)\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\narray([[72],\n       [45],\n       [ 8]])\n```\n:::\n:::\n\n\nIt works! \n\n**However**, Python actually already does this element-wise product for us! Using the `*` operator!\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\na*b\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\narray([[72],\n       [45],\n       [ 8]])\n```\n:::\n:::\n\n\nSo we didn't have to implement our own. But thats ok, because we know how to.  Lets get back to business! \n\n---\n\nWe've rewritten BP1 in vector notation. Lets now focus on BP2: **moving the error \"backward\"**.\n\nWell, we can probably already guess its going to involve the hadamard product with $\\sigma^{\\color{red}{\\prime}}(\\cdot)$ as it did previously. We just don't know with what yet! \n\n$$\n\\begin{align}\n\\delta^\\ell = ?? \\odot \\sigma^{\\color{red}{\\prime}}(\\mathbf{\\zx{\\ell}})\n\\end{align}\n$$\n\nWell, lets try to reason through it from \"first principles\" (**recall**: this means **make the sizes work!**).\n\n$\\delta^\\ell$ is a vector, which should be of size $k$, the number of elements in layer $\\ell$. We know its going to be formed with the **weight matrix** and $\\delta^{\\ell+1}$, so lets write their sizes:\n\n$$\n\\color{blue}{W^{\\ell+1}}: (j \\times k), \\quad\\quad \\delta^{\\ell+1}: (j \\times 1)\n$$\n\nHow do we multiply these out to get a vector of size $k \\times 1$ out?\n\nüßê\n\nIndeed! We need to **transpose** our weight matrix, and then take the usual matrix-vector product with $\\delta^{\\ell+1}$! \n\nNow we can write the vectorized equation for the error at any layer:\n\n<div class=\"info\">\n\nüìñ**Vectorized BP2**: The error at **any** layer $\\ell$ can be written as as function of the next layer as:\n\n$$\n\\delta^{\\ell} = (\\color{blue}{W^{\\ell+1}})^T \\delta^{\\ell+1} \\odot \\sigma^{\\color{red}{\\prime}}(\\mathbf{\\zx{\\ell}})\n$$\n\n</div>\n\n<div class=\"info\">\n\nüßê**Pause-and-ponder**: What can we say about our previous interpretation in light of this new equation? \n    \n</div>\n\nLets quickly tackle BP3:\n\n\n<div class=\"info\">\n\nüìñ**Vectorized-BP3**: \n\n$$\n\\frac{\\dc}{\\color{purple}{\\partial \\mathbf{b}^\\ell}} = \\delta^\\ell\n$$\n\n</div>\n\nWell, that was easy! Its just as it was above, but as a vector. \n\nHm. Now we get to the tough one: **BP4**. This is now a partial derivative of our cost **with respect to weight matrix**! \n\nLets start at the last layer. $\\color{blue}{W^L}$ is of size $j \\times k$, where $j$ is the size of the last layer. \n\nSo, we should expect its **gradient** to be of the same size: **a matrix**. If its not clear why, just imagine how were going to use this partial. We're going to add it the current value of $\\color{blue}{W}$ in gradient descent, so it better be of the same size!\n\nTo form this matrix, we have $\\color{green}{\\mathbf{a}^{L-1}}$, which is of size $k \\times 1$, the size of the previous layer, and $\\delta^L$, which is of size $j \\times 1$. \n\nSo how do we form this matrix using these two vectors? We take the **outer product**:\n\n$$\n\\delta^L(\\color{green}{\\mathbf{a}^{L-1}})^T\n$$\n\nLets make sure the sizes work! \n\n$$\n(j \\times 1)(1 \\times k) = (j \\times k)\n$$\n\nPerfect! \n\n<div class=\"info\">\n\nüìñ**Vectorized BP4**: \n    \n$$\n\\frac{\\dc}{ \\color{blue}{ \\partial \\mathbf{W^\\ell} } } = \\delta^L(\\color{green}{\\mathbf{a}^{L-1}})^T\n$$\n    \n</div>\n\n\nFinally! Lets give fully vectorized forms of the batchprop equations:\n\n--- \n\n### F.F.B.E - Vectorized‚Ñ¢\n\nLets cap this off by writing **vectorized versions** of the four fundamental backpropagation equations:\n\n<div class=\"info\">\n    \n**üìñDefinition**: **Vectorized** versions of the **Four Fundamental Backpropagation Equations‚Ñ¢** are given by:\n    \n$$\n\\begin{align}\n\\delta^L &= (\\color{green}{\\mathbf{a}^L} - \\vy)  \\odot  \\sigma^{\\color{red}{\\prime}}(\\color{orange}{\\mathbf{z}^L}) \\tag{BP1}\\\\\n\\delta^{\\ell} &= (\\color{blue}{W^{\\ell+1}})^T \\delta^{\\ell+1} \\odot \\sigma^{\\color{red}{\\prime}}(\\mathbf{\\zx{\\ell}})\\tag{BP2} \\\\\n\\frac{\\dc}{\\color{purple}{\\partial \\mathbf{b}^\\ell}} &= \\delta^\\ell \\tag{BP3} \\\\\n\\frac{\\dc}{ \\color{blue}{ \\partial \\mathbf{W^\\ell} } } &= \\delta^\\ell(\\color{green}{\\mathbf{a}^{\\ell-1}})^T \\tag{BP4}\n\\end{align}\n$$\n\n</div>\n\n--- \n\n### Implementation\n\nWow! Now we've really come a long way!\n\n<div class=\"info\">\n    \nüí™üèΩ**Exercise**: Implement the backprop equations above! \n\n</div>\n\nAs you're implementing this, think about the following:\n<div class=\"info\">\n    \nüßê**Pause-and-ponder**: How do you deal with multiple samples? \n    \n</div>\n\nInitialize our toy problem as always:\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\nX,y = toy()\n```\n:::\n\n\nSince we're still dealing with a **flat implementation**, lets go ahead and initialize our weights and biases here:\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\n# initialize weight matrices - notice the dimensions\nW_1_init = np.random.randn(3,1)\nW_2_init = np.random.randn(2,3)\nW_3_init = np.random.randn(1,2)\nweights = [W_1_init,W_2_init,W_3_init]\n\n# and our biases\nb_1_init = np.random.randn(3,1)\nb_2_init = np.random.randn(2,1)\nb_3_init = np.random.randn(1,1)\nbiases = [b_1_init,b_2_init,b_3_init]\n```\n:::\n\n\nNow we can use the feedforward batch implementation we gave above:\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\ndef feedforward_batch(X, weights, biases):\n    # assume inputs is of shape Nxd\n    W_1,W_2,W_3 = weights[0],weights[1],weights[2]\n    b_1,b_2,b_3 = biases[0],biases[1],biases[2]\n    \n    activities = []\n    activations = []\n    \n    # NOTE - there is no for loop here! \n    \n    # treat activations like layer_0 activations:\n    activations.append(X.T)\n    \n    # layer 1\n    z_1 = W_1.dot(X.T) + b_1\n    a_1 = sigmoid(z_1)\n    activities.append(z_1)\n    activations.append(np.squeeze(np.asarray(a_1)))\n    \n    # layer 2\n    z_2 = W_2.dot(a_1)+b_2\n    a_2 = sigmoid(z_2)\n    activities.append(z_2)\n    activations.append(np.squeeze(np.asarray(a_2)))\n    \n    # output layer\n    z_3 = W_3.dot(a_2)+b_3\n    y_hat = sigmoid(z_3)\n    activities.append(z_3)\n    activations.append(np.asarray(y_hat))\n    \n    return activities, activations\n```\n:::\n\n\nLets actually feed-forward our entire dataset:\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\nactivities, activations = feedforward_batch(X,weights,biases)\n```\n:::\n\n\nLets check the sizes of these:\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\nprint(activities[0].shape,activities[1].shape,activities[2].shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(3, 1000) (2, 1000) (1, 1000)\n```\n:::\n:::\n\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\nprint(activations[0].shape,activations[1].shape,\n      activations[2].shape, activations[3].shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1, 1000) (3, 1000) (2, 1000) (1, 1000)\n```\n:::\n:::\n\n\n**Note**: We're calling our input our first activity, which is why there is one extra.\n\nNow we're ready to run backprop **on each sample** in our dataset, to generate bias and weight updates for each sample.\n\nLets start with a sigmoid prime function we will need:\n\n::: {.cell execution_count=51}\n``` {.python .cell-code}\ndef sigmoid_prime(z):\n    return sigmoid(z)*(1-sigmoid(z))\n```\n:::\n\n\n::: {.cell execution_count=52}\n``` {.python .cell-code}\nplt.plot(sigmoid(np.linspace(-10,10)));\nplt.plot(sigmoid_prime(np.linspace(-10,10)));\n```\n\n::: {.cell-output .cell-output-display}\n![](nn_files/figure-html/cell-53-output-1.png){width=571 height=411}\n:::\n:::\n\n\nLets continue our work:\n\n::: {.cell execution_count=53}\n``` {.python .cell-code}\nN,d = inputs.shape\n\nweight_updates = []\nbias_updates = []\n```\n:::\n\n\n::: {.cell execution_count=54}\n``` {.python .cell-code}\nlayer_num = len(activations)\nweight_updates = []\nbias_updates = []\n```\n:::\n\n\nNow our equations are above are only for one sample! So lets pick one and proceed:\n\n::: {.cell execution_count=55}\n``` {.python .cell-code}\nsample_idx = 0\n```\n:::\n\n\nOk, so lets run backprop on that one sample, by starting with our last layer:\n\n::: {.cell execution_count=56}\n``` {.python .cell-code}\n########################################\n#           Last Level\n########################################\n# get last layer specific variables\nz_L = activities[-1][:,sample_idx].reshape(-1,1)\na_L = activations[-1][:,sample_idx].reshape(-1,1)\na_L_minus_1 = activations[-2][:,sample_idx].reshape(-1,1)\n```\n:::\n\n\n::: {.cell execution_count=57}\n``` {.python .cell-code}\n(z_L.shape,a_L.shape,a_L_minus_1.shape)\n```\n\n::: {.cell-output .cell-output-display execution_count=57}\n```\n((1, 1), (1, 1), (2, 1))\n```\n:::\n:::\n\n\nAre these what we expect?\n\nLets use them in the formulas we gave above!\n\n::: {.cell execution_count=58}\n``` {.python .cell-code}\n# calculate delta_L for the last level\ndelta_L = (a_L-y[sample_idx])*sigmoid_prime(z_L)\n```\n:::\n\n\n::: {.cell execution_count=59}\n``` {.python .cell-code}\ndelta_L.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=59}\n```\n(1, 1)\n```\n:::\n:::\n\n\nLets make a list to store all these delta values!\n\n::: {.cell execution_count=60}\n``` {.python .cell-code}\ndeltas = []\ndeltas.append(delta_L)\n```\n:::\n\n\nNow lets calculate the bias update:\n\n::: {.cell execution_count=61}\n``` {.python .cell-code}\n# calculate bias update\ndc_db_l = delta_L\nbias_updates = [dc_db_l] + bias_updates\ndc_db_l.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=61}\n```\n(1, 1)\n```\n:::\n:::\n\n\nAnd the weight update:\n\n::: {.cell execution_count=62}\n``` {.python .cell-code}\n# calcualte weight updates\ndc_dw_l = delta_L.dot((a_L_minus_1).T)\nweight_updates = [dc_dw_l] + weight_updates\ndc_dw_l.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=62}\n```\n(1, 2)\n```\n:::\n:::\n\n\nWow! We've run backprop across our last layer! Now all we have to do, is apply this to the rest of our layers!\n\n::: {.cell execution_count=63}\n``` {.python .cell-code}\n########################################\n# loop through each layer, from 2 to L-1\n######################################## \nfor layer in range(2,layer_num):\n    # using level as a **negative index**\n    l = -layer\n\n    # uncomment this print statement \n    # to help you understand how negative indexing works\n    print(f\"Loop variable: {layer}, l={l}, corresponding to layer: l{l}\")\n\n    # calculate delta_l for each layer\n    z_l = activities[l][:,sample_idx].reshape(-1,1)\n    a_l = activations[l][:,sample_idx].reshape(-1,1)\n    delta_l = weights[l+1].T.dot(deltas[l+1]) * sigmoid_prime(z_l)\n    deltas = [delta_l] + deltas\n\n    # calculate bias update\n    dc_db_l = delta_l\n    bias_updates = [dc_db_l] + bias_updates\n\n    # calcualte weight updates\n    a_l_minus_1 = activations[l-1][:,sample_idx].reshape(-1,1)\n    dc_dw_l = delta_l.dot((a_l_minus_1).T)\n    weight_updates = [dc_dw_l] + weight_updates\n    \n    print(f'=========Layer:{layer_num+l}=========')\n    print(f'Using negative index: {l}')\n    print(f'z_l:{z_l.shape}, a_l:{a_l.shape}, a_l_minus_1:{a_l_minus_1.shape}')\n    print(f'delta_l:{delta_l.shape}, dc_db_l:{dc_db_l.shape},dc_dw_l:{dc_dw_l.shape}')\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoop variable: 2, l=-2, corresponding to layer: l-2\n=========Layer:2=========\nUsing negative index: -2\nz_l:(2, 1), a_l:(2, 1), a_l_minus_1:(3, 1)\ndelta_l:(2, 1), dc_db_l:(2, 1),dc_dw_l:(2, 3)\n\nLoop variable: 3, l=-3, corresponding to layer: l-3\n=========Layer:1=========\nUsing negative index: -3\nz_l:(3, 1), a_l:(3, 1), a_l_minus_1:(1, 1)\ndelta_l:(3, 1), dc_db_l:(3, 1),dc_dw_l:(3, 1)\n\n```\n:::\n:::\n\n\nAll thats left to do, is wrap this up in a reusable function! \n\n::: {.cell execution_count=64}\n``` {.python .cell-code}\ndef backprop_1_example(activities,activations,y,sample_idx,quiet=True):\n    layer_num = len(activations)\n    weight_updates = []\n    bias_updates = []\n    \n    ########################################\n    #           Last Level\n    ########################################\n    # get last layer specific variables\n    a_L = activations[-1][:,sample_idx].reshape(-1,1)\n    a_L_minus_1 = activations[-2][:,sample_idx].reshape(-1,1)\n    z_L = activities[-1][:,sample_idx].reshape(-1,1)\n    \n    # calculate delta_L for the last level\n    delta_L = (a_L-y)*sigmoid_prime(z_L)\n    deltas = []\n    deltas.append(delta_L)\n\n    # calculate bias update\n    dc_db_L = delta_L\n    bias_updates = [dc_db_L] + bias_updates\n\n    # calcualte weight updates\n    dc_dw_L = delta_L.dot((a_L_minus_1).T)\n    weight_updates = [dc_dw_L] + weight_updates\n    \n    if not quiet:\n        print(f'=========Layer:{layer_num+-1}=========')\n        print(f'Using negative index: -1')\n        print(f'z_l:{z_L.shape}, a_l:{a_L.shape}, a_l_minus_1:{a_L_minus_1.shape}')\n        print(f'delta_l:{delta_L.shape}, dc_db_l:{dc_db_L.shape},dc_dw_l:{dc_dw_L.shape}')\n        print()\n    \n    ########################################\n    # loop through each layer, from 2 to L-1\n    ######################################## \n    for layer in range(2,layer_num):\n        # using level as a **negative index**\n        l = -layer\n        \n        # uncomment this print statement \n        # to help you understand how negative indexing works\n        # print(f\"Loop variable: {layer}, l={l}, corresponding to layer: l{l}\")\n\n        # calculate delta_l for each layer\n        a_l = activations[l][:,sample_idx].reshape(-1,1)\n        z_l = activities[l][:,sample_idx].reshape(-1,1)\n        delta_l = weights[l+1].T.dot(deltas[l+1]) * sigmoid_prime(z_l)\n        deltas = [delta_l] + deltas\n\n        # calculate bias update\n        dc_db_l = delta_l\n        bias_updates = [dc_db_l] + bias_updates\n\n        # calcualte weight updates\n        a_l_minus_1 = activations[l-1][:,sample_idx].reshape(-1,1)\n        dc_dw_l = delta_l.dot((a_l_minus_1).T)\n        weight_updates = [dc_dw_l] + weight_updates\n        \n        if not quiet:\n            print(f'=========Layer:{layer_num+l}=========')\n            print(f'Using negative index: {l}')\n            print(f'z_l:{z_l.shape}, a_l:{a_l.shape}, a_l_minus_1:{a_l_minus_1.shape}')\n            print(f'delta_l:{delta_l.shape}, dc_db_l:{dc_db_l.shape},dc_dw_l:{dc_dw_l.shape}')\n            print()\n    \n    return weight_updates, bias_updates\n```\n:::\n\n\nWe now have the following:\n* `feedforward_batch`: a function which feeds entire **batches** through our network\n* `backprop_1_example`: a function which backpropagates **the erorr associated with a single example** through the network\n\n\nLets now revisit: \n\n<div class=\"info\">\n    \nüßê**Pause-and-ponder**: How do you deal with multiple samples? \n    \n</div>\n\nAnd try to think of how we can implement this algorithm:\n\n## More interesting example\n\n::: {.cell execution_count=65}\n``` {.python .cell-code}\n# Generate the dataset\nX, y = sklearn.datasets.make_circles(\n    n_samples=1000, shuffle=False, factor=0.3, noise=0.1)\n\n# Separate the red and blue samples for plotting\nx_red = X[y==0]\nx_blue = X[y==1]\n\n# correct size of y\ny = y.reshape(-1,1)\n\nprint('shape of X: {}'.format(X.shape))\nprint('shape of y: {}'.format(y.shape))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nshape of X: (1000, 2)\nshape of y: (1000, 1)\n```\n:::\n:::\n\n\n::: {.cell execution_count=66}\n``` {.python .cell-code}\n# Plot both classes on the x1, x2 plane\nplt.figure(figsize=(10, 7))\nplt.plot(x_red[:,0], x_red[:,1], 'r*', \n         label='class: red star', alpha=0.75)\nplt.plot(x_blue[:,0], x_blue[:,1], 'bo', \n         label='class: blue circle', alpha=0.75)\nplt.legend(loc=1)\nplt.xlabel('$x_1$', fontsize=20)\nplt.ylabel('$x_2$', fontsize=20)\nplt.axis([-1.5, 1.5, -1.5, 1.5])\nplt.title('red star vs blue circle classes in the input space', fontsize=20);\n```\n\n::: {.cell-output .cell-output-display}\n![](nn_files/figure-html/cell-67-output-1.png){width=847 height=619}\n:::\n:::\n\n\n### Train our Network! \n\nNow we can actually train our network using gradient descent as we have before! \n\n::: {.cell execution_count=67}\n``` {.python .cell-code}\n# dataset\nN = X.shape[0]\nd = X.shape[1]\nbatch_size = 50\nepochs = 10\neta=1e-2\ndataset=X\nquiet=False\n\n# initialize weight matrices - notice the dimensions\nW_1_init = np.random.rand(3,2)\nW_2_init = np.random.rand(3,3)\nW_3_init = np.random.randn(1,3)\nweights = [W_1_init,W_2_init,W_3_init]\n\n# and our biases\nb_1_init = np.random.rand(3,1)\nb_2_init = np.random.rand(3,1)\nb_3_init = np.random.rand(1,1)\nbiases = [b_1_init,b_2_init,b_3_init]\n\n# network\nW_1,W_2,W_3 = weights[0],weights[1],weights[2]\nb_1,b_2,b_3 = biases[0],biases[1],biases[2]\n\n# mini-batch params \nnum_batches = int(N/batch_size)\niterations = 0\n\n# debugging lists\nloss = []\ngrad_norm_1 = []\ngrad_norm_2 = []\ngrad_norm_3 = []\nnorm_1 = []\nnorm_2 = []\nnorm_3 = []\n\n\nfor epoch in range(epochs):\n\n    # work on current batch \n    for batch_num in range(num_batches):\n\n        # get current batch\n        batch_start = batch_num*batch_size\n        batch_end = (batch_num+1)*batch_size\n        batch = dataset[batch_start:batch_end,:]\n        batch_labels = labels[batch_start:batch_end,:].reshape(-1,1)\n\n        # feedforward on batch\n        activities, activations = feedforward_batch(batch, weights,biases)\n        \n\n        # setup matrices to hold gradients\n        grad_W_1 = np.zeros_like(W_1)\n        grad_b_1 = np.zeros_like(b_1)\n        grad_W_2 = np.zeros_like(W_2)\n        grad_b_2 = np.zeros_like(b_2)\n        grad_W_3 = np.zeros_like(W_3)\n        grad_b_3 = np.zeros_like(b_3)\n        \n\n        # loop through each example in the batch\n        for idx in range(batch.shape[0]):\n            current_sample = batch[idx,:]\n            current_label = batch_labels[idx]\n\n            # get current weight and bias updates\n            weight_updates, bias_updates = backprop_1_example(activities,activations,current_label,idx)\n\n            # aggregate them \n            grad_W_1 = grad_W_1 + weight_updates[0]\n            grad_b_1 = grad_b_1 + bias_updates[0]\n\n            grad_W_2 = grad_W_2 + weight_updates[1]\n            grad_b_2 = grad_b_2 + bias_updates[1]\n\n            grad_W_3 = grad_W_3 + weight_updates[2]\n            grad_b_3 = grad_b_3 + bias_updates[2]\n            \n            \n        grad_norm_1.append(np.linalg.norm(grad_W_1))\n        grad_norm_2.append(np.linalg.norm(grad_W_2))\n        grad_norm_3.append(np.linalg.norm(grad_W_3))\n\n\n        # take your steps:\n        W_1 = W_1 - eta/batch_size*(grad_W_1)\n        b_1 = b_1 - eta/batch_size*(grad_b_1)\n\n        W_2 = W_2 - eta/batch_size*(grad_W_2)\n        b_2 = b_2 - eta/batch_size*(grad_b_2)\n\n        W_3 = W_3 - eta/batch_size*(grad_W_3)\n        b_3 = b_3 - eta/batch_size*(grad_b_3)\n        \n        norm_1.append(np.linalg.norm(W_1))\n        norm_2.append(np.linalg.norm(W_2))\n        norm_3.append(np.linalg.norm(W_3))\n        \n        # save new weights and biases to be used in the next feedforward step\n        weights = [W_1,W_2,W_3]\n        biases = [b_1,b_2,b_3]\n        \n        # calculate current loss \n        loss.append(mse(activations[-1],batch_labels))\n    \n    print(f\"Epoch:{epoch+1}/{epochs}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:1/10\nEpoch:2/10\nEpoch:3/10\nEpoch:4/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:5/10\nEpoch:6/10\nEpoch:7/10\nEpoch:8/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:9/10\nEpoch:10/10\n```\n:::\n:::\n\n\nüßê Hm. Wait a minute - **how do we know we're doing the right thing**? We should check it! \n\n## Gradient Checking\n\nBackprop implementations are notoriously difficult to get right! That means we should implement some kind of **numerical check for correctness**.\n\n<div class=\"info\">\n    \nüßê**Pause-and-ponder**: Can you think of a way we could **numerically check** that our *analytical* gradients are correct? \n    \n</div>\n\n### Numerical Gradients! \n\nHaving thought about it a bit, you might have thought to check our numerical approximations for the derivative! \n\nRecall, we could write the cost function as a function of **one long vector** of **all of our parameters**: \n\n$$\n\\czero(\\color{blue}{\\mathbf{W}_1,\\mathbf{W}_2,\\mathbf{W}_3},\\color{purple}{\\mathbf{b}_1,\\mathbf{b}_2,\\mathbf{b}_3}) = \\czero(\\color{blue}{w_{0,0},w_{0,1},\\ldots,w_{1,0},w_{1,1}},\\ldots,\\color{purple}{b_{0,0},b_{0,1}},\\ldots) \n$$\n\nTo abstract this away, we can write it out as:\n\n$$\n\\czero (\\theta_1,\\theta_2,\\ldots,\\theta_i,\\ldots)\n$$\n\nWell, then one way we could check for correctness, is to calculate **the two sided difference** $\\Delta_i$: \n\n$$\n\\begin{align}\n\\Delta_i &= \\frac{\\czero (\\theta_1,\\theta_2,\\ldots,\\color{red}{\\theta_i+\\epsilon},\\ldots) - \\czero (\\theta_1,\\theta_2,\\ldots,\\color{red}{\\theta_i-\\epsilon},\\ldots)}{2\\epsilon} \n\\end{align}\n$$\n\nWe know this is an **approximation** for our desired partial when $\\epsilon$ is **very small**:\n\n$$\n\\Delta_i \\approx \\frac{\\dc}{\\partial \\theta_i}\n$$\n\nSo we can use this approximation to see how close it is to our analytical gradient! \n\nSpecifically, we want to calculate: \n\n$$\n\\frac{\\|\\Delta_i - \\frac{\\dc}{\\partial \\theta_i}  \\|_2}{\\|\\Delta_i\\|_2 + \\|\\frac{\\dc}{\\partial \\theta_i}\\|_2  }\n$$\n\nAnd verify that this is **very small** for all of our parameters! Specifically, we can choose $\\epsilon = 10^{-7}$, and expect this difference to be $<10^{-7}$. If so, we can call our backprop code:\n\n<center><strong>numerically verified‚Ñ¢ üòé</strong></center>\n\nA note before we proceed, its very common to use the `ravel` function to do this *flattening out*, and then use `concatenate` to join our newly flattened arrays\"\n\n::: {.cell execution_count=68}\n``` {.python .cell-code}\na,b = np.random.randint(1,10,(3,2)),np.random.randint(1,10,(4,3))\na,b\n```\n\n::: {.cell-output .cell-output-display execution_count=68}\n```\n(array([[2, 5],\n        [6, 1],\n        [3, 8]]),\n array([[3, 5, 8],\n        [8, 1, 1],\n        [3, 9, 6],\n        [7, 5, 4]]))\n```\n:::\n:::\n\n\n::: {.cell execution_count=69}\n``` {.python .cell-code}\na.ravel()\n```\n\n::: {.cell-output .cell-output-display execution_count=69}\n```\narray([2, 5, 6, 1, 3, 8])\n```\n:::\n:::\n\n\n::: {.cell execution_count=70}\n``` {.python .cell-code}\nnp.concatenate((a.ravel(),b.ravel()))\n```\n\n::: {.cell-output .cell-output-display execution_count=70}\n```\narray([2, 5, 6, 1, 3, 8, 3, 5, 8, 8, 1, 1, 3, 9, 6, 7, 5, 4])\n```\n:::\n:::\n\n\nSo now, we can use these to flatten all of our parameters, and join them into one big vector which we can then **perturb**. \n\nLets define a couple helper functions which will help us **unravel** and then **reform** them.\n\nThe first is `get_params`, which will return a single vector of all our parameters, using python's **list comprehension**:\n\n::: {.cell execution_count=71}\n``` {.python .cell-code}\ndef get_params(weights,biases):\n    weight_params = [weight.ravel() for weight in weights]\n    bias_params = [bias.ravel() for bias in biases]\n    all_params = weight_params+bias_params\n    return np.concatenate(all_params)\n```\n:::\n\n\n::: {.cell execution_count=72}\n``` {.python .cell-code}\nget_params(weights,biases)\n```\n\n::: {.cell-output .cell-output-display execution_count=72}\n```\narray([ 0.14188865,  0.63484706,  0.12494954,  0.12396009,  0.84370276,\n        0.35345227,  0.92243417,  0.46469144,  0.24677724,  0.86751385,\n        0.10747816,  0.66656228,  0.15417883,  0.02532289,  0.60376072,\n        0.04574494, -1.0101568 , -0.64885254,  0.78366491,  0.34014546,\n        0.54846053,  0.11253149,  0.44440565,  0.58982193,  0.43253046])\n```\n:::\n:::\n\n\nVerify that this is of the correct length, based on our network structure! \n\n::: {.cell execution_count=73}\n``` {.python .cell-code}\nget_params(weights,biases).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=73}\n```\n(25,)\n```\n:::\n:::\n\n\nWe can also define a `set_params` function, which is responsible for **re-forming** our parameters into the correct shapes, given one long vector:\n\n::: {.cell execution_count=74}\n``` {.python .cell-code}\ndef set_params(param_vector,orig_weights,orig_biases):\n    # set weights\n    new_weights=[]\n    pointer = 0\n    for weight_mat in orig_weights:\n        curr_len = (weight_mat.shape[0]*weight_mat.shape[1])\n        new_mat = param_vector[pointer:pointer+curr_len].reshape(weight_mat.shape)\n        new_weights.append(new_mat)\n        pointer += curr_len\n    \n    # set biases\n    new_biases=[]\n    for weight_vec in orig_biases:\n        curr_len = (weight_vec.shape[0]*weight_vec.shape[1])\n        new_vec = param_vector[pointer:pointer+curr_len].reshape(weight_vec.shape)\n        new_biases.append(new_vec)\n        pointer += curr_len\n        \n    return new_weights,new_biases\n```\n:::\n\n\nNote: This function takes an original set of weights and biases, but only uses it for the sizes of each element! \n\nLets run both of our new functions and **verify** they work as expected:\n\n::: {.cell execution_count=75}\n``` {.python .cell-code}\nnew_weights,new_biases = set_params(get_params(weights,biases),weights,biases)\n```\n:::\n\n\nNow we can once again use list comprehension to quickly check if `new_weights` is equal to `weights`, and `new_biases` is equal to `biases` at each element:\n\n::: {.cell execution_count=76}\n``` {.python .cell-code}\n[np.array_equal(i,j) for i,j in zip(weights,new_weights)]\n```\n\n::: {.cell-output .cell-output-display execution_count=76}\n```\n[True, True, True]\n```\n:::\n:::\n\n\n::: {.cell execution_count=77}\n``` {.python .cell-code}\n[np.array_equal(i,j) for i,j in zip(biases,new_biases)]\n```\n\n::: {.cell-output .cell-output-display execution_count=77}\n```\n[True, True, True]\n```\n:::\n:::\n\n\nWe should see all `True` there! Perfect, our unraveling and raveling works as we expect. Now we can use this to perform gradient checking.\n\n### Implementation\n\nWe can now begin to implement our **numerical gradient checking**, by defining a function that lets us do this:\n\n::: {.cell execution_count=78}\n``` {.python .cell-code}\ndef gradient_checking(weights,biases,batch,idx,current_label,weight_updates,bias_updates,epsilon=1e-7):\n\n    # unravel the current params\n    params = get_params(weights,biases)\n    diff_vec = np.zeros_like(params)\n\n    # loop through, perturbing one parameter at a time:\n    for p_index in range(params.shape[0]):\n        perturb = np.zeros_like(params)\n        perturb[p_index] = epsilon\n\n        # feedforward at each side\n        _, activations_p = feedforward_batch(batch,*set_params(params + perturb,weights,biases))\n        _, activations_m = feedforward_batch(batch,*set_params(params - perturb,weights,biases))\n\n        # calculate cost of each side\n        cost_plus = cost_mse(activations_p[-1][:,idx],current_label)\n        cost_minus = cost_mse(activations_m[-1][:,idx],current_label)\n\n        # calcualte \\Delta_i\n        diff_vec[p_index] = (cost_plus - cost_minus)/(2*epsilon)\n\n    # calculate the difference for this round of backprop\n    grad_vec = get_params(weight_updates, bias_updates)\n    grad_error = np.linalg.norm(diff_vec - grad_vec)/(np.linalg.norm(diff_vec) + np.linalg.norm(grad_vec))\n   \n    if grad_error > epsilon:\n        print(\"Error in gradient calculation!\")\n   \n    return grad_error\n```\n:::\n\n\nThis function takes the following:\n\n* `weights`: the current weights of our network\n* `biases`: the current biases of our network \n* `batch`: the current batch of data\n* `idx`: the index of our current sample,\n* `current_label`: the current label, of the **current sample**\n* `weight_updates`: the list of gradients of the weights we calculated\n* `bias_updates`: the list of gradients of the biases we calculated\n* `epsilon=1e-7`: a default epsilon value\n\nIts a bit verbose, but thats because we're going for a flat implementation. Once we've understood all the pieces, we can wrap everything up nice and neat. \n\nWith this, lets re-run our training but this time perform gradient checking! \n\n::: {.cell execution_count=79}\n``` {.python .cell-code}\ndef backprop_1_example1(activities,activations,y,quiet=True):\n    my_bp['act'].append(activations)\n    \n    layer_num = len(activations)\n    weight_updates = []\n    bias_updates = []\n    \n    ########################################\n    #           Last Level\n    ########################################\n    # get last layer specific variables\n    a_L = activations[-1].reshape(-1,1)\n    a_L_minus_1 = activations[-2].reshape(-1,1)\n    z_L = activities[-1].reshape(-1,1)\n    \n    # calculate delta_L for the last level\n    delta_L = (a_L-y)*sigmoid_prime(z_L)\n    my_bp['delta'].append(delta_L)\n    \n    deltas = []\n    deltas.append(delta_L)\n\n    # calculate bias update\n    dc_db_L = delta_L\n    bias_updates = [dc_db_L] + bias_updates\n\n    # calcualte weight updates\n    dc_dw_L = delta_L.dot((a_L_minus_1).T)\n    my_bp['w'].append(dc_dw_L)\n    weight_updates = [dc_dw_L] + weight_updates\n    \n    if not quiet:\n        print(f'=========Layer:{layer_num+-1}=========')\n        print(f'Using negative index: -1')\n        print(f'z_l:{z_L.shape}, a_l:{a_L.shape}, a_l_minus_1:{a_L_minus_1.shape}')\n        print(f'delta_l:{delta_L.shape}, dc_db_l:{dc_db_L.shape},dc_dw_l:{dc_dw_L.shape}')\n        print()\n    \n    ########################################\n    # loop through each layer, from 2 to L-1\n    ######################################## \n    for layer in range(2,layer_num):\n        # using level as a **negative index**\n        l = -layer\n        \n        # uncomment this print statement \n        # to help you understand how negative indexing works\n        # print(f\"Loop variable: {layer}, l={l}, corresponding to layer: l{l}\")\n\n        # calculate delta_l for each layer\n        a_l = activations[l].reshape(-1,1)\n        z_l = activities[l].reshape(-1,1)\n        delta_l = weights[l+1].T.dot(deltas[l+1]) * sigmoid_prime(z_l)\n        my_bp['delta'].append(delta_l)\n        deltas = [delta_l] + deltas\n\n        # calculate bias update\n        dc_db_l = delta_l\n        bias_updates = [dc_db_l] + bias_updates\n\n        # calcualte weight updates\n        a_l_minus_1 = activations[l-1].reshape(-1,1)\n        dc_dw_l = delta_l.dot((a_l_minus_1).T)\n        my_bp['w'].append(dc_dw_l)\n        weight_updates = [dc_dw_l] + weight_updates\n        \n        if not quiet:\n            print(f'=========Layer:{layer_num+l}=========')\n            print(f'Using negative index: {l}')\n            print(f'z_l:{z_l.shape}, a_l:{a_l.shape}, a_l_minus_1:{a_l_minus_1.shape}')\n            print(f'delta_l:{delta_l.shape}, dc_db_l:{dc_db_l.shape},dc_dw_l:{dc_dw_l.shape}')\n            print()\n    \n    return weight_updates, bias_updates\n```\n:::\n\n\n::: {.cell execution_count=80}\n``` {.python .cell-code}\n# dataset\nN = X.shape[0]\nd = X.shape[1]\nbatch_size = 50\nepochs = 10\neta=1e-2\ndataset=X\nlabels=y\nquiet=False\n\n# gradient checking\nepsilon = 1e-7\n\n# initialize weight matrices - notice the dimensions\nW_1_init = np.random.rand(3,2)\nW_2_init = np.random.rand(3,3)\nW_3_init = np.random.randn(1,3)\nweights = [W_1_init,W_2_init,W_3_init]\n\n# and our biases\nb_1_init = np.random.rand(3,1)\nb_2_init = np.random.rand(3,1)\nb_3_init = np.random.rand(1,1)\nbiases = [b_1_init,b_2_init,b_3_init]\n\n# network\nW_1,W_2,W_3 = weights[0],weights[1],weights[2]\nb_1,b_2,b_3 = biases[0],biases[1],biases[2]\n\n# mini-batch params \nnum_batches = int(N/batch_size)\niterations = 0\n\n# various debugging lists\nloss = []\ngrad_norm_1 = []\ngrad_norm_2 = []\ngrad_norm_3 = []\nnorm_1 = []\nnorm_2 = []\nnorm_3 = []\ngrad_errors = []\ndiff_vecs = []\ngrad_vecs = []\nmy_act = []\nmy_w = []\nmy_b = []\nmy_bp = {'x':[],'y':[],'act':[],'delta':[],'w':[]}\n\nfor epoch in range(epochs):\n    # shuffle the dataset - note we do this by shuffling indices\n#     indices = np.random.permutation(N)\n#     shuffled_dataset = dataset[indices,:]\n#     shuffled_labels = labels[indices,:]\n    shuffled_dataset = dataset\n    shuffled_labels = labels\n        \n    # work on current batch \n    for batch_num in range(num_batches):\n\n        # get current batch\n        batch_start = batch_num*batch_size\n        batch_end = (batch_num+1)*batch_size\n        batch = shuffled_dataset[batch_start:batch_end,:]\n        batch_labels = shuffled_labels[batch_start:batch_end,:].reshape(-1,1)\n\n        # feedforward on batch\n        activities, activations = feedforward_batch(batch, weights,biases)\n        my_act.append(activations[-1])\n\n        # setup matrices to hold gradients\n        grad_W_1 = np.zeros_like(W_1)\n        grad_b_1 = np.zeros_like(b_1)\n        grad_W_2 = np.zeros_like(W_2)\n        grad_b_2 = np.zeros_like(b_2)\n        grad_W_3 = np.zeros_like(W_3)\n        grad_b_3 = np.zeros_like(b_3)\n        \n\n        # loop through each example in the batch\n        for idx in range(batch.shape[0]):\n            current_sample = batch[idx,:].reshape(1,-1)\n            current_label = batch_labels[idx]\n            my_bp['x'].append(current_sample)\n            my_bp['y'].append(current_label)\n            \n            curr_act, curr_activ = feedforward_batch(current_sample, weights,biases)\n            \n            # get current weight and bias updates\n            weight_updates, bias_updates = backprop_1_example1(curr_act,curr_activ,\n                                                              current_label)\n            my_w.append(weight_updates)\n            my_b.append(bias_updates)\n\n            # perform gradient cehcking\n#             grad_error = gradient_checking(weights,biases,batch,idx,current_label,\n#                                            weight_updates,bias_updates,epsilon)\n#             grad_errors.append(grad_error)\n            \n            # aggregate our updates\n            grad_W_1 = grad_W_1 + weight_updates[0]\n            grad_b_1 = grad_b_1 + bias_updates[0]\n\n            grad_W_2 = grad_W_2 + weight_updates[1]\n            grad_b_2 = grad_b_2 + bias_updates[1]\n\n            grad_W_3 = grad_W_3 + weight_updates[2]\n            grad_b_3 = grad_b_3 + bias_updates[2]\n            \n            \n        grad_norm_1.append(np.linalg.norm(grad_W_1))\n        grad_norm_2.append(np.linalg.norm(grad_W_2))\n        grad_norm_3.append(np.linalg.norm(grad_W_3))\n\n\n        # take your steps:\n        W_1 = W_1 - eta/batch_size*(grad_W_1)\n        b_1 = b_1 - eta/batch_size*(grad_b_1)\n\n        W_2 = W_2 - eta/batch_size*(grad_W_2)\n        b_2 = b_2 - eta/batch_size*(grad_b_2)\n\n        W_3 = W_3 - eta/batch_size*(grad_W_3)\n        b_3 = b_3 - eta/batch_size*(grad_b_3)\n        \n        norm_1.append(np.linalg.norm(W_1))\n        norm_2.append(np.linalg.norm(W_2))\n        norm_3.append(np.linalg.norm(W_3))\n        \n        # save new weights and biases to be used in the next feedforward step\n        weights = [W_1,W_2,W_3]\n        biases = [b_1,b_2,b_3]\n        \n  \n    \n    print(f\"Epoch:{epoch+1}/{epochs}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:1/10\nEpoch:2/10\nEpoch:3/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:4/10\nEpoch:5/10\nEpoch:6/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:7/10\nEpoch:8/10\nEpoch:9/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:10/10\n```\n:::\n:::\n\n\nNow, lets plot our estimated **gradient errors**:\n\n::: {.cell execution_count=81}\n``` {.python .cell-code}\nplt.plot(grad_errors);\n```\n\n::: {.cell-output .cell-output-display}\n![](nn_files/figure-html/cell-82-output-1.png){width=590 height=411}\n:::\n:::\n\n\nWow! Look at that y axis! Pretty good! We cal also verify:\n\n::: {.cell execution_count=82}\n``` {.python .cell-code}\n(np.array(grad_errors) < epsilon).all()\n```\n\n::: {.cell-output .cell-output-display execution_count=82}\n```\nTrue\n```\n:::\n:::\n\n\nPerfect! Our implementation of backprop is now completely: \n\n<center><strong>numerically verified‚Ñ¢ üòé</strong></center>\n\n\n## Compare to Nielsen implementation:\n\n::: {.cell execution_count=83}\n``` {.python .cell-code}\nclass Network(object):\n\n    def __init__(self, sizes, weights=None, biases=None):\n        \"\"\"The list ``sizes`` contains the number of neurons in the\n        respective layers of the network.  For example, if the list\n        was [2, 3, 1] then it would be a three-layer network, with the\n        first layer containing 2 neurons, the second layer 3 neurons,\n        and the third layer 1 neuron.  The biases and weights for the\n        network are initialized randomly, using a Gaussian\n        distribution with mean 0, and variance 1.  Note that the first\n        layer is assumed to be an input layer, and by convention we\n        won't set any biases for those neurons, since biases are only\n        ever used in computing the outputs from later layers.\"\"\"\n        self.num_layers = len(sizes)\n        self.sizes = sizes\n        if weights is None:\n            self.weights = [np.random.randn(y, x)\n                            for x, y in zip(sizes[:-1], sizes[1:])]\n            print(\"random weights\")\n        else:\n            self.weights = weights\n\n        if biases is None:\n            self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n            print(\"random biases\")\n        else:\n            self.biases = biases\n\n    def feedforward(self, a):\n        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n        for b, w in zip(self.biases, self.weights):\n            a = sigmoid(np.dot(w, a)+b)\n        return a\n\n    def SGD(self, training_data, epochs, mini_batch_size, eta,\n            test_data=None):\n        \"\"\"Train the neural network using mini-batch stochastic\n        gradient descent.  The ``training_data`` is a list of tuples\n        ``(x, y)`` representing the training inputs and the desired\n        outputs.  The other non-optional parameters are\n        self-explanatory.  If ``test_data`` is provided then the\n        network will be evaluated against the test data after each\n        epoch, and partial progress printed out.  This is useful for\n        tracking progress, but slows things down substantially.\"\"\"\n        if test_data: n_test = len(test_data)\n        n = len(training_data)\n        for j in range(epochs):\n#             random.shuffle(training_data)\n            mini_batches = [\n                training_data[k:k+mini_batch_size]\n                for k in range(0, n, mini_batch_size)]\n            for mini_batch_idx in range(len(mini_batches)):\n                mini_batch = mini_batches[mini_batch_idx]\n                self.update_mini_batch(mini_batch, eta)\n            if test_data:\n                print(\"Epoch {0}: {1} / {2}\".format(\n                    j, self.evaluate(test_data), n_test))\n            else:\n                print(\"Epoch {0} complete\".format(j))\n\n    def update_mini_batch(self, mini_batch, eta):\n        \"\"\"Update the network's weights and biases by applying\n        gradient descent using backpropagation to a single mini batch.\n        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n        is the learning rate.\"\"\"\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        for x, y in mini_batch:\n            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n            nielsen_w.append(delta_nabla_w)\n            nielsen_b.append(delta_nabla_b)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n        self.weights = [w-(eta/len(mini_batch))*nw\n                        for w, nw in zip(self.weights, nabla_w)]\n        self.biases = [b-(eta/len(mini_batch))*nb\n                       for b, nb in zip(self.biases, nabla_b)]\n\n    def backprop(self, x, y):\n        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n        gradient for the cost function C_x.  ``nabla_b`` and\n        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n        to ``self.biases`` and ``self.weights``.\"\"\"\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        nielsen_bp['x'].append(x)\n        nielsen_bp['y'].append(y)\n        \n        # feedforward\n        activation = x\n        activations = [x] # list to store all the activations, layer by layer\n        zs = [] # list to store all the z vectors, layer by layer\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation)+b\n            zs.append(z)\n            activation = sigmoid(z)\n            activations.append(activation)\n        nielsen_bp['act'].append(activations)\n        # backward pass\n        delta = self.cost_derivative(activations[-1], y) * \\\n            sigmoid_prime(zs[-1])\n        nielsen_bp['delta'].append(delta)\n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n        nielsen_bp['w'].append(nabla_w[-1])\n        # Note that the variable l in the loop below is used a little\n        # differently to the notation in Chapter 2 of the book.  Here,\n        # l = 1 means the last layer of neurons, l = 2 is the\n        # second-last layer, and so on.  It's a renumbering of the\n        # scheme in the book, used here to take advantage of the fact\n        # that Python can use negative indices in lists.\n        for l in range(2, self.num_layers):\n            z = zs[-l]\n            sp = sigmoid_prime(z)\n            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n            nabla_b[-l] = delta\n            nielsen_bp['delta'].append(delta)\n            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n            nielsen_bp['w'].append(nabla_w[-1])\n        return (nabla_b, nabla_w)\n\n    def evaluate(self, test_data):\n        \"\"\"Return the number of test inputs for which the neural\n        network outputs the correct result. Note that the neural\n        network's output is assumed to be the index of whichever\n        neuron in the final layer has the highest activation.\"\"\"\n        test_results = [(np.argmax(self.feedforward(x)), y)\n                        for (x, y) in test_data]\n        return sum(int(x == y) for (x, y) in test_results)\n\n    def cost_derivative(self, output_activations, y):\n        \"\"\"Return the vector of partial derivatives \\partial C_x /\n        \\partial a for the output activations.\"\"\"\n        return (output_activations-y)\n\n#### Miscellaneous functions\ndef sigmoid(z):\n    \"\"\"The sigmoid function.\"\"\"\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    \"\"\"Derivative of the sigmoid function.\"\"\"\n    return sigmoid(z)*(1.0-sigmoid(z))\n```\n:::\n\n\n::: {.cell execution_count=84}\n``` {.python .cell-code}\nnielsen_w = []\nnielsen_b = []\nnielsen_bp = {'x':[],'y':[],'act':[],'delta':[],'w':[]}\n\nnet = Network([2,3,3,1],weights=[W_1_init, W_2_init, W_3_init],\n              biases=[b_1_init, b_2_init, b_3_init])\n```\n:::\n\n\n::: {.cell execution_count=85}\n``` {.python .cell-code}\ny_nielsen = net.feedforward(X.T) \n_,act = feedforward_batch(X,weights=[W_1_init, W_2_init, W_3_init],\n                           biases=[b_1_init, b_2_init, b_3_init])\ny_mine = act[-1]\n```\n:::\n\n\n::: {.cell execution_count=86}\n``` {.python .cell-code}\ny_nielsen[:,1:10]\n```\n\n::: {.cell-output .cell-output-display execution_count=86}\n```\narray([[0.74194328, 0.74151133, 0.74129913, 0.74195282, 0.74186864,\n        0.74195579, 0.74205958, 0.74252934, 0.74216293]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=87}\n``` {.python .cell-code}\ny_mine[:,1:10]\n```\n\n::: {.cell-output .cell-output-display execution_count=87}\n```\narray([[0.74194328, 0.74151133, 0.74129913, 0.74195282, 0.74186864,\n        0.74195579, 0.74205958, 0.74252934, 0.74216293]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=88}\n``` {.python .cell-code}\n((y_nielsen - y_mine)<0.001).all()\n```\n\n::: {.cell-output .cell-output-display execution_count=88}\n```\nTrue\n```\n:::\n:::\n\n\n::: {.cell execution_count=89}\n``` {.python .cell-code}\nnp.array_equal(y_nielsen,y_mine)\n```\n\n::: {.cell-output .cell-output-display execution_count=89}\n```\nTrue\n```\n:::\n:::\n\n\nThey are the same before training with he same weights, so the problem isnt in feedforward.\n\n::: {.cell execution_count=90}\n``` {.python .cell-code}\nnet.SGD([(lx.reshape(-1,1),ly.reshape(-1,1)) for lx,ly in zip(X,y)], epochs,batch_size,eta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 0 complete\nEpoch 1 complete\nEpoch 2 complete\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 3 complete\nEpoch 4 complete\nEpoch 5 complete\nEpoch 6 complete\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 7 complete\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 8 complete\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 9 complete\n```\n:::\n:::\n\n\n::: {.cell execution_count=91}\n``` {.python .cell-code}\nnet.weights\n```\n\n::: {.cell-output .cell-output-display execution_count=91}\n```\n[array([[0.04080823, 0.37448822],\n        [0.15191249, 0.71274118],\n        [0.59873365, 0.67791594]]),\n array([[0.32794289, 0.65351068, 0.1330368 ],\n        [0.58658677, 0.03377164, 0.5889434 ],\n        [0.45534099, 0.49025834, 0.24421904]]),\n array([[ 0.84503569, -0.30390145,  0.36283462]])]\n```\n:::\n:::\n\n\n::: {.cell execution_count=92}\n``` {.python .cell-code}\nweights\n```\n\n::: {.cell-output .cell-output-display execution_count=92}\n```\n[array([[0.04080823, 0.37448822],\n        [0.15191249, 0.71274118],\n        [0.59873365, 0.67791594]]),\n array([[0.32794289, 0.65351068, 0.1330368 ],\n        [0.58658677, 0.03377164, 0.5889434 ],\n        [0.45534099, 0.49025834, 0.24421904]]),\n array([[ 0.84503569, -0.30390145,  0.36283462]])]\n```\n:::\n:::\n\n\n::: {.cell execution_count=93}\n``` {.python .cell-code}\n[np.array_equal(i,j) for i,j in zip(weights,net.weights)]\n```\n\n::: {.cell-output .cell-output-display execution_count=93}\n```\n[True, True, True]\n```\n:::\n:::\n\n\n::: {.cell execution_count=94}\n``` {.python .cell-code}\n[np.array_equal(i,j) for i,j in zip(biases,net.biases)]\n```\n\n::: {.cell-output .cell-output-display execution_count=94}\n```\n[True, True, True]\n```\n:::\n:::\n\n\n## Fully vectorized implementation\n\nOk! We've done a lot of hard work! Now we've come to the final linear algebraic thing we need to do to make this fast üèéüí®.\n\nWe want to be able to backprop **across batches**, instead of across individual samples in a batch. \n\nTo do so, we will have to introduce **3D matrices!**. In math these are called **tensors**! \n\n<div class=\"info\">\n    \nüìñ **Definition**: For our purposes, a **tensor** is defined as a multidimensional matrix. \n    \n**Note:** In python, these objects are called \"N-D arrays\" or just \"arrays\". Vectors, and matrices are just 1D and 2D versions of these tensors/arrays. \n\n</div>\n\nLets create one right now!\n\n::: {.cell execution_count=95}\n``` {.python .cell-code}\na = np.random.randint(1,10,(3,3,3))\na\n```\n\n::: {.cell-output .cell-output-display execution_count=95}\n```\narray([[[9, 7, 3],\n        [5, 4, 2],\n        [7, 7, 8]],\n\n       [[2, 7, 1],\n        [7, 3, 3],\n        [7, 9, 3]],\n\n       [[4, 6, 9],\n        [5, 4, 2],\n        [2, 8, 5]]])\n```\n:::\n:::\n\n\n<font size=\"20\">ü§î</font>\n\nThis doesn't look that special. Thats just because were printing this 3D object on a 2D notebook. Pay special attention to the `[` and `]` characters and you'll notice its almost like its a list of a 2D matrices. Indeed, for right now, we can just think of it like that.\n\nLets grab the first matrix in the list:\n\n::: {.cell execution_count=96}\n``` {.python .cell-code}\na[0,:,:]\n```\n\n::: {.cell-output .cell-output-display execution_count=96}\n```\narray([[9, 7, 3],\n       [5, 4, 2],\n       [7, 7, 8]])\n```\n:::\n:::\n\n\nAnd the second:\n\n::: {.cell execution_count=97}\n``` {.python .cell-code}\na[1,:,:]\n```\n\n::: {.cell-output .cell-output-display execution_count=97}\n```\narray([[2, 7, 1],\n       [7, 3, 3],\n       [7, 9, 3]])\n```\n:::\n:::\n\n\nAnd the last:\n\n::: {.cell execution_count=98}\n``` {.python .cell-code}\na[-1,:,:]\n```\n\n::: {.cell-output .cell-output-display execution_count=98}\n```\narray([[4, 6, 9],\n       [5, 4, 2],\n       [2, 8, 5]])\n```\n:::\n:::\n\n\nOk, so far so good. Lets understand one more thing we need to do with these objects.\n\nWe want to be able to take this list/stack of matrices, and multiply the stack with another vector matrix. In other words, if I have a tensor `T`, which contains four matrices which we will call `A`, `B`, `C`, and `D` I want to be able to do something like:\n\n    T*v = R\n\nwhere `R` is now a stack which contains `Av`,`Bv`, `Cv`, `Dv`. \n\nAnd then at the end, I can **average across the stack dimension**, to produce one final 2D matrix which is just: \n\n    (Av+Bv+Cv+Dv)/4\n\nOk. We've described in words the procedure we want to perform, so lets go about coding it up! To make this clear, lets define our tensor T as we described above:\n\n::: {.cell execution_count=99}\n``` {.python .cell-code}\nA = np.ones((3,3))\nA\n```\n\n::: {.cell-output .cell-output-display execution_count=99}\n```\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=100}\n``` {.python .cell-code}\nB = np.ones((3,3))*2\nB\n```\n\n::: {.cell-output .cell-output-display execution_count=100}\n```\narray([[2., 2., 2.],\n       [2., 2., 2.],\n       [2., 2., 2.]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=101}\n``` {.python .cell-code}\nC = np.ones((3,3))*3\nC\n```\n\n::: {.cell-output .cell-output-display execution_count=101}\n```\narray([[3., 3., 3.],\n       [3., 3., 3.],\n       [3., 3., 3.]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=102}\n``` {.python .cell-code}\nD = np.ones((3,3))*4\nD\n```\n\n::: {.cell-output .cell-output-display execution_count=102}\n```\narray([[4., 4., 4.],\n       [4., 4., 4.],\n       [4., 4., 4.]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=103}\n``` {.python .cell-code}\nT = np.zeros((4,3,3))\nT[0,:,:] = A\nT[1,:,:] = B\nT[2,:,:] = C\nT[3,:,:] = D\nT.shape,T\n```\n\n::: {.cell-output .cell-output-display execution_count=103}\n```\n((4, 3, 3),\n array([[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n \n        [[2., 2., 2.],\n         [2., 2., 2.],\n         [2., 2., 2.]],\n \n        [[3., 3., 3.],\n         [3., 3., 3.],\n         [3., 3., 3.]],\n \n        [[4., 4., 4.],\n         [4., 4., 4.],\n         [4., 4., 4.]]]))\n```\n:::\n:::\n\n\nNote: we could have also used `np.stack` to do this for us:\n\n::: {.cell execution_count=104}\n``` {.python .cell-code}\nnp.stack((A,B,C,D)).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=104}\n```\n(4, 3, 3)\n```\n:::\n:::\n\n\nAnd now lets create that vector we wanted:\n\n::: {.cell execution_count=105}\n``` {.python .cell-code}\nv = np.ones((3,1))\nv\n```\n\n::: {.cell-output .cell-output-display execution_count=105}\n```\narray([[1.],\n       [1.],\n       [1.]])\n```\n:::\n:::\n\n\nand verify how it interacts with each matrix in our stack:\n\n::: {.cell execution_count=106}\n``` {.python .cell-code}\nA.dot(v)\n```\n\n::: {.cell-output .cell-output-display execution_count=106}\n```\narray([[3.],\n       [3.],\n       [3.]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=107}\n``` {.python .cell-code}\nB.dot(v)\n```\n\n::: {.cell-output .cell-output-display execution_count=107}\n```\narray([[6.],\n       [6.],\n       [6.]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=108}\n``` {.python .cell-code}\nC.dot(v)\n```\n\n::: {.cell-output .cell-output-display execution_count=108}\n```\narray([[9.],\n       [9.],\n       [9.]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=109}\n``` {.python .cell-code}\nD.dot(v)\n```\n\n::: {.cell-output .cell-output-display execution_count=109}\n```\narray([[12.],\n       [12.],\n       [12.]])\n```\n:::\n:::\n\n\nOk! So now we know what we want `R` to look like. It should be the stack of those vectors. \n\nWe can create this using the `matmul` function, which stands for **matrix multiply**. We can check its documentation:\n\n::: {.cell execution_count=110}\n``` {.python .cell-code}\n# np.matmul?\n```\n:::\n\n\nThe line that we are interested in is the following:\n\n    If either argument is N-D, N > 2, it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly.\n\nThats exactly what we want! So lets use it!\n\n::: {.cell execution_count=111}\n``` {.python .cell-code}\nR = np.matmul(T,v)\nR\n```\n\n::: {.cell-output .cell-output-display execution_count=111}\n```\narray([[[ 3.],\n        [ 3.],\n        [ 3.]],\n\n       [[ 6.],\n        [ 6.],\n        [ 6.]],\n\n       [[ 9.],\n        [ 9.],\n        [ 9.]],\n\n       [[12.],\n        [12.],\n        [12.]]])\n```\n:::\n:::\n\n\nPerfect! Thats what we expected! Lets stop and realize this:\n\nYou can think of this one function as having handled the three separate multiplications for us! In one function! And it did so **as fast as possible** because its relying on the underlying numerical libraries! \n\n**Hint:** In a few cells from now, we will see this is exactly what we want to do for our batch! \n\nThe `matmul` function is also represented by the handy `@` symbol, meaning we could also just type:\n\n::: {.cell execution_count=112}\n``` {.python .cell-code}\nR = T@v\nR\n```\n\n::: {.cell-output .cell-output-display execution_count=112}\n```\narray([[[ 3.],\n        [ 3.],\n        [ 3.]],\n\n       [[ 6.],\n        [ 6.],\n        [ 6.]],\n\n       [[ 9.],\n        [ 9.],\n        [ 9.]],\n\n       [[12.],\n        [12.],\n        [12.]]])\n```\n:::\n:::\n\n\nNote: `matmul/@` work as we expect in simpler cases:\n\n::: {.cell execution_count=113}\n``` {.python .cell-code}\na,b = np.random.randint(1,10,(3,2)),np.random.randint(1,10,(2,3))\na,b\n```\n\n::: {.cell-output .cell-output-display execution_count=113}\n```\n(array([[3, 2],\n        [7, 1],\n        [2, 3]]),\n array([[6, 8, 5],\n        [4, 4, 1]]))\n```\n:::\n:::\n\n\n::: {.cell execution_count=114}\n``` {.python .cell-code}\na@b, a.dot(b)\n```\n\n::: {.cell-output .cell-output-display execution_count=114}\n```\n(array([[26, 32, 17],\n        [46, 60, 36],\n        [24, 28, 13]]),\n array([[26, 32, 17],\n        [46, 60, 36],\n        [24, 28, 13]]))\n```\n:::\n:::\n\n\nOne final thing which might be useful, is the `einsum` function:\n\n::: {.cell execution_count=115}\n``` {.python .cell-code}\n# np.einsum?\n```\n:::\n\n\nThis is a very powerful function, but we can use it in a straightforward manner to just describe to numpy the shapes of each of our matrices, and the **shape we want** afterwards! \n\nLets revisit the T/R example above\n\n::: {.cell execution_count=116}\n``` {.python .cell-code}\nnp.einsum('ijk,kl->ikl',T,v)\n```\n\n::: {.cell-output .cell-output-display execution_count=116}\n```\narray([[[ 3.],\n        [ 3.],\n        [ 3.]],\n\n       [[ 6.],\n        [ 6.],\n        [ 6.]],\n\n       [[ 9.],\n        [ 9.],\n        [ 9.]],\n\n       [[12.],\n        [12.],\n        [12.]]])\n```\n:::\n:::\n\n\n<font size=20>ü§Ø</font>\n\nWhat?! What just happened! Well, we just described the operation we wanted directly in terms of sizes using the **Einstein summation convention**.\n\nWe said `T` is of size `i x j x k`, and that `v` is of size `k x l`, and I want to  turn this into something that is of size `i x k x l`. And thats exactly what we got! \n\nThe last thing we want to do is \"collapse\" across one of our dimensions to form the average. This is easy in numpy, we just tell numpy across what **axis** we want to average:\n\n::: {.cell execution_count=117}\n``` {.python .cell-code}\nR = T@v\nR\n```\n\n::: {.cell-output .cell-output-display execution_count=117}\n```\narray([[[ 3.],\n        [ 3.],\n        [ 3.]],\n\n       [[ 6.],\n        [ 6.],\n        [ 6.]],\n\n       [[ 9.],\n        [ 9.],\n        [ 9.]],\n\n       [[12.],\n        [12.],\n        [12.]]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=118}\n``` {.python .cell-code}\nR.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=118}\n```\n(4, 3, 1)\n```\n:::\n:::\n\n\n::: {.cell execution_count=119}\n``` {.python .cell-code}\nnp.mean(R,axis=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=119}\n```\narray([[7.5],\n       [7.5],\n       [7.5]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=120}\n``` {.python .cell-code}\nnp.mean(R,axis=0).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=120}\n```\n(3, 1)\n```\n:::\n:::\n\n\nIt worked! We told numpy to take the average across the first dimension (`axis=0`), so the result is the `3x1` matrix/vector which is just the average of all of the members of our stack! \n\n---\n\nLets point out one more potentially confusing aspect of our implementations. Lets take an activation matrix as our example. The tranpose of this matrix is of size `batch_size x nodes in our layer`. \n\nWe've been using this 2D matrix to represent our collection of vectors, implicitly. \n\nWe could make this more explicit, by adding on the missing dimension, making it: `batch_size x nodes in our layer x 1` - giving us a clearer interpretation of this as a **stack of vectors**. \n\n::: {.cell execution_count=121}\n``` {.python .cell-code}\na = np.random.randint(1,10,(10,3))\na\n```\n\n::: {.cell-output .cell-output-display execution_count=121}\n```\narray([[9, 4, 3],\n       [6, 1, 5],\n       [4, 3, 6],\n       [2, 5, 9],\n       [1, 6, 2],\n       [7, 7, 6],\n       [4, 5, 4],\n       [3, 6, 9],\n       [6, 3, 2],\n       [3, 1, 6]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=122}\n``` {.python .cell-code}\na = a[:,:,np.newaxis]\n```\n:::\n\n\nNow, `a` is a **stack of column vectors**! Lets grab the first vector:\n\n::: {.cell execution_count=123}\n``` {.python .cell-code}\na[0,:,:]\n```\n\n::: {.cell-output .cell-output-display execution_count=123}\n```\narray([[9],\n       [4],\n       [3]])\n```\n:::\n:::\n\n\n### Implementation\nThis is the final piece of the puzzle we need. We are now ready to use this in our backprop implementation! Lets look at our old implementation:\n\n::: {.cell execution_count=124}\n``` {.python .cell-code}\nactivities, activations = feedforward_batch(X,weights,biases)\n```\n:::\n\n\n::: {.cell execution_count=125}\n``` {.python .cell-code}\nweight_updates = []\nbias_updates = []\nlayer_num = len(activations)\n```\n:::\n\n\n::: {.cell execution_count=126}\n``` {.python .cell-code}\nfor activity in activities:\n    print(activity.shape) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(3, 1000)\n(3, 1000)\n(1, 1000)\n```\n:::\n:::\n\n\n::: {.cell execution_count=127}\n``` {.python .cell-code}\nfor activation in activations:\n    print(activation.shape) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(2, 1000)\n(3, 1000)\n(3, 1000)\n(1, 1000)\n```\n:::\n:::\n\n\nThis is like having a `batch_size` of $1000$. Lets continue! We want to move the batch size **to the front** so that its out of the way! \n\n::: {.cell execution_count=128}\n``` {.python .cell-code}\nfor activation in activations:\n    print(activation.T.shape) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1000, 2)\n(1000, 3)\n(1000, 3)\n(1000, 1)\n```\n:::\n:::\n\n\nIts to these that we want to add our \"new dimension\" to clearly make these a stack:\n\n::: {.cell execution_count=129}\n``` {.python .cell-code}\nfor activation in activations:\n    print((activation.T)[:,:,np.newaxis].shape) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1000, 2, 1)\n(1000, 3, 1)\n(1000, 3, 1)\n(1000, 1, 1)\n```\n:::\n:::\n\n\nThis is saying: I have a `batch_size`-many stack, composed of vectors, where each vector is ___ size.\n\nPerfect! Lets get cooking! \n\n::: {.cell execution_count=130}\n``` {.python .cell-code}\n########################################\n#           Last Level\n########################################\n# get last layer specific variables\nz_L = (activities[-1].T)[:,:,np.newaxis]\na_L = (activations[-1].T)[:,:,np.newaxis]\na_L_minus_1 = (activations[-2].T)[:,:,np.newaxis]\nz_L.shape, a_L.shape,a_L_minus_1.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=130}\n```\n((1000, 1, 1), (1000, 1, 1), (1000, 3, 1))\n```\n:::\n:::\n\n\n::: {.cell execution_count=131}\n``` {.python .cell-code}\n# calculate delta_L for the last level\ndelta_L = (a_L-y[:,:,np.newaxis])*sigmoid_prime(z_L)\ndelta_L.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=131}\n```\n(1000, 1, 1)\n```\n:::\n:::\n\n\n::: {.cell execution_count=132}\n``` {.python .cell-code}\ndeltas = []\ndeltas.append(delta_L)\n```\n:::\n\n\nHere, remember we need to collapse it down across the batch axis! \n\n::: {.cell execution_count=133}\n``` {.python .cell-code}\n# calculate bias update\ndc_db_l = delta_L.sum(axis=0)\nbias_updates = [dc_db_l] + bias_updates\ndc_db_l.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=133}\n```\n(1, 1)\n```\n:::\n:::\n\n\n::: {.cell execution_count=134}\n``` {.python .cell-code}\n# calcualte weight updates\ndc_dw_l = (delta_L @ a_L_minus_1.transpose(0,2,1)).sum(axis=0)\nweight_updates = [dc_dw_l] + weight_updates\ndc_dw_l.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=134}\n```\n(1, 3)\n```\n:::\n:::\n\n\n::: {.cell execution_count=135}\n``` {.python .cell-code}\n########################################\n# loop through each layer, from 2 to L-1\n######################################## \nfor layer in range(2,layer_num):\n    # using level as a **negative index**\n    l = -layer\n\n    # uncomment this print statement \n    # to help you understand how negative indexing works\n    print(f\"Loop variable: {layer}, l={l}, corresponding to layer: l{l}\")\n\n    # get layer values\n    z_l = (activities[l].T)[:,:,np.newaxis]\n    a_l = (activations[l].T)[:,:,np.newaxis]\n    a_l_minus_1 = (activations[l-1].T)[:,:,np.newaxis]\n    \n    # calculate delta_l for each layer\n    delta_l = (weights[l+1].T @ deltas[l+1]) * sigmoid_prime(z_l)\n    deltas = [delta_l] + deltas\n\n    # calculate bias update\n    dc_db_l = delta_l.sum(axis=0)\n    bias_updates = [dc_db_l] + bias_updates\n\n    # calcualte weight updates\n    dc_dw_l = (delta_l @ a_l_minus_1.transpose(0,2,1)).sum(axis=0)\n    weight_updates = [dc_dw_l] + weight_updates\n    \n    print(f'=========Layer:{layer_num+l}=========')\n    print(f'Using negative index: {l}')\n    print(f'z_l:{z_l.shape}, a_l:{a_l.shape}, a_l_minus_1:{a_l_minus_1.shape}')\n    print(f'delta_l:{delta_l.shape}, dc_db_l:{dc_db_l.shape}, dc_dw_l:{dc_dw_l.shape}')\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoop variable: 2, l=-2, corresponding to layer: l-2\n=========Layer:2=========\nUsing negative index: -2\nz_l:(1000, 3, 1), a_l:(1000, 3, 1), a_l_minus_1:(1000, 3, 1)\ndelta_l:(1000, 3, 1), dc_db_l:(3, 1), dc_dw_l:(3, 3)\n\nLoop variable: 3, l=-3, corresponding to layer: l-3\n=========Layer:1=========\nUsing negative index: -3\nz_l:(1000, 3, 1), a_l:(1000, 3, 1), a_l_minus_1:(1000, 2, 1)\ndelta_l:(1000, 3, 1), dc_db_l:(3, 1), dc_dw_l:(3, 2)\n\n```\n:::\n:::\n\n\nWow! That was easy! We backproped across **every single example in our batch!**\n\nWe're done! Lets define a new function that does the above:\n\n::: {.cell execution_count=136}\n``` {.python .cell-code}\ndef backprop_batch(activities,activations,y_batch,quiet=True):\n    weight_updates = []\n    bias_updates = []\n    layer_num = len(activations)\n    \n    ########################################\n    #           Last Level\n    ########################################\n    # get last layer specific variables\n    z_L = (activities[-1].T)[:,:,np.newaxis]\n    a_L = (activations[-1].T)[:,:,np.newaxis]\n    a_L_minus_1 = (activations[-2].T)[:,:,np.newaxis]\n    \n    # calculate delta_L for the last level\n    delta_L = (a_L-y_batch[:,:,np.newaxis])*sigmoid_prime(z_L)\n\n    deltas = []\n    deltas.append(delta_L)\n    \n    # calculate bias update\n    dc_db_L = delta_L.sum(axis=0)\n    bias_updates = [dc_db_L] + bias_updates\n    \n    # calcualte weight updates\n    dc_dw_L = (delta_L @ a_L_minus_1.transpose(0,2,1)).sum(axis=0)\n    weight_updates = [dc_dw_L] + weight_updates\n    \n\n    ########################################\n    # loop through each layer, from 2 to L-1\n    ######################################## \n    for layer in range(2,layer_num):\n        # using level as a **negative index**\n        l = -layer\n\n        # uncomment this print statement \n        # to help you understand how negative indexing works\n        if not quiet: print(f\"Loop variable: {layer}, l={l}, corresponding to layer: l{l}\")\n\n        # get layer values\n        z_l = (activities[l].T)[:,:,np.newaxis]\n        a_l = (activations[l].T)[:,:,np.newaxis]\n        a_l_minus_1 = (activations[l-1].T)[:,:,np.newaxis]\n\n        # calculate delta_l for each layer\n        delta_l = (weights[l+1].T @ deltas[l+1]) * sigmoid_prime(z_l)\n        deltas = [delta_l] + deltas\n\n        # calculate bias update\n        dc_db_l = delta_l.sum(axis=0)\n        bias_updates = [dc_db_l] + bias_updates\n\n        # calcualte weight updates\n        dc_dw_l = (delta_l @ a_l_minus_1.transpose(0,2,1)).sum(axis=0)\n        weight_updates = [dc_dw_l] + weight_updates\n    \n        if not quiet:\n            print(f'=========Layer:{layer_num+l}=========')\n            print(f'Using negative index: {l}')\n            print(f'z_l:{z_l.shape}, a_l:{a_l.shape}, a_l_minus_1:{a_l_minus_1.shape}')\n            print(f'delta_l:{delta_l.shape}, dc_db_l:{dc_db_l.shape}, dc_dw_l:{dc_dw_l.shape}')\n            print()\n    \n    return weight_updates,bias_updates\n```\n:::\n\n\nAnd now lets use it! \n\n::: {.cell execution_count=137}\n``` {.python .cell-code}\n# dataset\nN = X.shape[0]\nd = X.shape[1]\nbatch_size = 50\nepochs = 10\neta=1e-2\ndataset=X\nlabels=y\nquiet=False\n\n# gradient checking\nepsilon = 1e-7\n\n# initialize weight matrices - notice the dimensions\nW_1_init = np.random.rand(3,2)\nW_2_init = np.random.rand(3,3)\nW_3_init = np.random.randn(1,3)\nweights = [W_1_init,W_2_init,W_3_init]\n\n# and our biases\nb_1_init = np.random.rand(3,1)\nb_2_init = np.random.rand(3,1)\nb_3_init = np.random.rand(1,1)\nbiases = [b_1_init,b_2_init,b_3_init]\n\n# network\nW_1,W_2,W_3 = weights[0],weights[1],weights[2]\nb_1,b_2,b_3 = biases[0],biases[1],biases[2]\n\n# mini-batch params \nnum_batches = int(N/batch_size)\niterations = 0\n\n# various debugging lists\nloss = []\ngrad_norm_1 = []\ngrad_norm_2 = []\ngrad_norm_3 = []\nnorm_1 = []\nnorm_2 = []\nnorm_3 = []\ngrad_errors = []\ndiff_vecs = []\ngrad_vecs = []\nmy_act = []\nmy_w = []\nmy_b = []\nmy_bp = {'x':[],'y':[],'act':[],'delta':[],'w':[]}\n\nfor epoch in range(epochs):\n    # shuffle the dataset - note we do this by shuffling indices\n#     indices = np.random.permutation(N)\n#     shuffled_dataset = dataset[indices,:]\n#     shuffled_labels = labels[indices,:]\n    shuffled_dataset = dataset\n    shuffled_labels = labels\n        \n    # work on current batch \n    for batch_num in range(num_batches):\n\n        # get current batch\n        batch_start = batch_num*batch_size\n        batch_end = (batch_num+1)*batch_size\n        batch = shuffled_dataset[batch_start:batch_end,:]\n        batch_labels = shuffled_labels[batch_start:batch_end,:].reshape(-1,1)\n\n        # feedforward on batch\n        activities, activations = feedforward_batch(batch,weights,biases)\n        my_act.append(activations[-1])\n\n        # backprop on batch\n        weight_grads,bias_grads = backprop_batch(activities,activations,batch_labels)\n\n        # take your steps:\n        W_1 = W_1 - eta/batch_size*(weight_grads[0])\n        b_1 = b_1 - eta/batch_size*(bias_grads[0])\n\n        W_2 = W_2 - eta/batch_size*(weight_grads[1])\n        b_2 = b_2 - eta/batch_size*(bias_grads[1])\n\n        W_3 = W_3 - eta/batch_size*(weight_grads[2])\n        b_3 = b_3 - eta/batch_size*(bias_grads[2])\n        \n        norm_1.append(np.linalg.norm(W_1))\n        norm_2.append(np.linalg.norm(W_2))\n        norm_3.append(np.linalg.norm(W_3))\n        \n        # save new weights and biases to be used in the next feedforward step\n        weights = [W_1,W_2,W_3]\n        biases = [b_1,b_2,b_3]\n        \n  \n    \n    print(f\"Epoch:{epoch+1}/{epochs}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:1/10\nEpoch:2/10\nEpoch:3/10\nEpoch:4/10\nEpoch:5/10\nEpoch:6/10\nEpoch:7/10\nEpoch:8/10\nEpoch:9/10\nEpoch:10/10\n```\n:::\n:::\n\n\n::: {.cell execution_count=138}\n``` {.python .cell-code}\nweights\n```\n\n::: {.cell-output .cell-output-display execution_count=138}\n```\n[array([[0.1081274 , 0.47951698],\n        [0.60154837, 0.12693014],\n        [0.87717778, 0.2176614 ]]),\n array([[0.94690038, 0.02751412, 0.60631782],\n        [0.22879508, 0.52390474, 0.27270812],\n        [0.43900256, 0.60783846, 0.48106349]]),\n array([[ 1.11310853,  1.01175885, -1.22714557]])]\n```\n:::\n:::\n\n\n::: {.cell execution_count=139}\n``` {.python .cell-code}\nnielsen_w = []\nnielsen_b = []\nnielsen_bp = {'x':[],'y':[],'act':[],'delta':[],'w':[]}\n\nnet = Network([2,3,3,1],weights=[W_1_init, W_2_init, W_3_init],\n              biases=[b_1_init, b_2_init, b_3_init])\n```\n:::\n\n\n::: {.cell execution_count=140}\n``` {.python .cell-code}\nnet.SGD([(lx.reshape(-1,1),ly.reshape(-1,1)) for lx,ly in zip(X,y)], epochs,batch_size,eta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 0 complete\nEpoch 1 complete\nEpoch 2 complete\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 3 complete\nEpoch 4 complete\nEpoch 5 complete\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 6 complete\nEpoch 7 complete\nEpoch 8 complete\nEpoch 9 complete\n```\n:::\n:::\n\n\n::: {.cell execution_count=141}\n``` {.python .cell-code}\nnet.weights\n```\n\n::: {.cell-output .cell-output-display execution_count=141}\n```\n[array([[0.1081274 , 0.47951698],\n        [0.60154837, 0.12693014],\n        [0.87717778, 0.2176614 ]]),\n array([[0.94690038, 0.02751412, 0.60631782],\n        [0.22879508, 0.52390474, 0.27270812],\n        [0.43900256, 0.60783846, 0.48106349]]),\n array([[ 1.11310853,  1.01175885, -1.22714557]])]\n```\n:::\n:::\n\n\n::: {.cell execution_count=142}\n``` {.python .cell-code}\nweights\n```\n\n::: {.cell-output .cell-output-display execution_count=142}\n```\n[array([[0.1081274 , 0.47951698],\n        [0.60154837, 0.12693014],\n        [0.87717778, 0.2176614 ]]),\n array([[0.94690038, 0.02751412, 0.60631782],\n        [0.22879508, 0.52390474, 0.27270812],\n        [0.43900256, 0.60783846, 0.48106349]]),\n array([[ 1.11310853,  1.01175885, -1.22714557]])]\n```\n:::\n:::\n\n\n# Additional Resources\nThis implementation follows the details of: \n* http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\n* https://www.youtube.com/watch?v=GlcnxUlrtek&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU&index=4&pbjreload=101\n* https://github.com/stephencwelch/Neural-Networks-Demystified/blob/master/Part%202%20Forward%20Propagation.ipynb\n* https://www.youtube.com/watch?v=7bLEWDZng_M&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=33\n* https://www.youtube.com/watch?v=gl3lfL-g5mA\n* https://news.ycombinator.com/item?id=15782156\n* https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll\n\n",
    "supporting": [
      "nn_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}