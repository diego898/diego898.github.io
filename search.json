[
  {
    "objectID": "posts/prob/prob-mcmc-gibbs.html",
    "href": "posts/prob/prob-mcmc-gibbs.html",
    "title": "An Introduction to Markov Chains, MCMC and The Gibbs Sampler",
    "section": "",
    "text": "In this notebook, we begin by reviewing some of the basics of probability and Bayes rule, then make out way into markov chains, and monte-carlo sampling, combining them to give us MCMC, and then instate this in the Gibbs Sampler.\nBy the end, we will generate and understand the following animation:\n\n\n\n\n2023-02-24 - first published using quarto\n\n\n\n\nThis notebook can (re)generate animations of optimization procedures, but that can take quite some time. In its place, you can use pre-generated GIFs/MP4s by togglnig this flag:\n\nreanimate_gifs = False\n\n\n\n\n\nimport numpy as np \nimport sklearn\nimport sklearn.datasets\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport seaborn as sns\n\n# for creating animations\nimport matplotlib.animation\nfrom IPython.display import HTML, Video\n\n\n# styling additions\nfrom IPython.display import HTML\n# style = \"<style>div.warn{background-color: #fcf2f2;border-color: #dFb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;}</style>\"\nstyle = \"<style>div.info{padding: 15px; border: 1px solid transparent; border-left: 5px solid #dfb5b4; border-color: transparent; margin-bottom: 10px; border-radius: 4px; background-color: #fcf8e3; border-color: #faebcc;}</style>\"\nHTML(style)\n\n\n\n\n\nsns.color_palette()\n\n\n\n\n\nsblue,sorange,sgreen,sred = sns.color_palette()[0:4]"
  },
  {
    "objectID": "posts/prob/prob-mcmc-gibbs.html#thinking-about-the-tails",
    "href": "posts/prob/prob-mcmc-gibbs.html#thinking-about-the-tails",
    "title": "An Introduction to Markov Chains, MCMC and The Gibbs Sampler",
    "section": "Thinking about the tails",
    "text": "Thinking about the tails\nOk, so whats the difference between the gaussian density and the logistic density?\nIts all in the tails.\nThis is a fantastic thinking tool to keep in mind when you‚Äôre thinking about the behavior of distributions/probabilistic objects.\nIn order to aid this kind of thinking, we will do something very common in probability. Instead of thinking of them directly, we will think about log-probabilities.\n\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,7))\nx = np.linspace(-4,4,100)\nax1.plot(x,norm.pdf(x));\nax1.set_title('Gaussian PDF')\nax2.plot(x,norm.logpdf(x),color=sorange);\nax2.set_title('Log PDF of a Gaussian');\n\n\n\n\n\nüßê Pause-and-ponder: Does the shape of this make sense? Why or why not?\n\nLets compare this to the logistic density we gave above:\n\nfrom scipy.stats import logistic\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,7))\nx = np.linspace(-4,4,100)\nax1.plot(x,logistic.pdf(x));\nax1.set_title('Logistic PDF')\nax2.plot(x,logistic.logpdf(x),color=sorange);\nax2.set_title('Log PDF of a Logistic');\n\n\n\n\nHm. Lets plot them together to get a better idea, and add in a cauchy distribution:\n\nfrom scipy.stats import logistic\nfrom scipy.stats import cauchy\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,7))\nx = np.linspace(-4.5,4.5,100)\nax1.plot(x,norm.pdf(x),label='norm');\nax1.plot(x,logistic.pdf(x),linestyle='--',label='logistic');\nax1.plot(x,cauchy.pdf(x),linestyle=':',label='cauchy');\nax1.set_title('PDFs')\nax1.legend();\n\nax2.plot(x,norm.logpdf(x),label='norm');\nax2.plot(x,logistic.logpdf(x),linestyle='--',label='logsitic');\nax2.plot(x,cauchy.logpdf(x),linestyle=':',label='cauchy');\nax2.set_title('Log PDFs');\nax2.set_ylim([-5,0]);\nax2.legend();\n\n\n\n\nPlay around with the graph above, and lets discuss what we observe:"
  },
  {
    "objectID": "posts/prob/prob-mcmc-gibbs.html#drug-testing-example",
    "href": "posts/prob/prob-mcmc-gibbs.html#drug-testing-example",
    "title": "An Introduction to Markov Chains, MCMC and The Gibbs Sampler",
    "section": "Drug testing example",
    "text": "Drug testing example\nA common example problem for Bayes rule is the drug testing problem.\nLets assume the following:\n\nprobability of a general member of the public having a disease is 1/100000\nprobability of having a symptom given that you have the disease is: 1.\nprobability of a non-diseased person having a symptom is: 1/10000.\n\n\n\n\\begin{align}\nP(\\text{Disease} | \\text{Symptoms}) &= \\frac{P(\\text{Disease})\\;P(\\text{Symptoms}|\\text{Disease})}{P(\\text{Symptoms})} \\\\\n& = \\frac{P(\\text{Disease})\\;P(\\text{Symptoms}|\\text{Disease})}{P(\\text{Symptoms}|\\text{Disease})P(\\text{Disease}) + P(\\text{Symptoms}|\\text{NON-Disease})P(\\text{NON-Disease})}\n\\end{align}"
  },
  {
    "objectID": "posts/prob/prob-mcmc-gibbs.html#variational-bayes",
    "href": "posts/prob/prob-mcmc-gibbs.html#variational-bayes",
    "title": "An Introduction to Markov Chains, MCMC and The Gibbs Sampler",
    "section": "Variational Bayes",
    "text": "Variational Bayes\nOften, you‚Äôll find Bayes rule written with a log:\n\n\\log P(X|Y) = \\log P(X) + \\log P(Y|X) - \\log P(Y)\n\nThis form is often used in optimization problems, where we are tying to find a variational approximation the posterior distribution.\nIn these problems, our data is fixed, and so we can rewrite Bayes rule to expose this variational principle:\n\n\\begin{align}\n\\log P(Y) &= \\log P(X) + \\log P(Y|X) - \\log P(X|Y) \\\\\n          &= \\log P(X) - [\\log P(X|Y) - \\log P(Y|X)]\n\\end{align}\n\nNotice that the left hand side does not change.\n\nüßêPause-and-ponder: Think about this. What does this suggest is a good thing to check for in an optimization problem? What is the left hand side? What about the right?\nWe can think about some pretty advanced methods using this basic observation!\n\nWe will not explore these topics further in this course, but we‚Äôve gone a long way just by exposing you to this kind of thinking!"
  },
  {
    "objectID": "posts/prob/prob-mcmc-gibbs.html#inverse-transform-sampling",
    "href": "posts/prob/prob-mcmc-gibbs.html#inverse-transform-sampling",
    "title": "An Introduction to Markov Chains, MCMC and The Gibbs Sampler",
    "section": "Inverse Transform Sampling",
    "text": "Inverse Transform Sampling\nLets start by looking at the CDF of a Gaussian RV:\n\nplt.plot(np.linspace(-5,5),norm.cdf(np.linspace(-5,5)));\n\n\n\n\nRecall, this tells us:\n\nF_X(x) = P(X\\leq x)\n\n\nüßêPause-and-ponder: What happens if I plug in a Gaussian random variable into its own CDF?\nWhat question am I asking in doing so?\n\nLets cary the experiment out!\n\nsamples = np.random.standard_normal(1000)\nplt.hist(samples);\n\n\n\n\nNow lets plug them into the CDF and plot:\n\nplt.hist(norm.cdf(samples));\n\n\n\n\n\nüßêPause-and-ponder: What do we see here? Does this make sense with what you said above? Think about this!\n\nAh - A random variable plugged into its own CDF gives a uniform rv.\nHm. So a CDF tell us the prob that X is in a certain range. Well, it shouldn‚Äôt surprise you that we can also define an inverse cdf, also called a quantile function, which is the ‚Äúinverse‚Äù of this property.\nSo in a CDF you plug in values, and get back a probability. Well, for an inverse CDF you plugin probabilities and get back values!\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,7))\nx = np.linspace(-2.1,2.1)\nax1.plot(x,norm.cdf(x));\n\np = np.linspace(0,1)\nax2.plot(p,norm.ppf(p),c=sorange);\n\n\n\n\n\nüßêPause-and-ponder: What would happen if we plug in uniform values into an inverse CDF? What question is that asking?\n\n\nsamples = np.random.rand(10000)\nplt.hist(samples);\n\n\n\n\n\nplt.hist(norm.ppf(samples));\n\n\n\n\nAha! So plugging in uniform values into an inverse CDF gives samples from the original CDF!\nWe‚Äôve discovered inverse transform sampling!\nIf we can: * draw samples from a uniform distribution (easy) * represent a distribution by its CDF (not always easy!)\nWe can easily draw samples from that distribution!"
  },
  {
    "objectID": "posts/prob/prob-mcmc-gibbs.html#rejection-sampling",
    "href": "posts/prob/prob-mcmc-gibbs.html#rejection-sampling",
    "title": "An Introduction to Markov Chains, MCMC and The Gibbs Sampler",
    "section": "Rejection Sampling",
    "text": "Rejection Sampling\nAbove, we saw one way to sample from a distribution. Lets look at another here.\nLets introduce it by way of a fun problem: try and build a probabilistic way to estimate the value of \\pi!\n\nüó£Discuss: Can anyone think of a way to do this? Lets\n\nLets do one way: start by taking samples in the unit square:\n\nsamples = np.random.rand(2000,2)*2-1\n\nplt.figure(figsize=(7,7))\nplt.scatter(samples[:,0],samples[:,1],s=5);\n\n\n\n\n\nüßêPause-and-ponder: How can we build an estimator for the value of \\pi from this?\n\nTry it! Lets plot the unit-circle:\n\nplt.figure(figsize=(7,7))\nplt.scatter(samples[:,0],samples[:,1],s=5);\ntheta = np.linspace(0, 2*np.pi, 100);\nr = np.sqrt(1.0);x1 = r*np.cos(theta);x2 = r*np.sin(theta)\nplt.plot(x1,x2,'r');\n\n\n\n\nAh. So lets take the ratio of samples that fall in the unit circle, as an estimator for its area, and the total number of samples as estimator for the the area of the square.\nWith this, we can calculate:\n\n\\frac{\\text{area of circle}}{\\text{area of square}} = \\frac{\\pi r^2}{(2r)^2}  = \\frac{\\pi}{4}\n\nSo the ratio of our areas is an estimator for \\pi/4!\nLets see how many samples fall within the unit circle:\n\ninCirc = samples[:,0]**2 + samples[:,1]**2 <= 1\n\nLets plot them:\n\nplt.figure(figsize=(7,7))\nplt.scatter(samples[:,0],samples[:,1],s=5);\nplt.plot(x1,x2,'r');\nplt.scatter(samples[inCirc][:,0],samples[inCirc][:,1],s=5,c='r');\n\n\n\n\nOk! So we can count the proportion of red circles to total circles, giving us an estimate of the area of the circle relative to the square:\n\nrelArea = len(samples[inCirc])/len(samples)\nrelArea\n\n0.783\n\n\nLets multiply by 4 to see how we did!\n\npi_est = relArea*4\npi_est\n\n3.132\n\n\nHm. Not bad! Lets write a function and see how we do with more samples:\n\ndef estimate_pi(samples_list):\n    vals = []\n    for sample_num in samples_list:\n        samples = np.random.rand(int(sample_num),2)*2-1\n        inCirc = samples[:,0]**2 + samples[:,1]**2 <= 1\n        vals.append(len(samples[inCirc])/len(samples))\n    return vals\n\nLets plot it!\n\nsamples_list = [1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8]\npi_estimates = estimate_pi(samples_list)\n\n\nplt.plot(samples_list,[np.pi - curr*4 for curr in pi_estimates],c='r');\nplt.gca().set_xscale('log')\n\n\n\n\nWe‚Äôve looked at fun little rejection sampling problem.\nBroadly speaking, rejection sampling is interested in using a proxy distribution which is easy to sample from, to simulate samples from another distribution that its usually hard to sample from.\nIt does this by oversampling from the easy one, and throwing away the ones that don‚Äôt match an inclusion criteria (rejection them). Rejection samplers, therefore, have a concept of ‚Äúefficiency‚Äù, which tell you how many samples you needed to generate to have one included, or how many you ended up throwing away, etc. Or, how many samples do you need to achieve a certain accuracy, etc.\nThey can be roughly thought of a kind of Monte Carlo technique, where Monte Carlo Methods can be described as random sampling algorithms."
  },
  {
    "objectID": "posts/prob/prob-mcmc-gibbs.html#evolving-distributions",
    "href": "posts/prob/prob-mcmc-gibbs.html#evolving-distributions",
    "title": "An Introduction to Markov Chains, MCMC and The Gibbs Sampler",
    "section": "Evolving Distributions",
    "text": "Evolving Distributions\nPreviously, we‚Äôve been simulating a chain and plotting trajectories to try and understand them.\nBut we can do better!\nWe can do better by imagining not running a particular chain, but asking ourselves, how does a transition matrix affect a distribution?\nOr, given a starting distribution, what can I say about the ending distribution?\nThis idea may seem strange at first. Maybe even at second! But thats ok - let dig in and explore it some more.\nLets go back to the weather example:\n\np_weather = np.array([[0.7,0.2,0.1],     # If rainy, moving to rainy, sunny, cold\n                     [0.1, 0.8,0.1],     # If sunny, moving to rainy, sunny, cold\n                     [0.1,0,0.9]])    # if cold,  moving to rainy, sunny, cold\n\nLets pick a simple starting distribution: We always start in the raining state. Well, then this would be:\n\ninitial_dist = np.array([0.5,0.4,0.1])\n\nBefore we try and understand the formula, lets first try to ‚Äúrun it through once‚Äù and understand the result:\n\nüßêPause-and-ponder:What are we looking at here? What do you notice? Discuss!\n\nHm. Lets check our understanding by running it through twice!\n\n(initial_dist.dot(p_weather)).dot(p_weather)\n\narray([0.34 , 0.416, 0.244])\n\n\nHm‚Ä¶ what do we notice?\nWhat if we keep going? Lets try to make this a bit nicer so we can see whats happening:\n\nnum_steps = 100\ndists = [initial_dist]\nstep = initial_dist.dot(p_weather)\ndists.append(step)\nfor _ in range(num_steps):\n    step = step.dot(p_weather)\n    dists.append(step)\n\n\nif reanimate_gifs:\n    fig, ax = plt.subplots(1,1,figsize=(10,7));\n    markerline,stemline,baseline = ax.stem(dists[0],basefmt='--w',use_line_collection=True);\n    plt.xticks([0,1,2],labels=['Rainy','Sunny','Cold'],fontsize=20);\n\n    def update_graph(frame):\n        ax.cla();\n        markerline, stemlines, baseline = ax.stem(dists[frame],basefmt='--w',use_line_collection=True);\n        ax.set_ylim((-0.01, 1.01));\n        plt.xticks([0,1,2],labels=['Rainy','Sunny','Cold'],fontsize=20);\n        ax.set_title(f'Step:{frame}');\n        xs = np.array([0,1,2])\n        ys = np.array(dists[frame])\n        for x,y in zip(xs,ys):\n            plt.annotate(f'{y:.2f}', xy=(x,y), xytext=(0,5), \n                         textcoords='offset points',ha='center',fontsize=20);\n\n    anim = matplotlib.animation.FuncAnimation(fig, update_graph, \n                                              frames=range(len(dists)), interval=500);\n    # HTML(anim.to_html5_video())\n    anim.save('mc_evolve.mp4')\n    \nVideo('mc_evolve.mp4', width=800)\n\n\n      Your browser does not support the video element.\n    \n\n\n\nüßêPause-and-ponder:What are we looking at here? What do you notice? Discuss!\n\nWe‚Äôve discovered the stationary distribution of this chain!\nIts the distribution of where we can expect to find the chain after enough time!\nInterestingly (very much so!) we can calculate this by taking the matrix power of the transition matrix directly!\n\nnp.linalg.matrix_power(p_weather,num_steps)\n\narray([[0.25, 0.25, 0.5 ],\n       [0.25, 0.25, 0.5 ],\n       [0.25, 0.25, 0.5 ]])\n\n\nThen, we just left-multiply this by our initial distribution:\n\ninitial_dist.dot(np.linalg.matrix_power(p_weather,num_steps))\n\narray([0.25, 0.25, 0.5 ])\n\n\n\nLets bring back the population example and make it more interesting. To make it explicit, lets create that ‚Äúbig‚Äù transition matrix I told you about previously:\n\neps = 2/3  # prob \nprob_change_island = np.array([1/3,1/3,1/3])   # leave, no change, arrive\n\n\np_island = np.zeros((101,101))\np_island[0,0] = 1     # if we are at zero, we stay at zero\np_island[-1,-1] = 1   # if we are 100, we stay at 100\n\n\n# p_island = np.zeros((101,101))\nfor curr_pop in range(1,100):\n    p_island[curr_pop,curr_pop-1:curr_pop+2] = prob_change_island\n\n\np_island\n\narray([[1.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.33333333, 0.33333333, 0.33333333, ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.33333333, 0.33333333, ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.33333333, 0.33333333,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.33333333, 0.33333333,\n        0.33333333],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        1.        ]])\n\n\n\n(np.sum(p_island,axis=1)==1).all()\n\nTrue\n\n\nNow lets simulate it!\n\nnum_steps = 3000\ninitial_dist = np.zeros(101)\ninitial_dist[50] = 1\ndists = [initial_dist]\nstep = initial_dist.dot(p_island)\ndists.append(step)\nfor _ in range(num_steps):\n    step = step.dot(p_island)\n    dists.append(step)\n\n\nif reanimate_gifs:\n    fig, ax = plt.subplots(1,1,figsize=(10,7));\n    ax.get_yaxis().set_visible(False)\n    markerline,stemline,baseline = ax.stem(dists[0],basefmt='--w',use_line_collection=True);\n\n    def update_graph(frame):\n        ax.cla();\n        markerline, stemlines, baseline = ax.stem(dists[frame],basefmt='--w',use_line_collection=True);\n        ax.set_title(f'Step:{frame}');\n\n    anim = matplotlib.animation.FuncAnimation(fig, update_graph, \n                                              frames=range(1,len(dists),100), interval=100);\n    plt.close();\n    # HTML(anim.to_html5_video())\n    anim.save('mc_evolve_2.mp4')\n    \nVideo('mc_evolve_2.mp4', width=800)\n\n\n      Your browser does not support the video element.\n    \n\n\n\nüßêPause-and-ponder:What are we looking at here? What do you notice? Discuss!\n\nLets use our matrix power formula to verify:\n\nend_point = 400\nplt.figure(figsize=(10,7))\nplt.stem(initial_dist.dot(np.linalg.matrix_power(p_island,end_point)),use_line_collection=True);\n\n/var/folders/1j/c5scsycs1s5_nzj08g133rp40000gn/T/ipykernel_5554/544233340.py:3: MatplotlibDeprecationWarning: The 'use_line_collection' parameter of stem() was deprecated in Matplotlib 3.6 and will be removed two minor releases later. If any parameter follows 'use_line_collection', they should be passed as keyword, not positionally.\n  plt.stem(initial_dist.dot(np.linalg.matrix_power(p_island,end_point)),use_line_collection=True);\n\n\n\n\n\nWow! What have we discovered?\n\nüßêPause-and-ponder:What are we looking at here? What do you notice? Discuss!"
  },
  {
    "objectID": "posts/prob/prob-mcmc-gibbs.html#metropolis-algorithm",
    "href": "posts/prob/prob-mcmc-gibbs.html#metropolis-algorithm",
    "title": "An Introduction to Markov Chains, MCMC and The Gibbs Sampler",
    "section": "Metropolis Algorithm",
    "text": "Metropolis Algorithm\nLets first state the algorithm, and then try and analyze it.\nWe begin by picking, or specifying, a proposal distribution, or transition kernel: Q. You can think of it as specifying a probabilistic way of moving, randomly, to a new position in space (y say), given a current position (x say). That is, Q is a distribution on y given x, and we will write it: Q(y|x).\nNote: when were working continuously, then Q will be a density on y.\nAs an example, lets choose the random walk kernel/proposal, which is a fancy way to say:\n\nThe Gaussian‚Ñ¢ üòé\n\nUnder this proposal, if we start at a position x, then our next position will be a standard Normal away:\n\ny = x + \\mathcal{N}(0,1)\n\nThis makes our conditional distribution Q(y|x), or the transition kernel be centered on our current location:\n\nQ(y|x) = \\mathcal{N}(x,1)\n\nNote: Often, we don‚Äôt want the variance to be 1. This could cause us to take too large steps!\nNow we can actually define the algorithm:\n\nüëæMetropolis Algorithm Definition:\nInitialize X_1 to some value: X_1=x_1. For t= 1,2,\\ldots: 1. sample from the proposal distribution: $ y Q(y|x_t)$. We are proposing this value to be x_{t+1}.\n2. compute the acceptance probability of this sample:\n\nA = \\min \\left( 1, \\frac{\\pi(y)}{\\pi(x_t)} \\right)\n\n\nWith probability A, accept the proposed value and set x_{t+1} = y. Otherwise reject it and keep x_{t+1} = x_t.\n\n\n\nü§îüí≠\n\nHm. I can follow the steps so far, but its a bit unclear how this relates to chains, integration, etc.\nNo problem! Lets first run it on a simple example!"
  },
  {
    "objectID": "posts/prob/prob-mcmc-gibbs.html#sampling-from-exponential",
    "href": "posts/prob/prob-mcmc-gibbs.html#sampling-from-exponential",
    "title": "An Introduction to Markov Chains, MCMC and The Gibbs Sampler",
    "section": "Sampling from Exponential",
    "text": "Sampling from Exponential\nLets pick the difficult distribution \\pi to be the exponential distribution:\n\n\\pi(x) = e^{-x}\n\n\nfrom scipy.stats import expon\nx = np.linspace(0,10)\nplt.figure(figsize=(10,7))\nplt.plot(expon.pdf(x));\n\n\n\n\nPretend this is the distribution we can‚Äôt easily sample from! Lets run the algorithm above.\nLets pick some random starting point:\n\nnum_steps = 1000\nx = np.zeros(num_steps)\nx[0] = 3   # random starting point\n\nnum_accepted = 0\n\nfor t in range(1, num_steps):\n    # propose a new value\n    proposed_x = x[t-1] + np.random.randn()\n    \n    # calculate its acceptance probability\n    A = expon.pdf(proposed_x)/expon.pdf(x[t-1])\n    A = np.amin([1,A])\n    \n    # accept it with that prob\n    if np.random.rand() < A:    # accept\n        x[t] = proposed_x\n        num_accepted += 1\n    else:                       # reject\n        x[t] = x[t-1]\n\n\nnum_accepted/num_steps\n\n0.554\n\n\nAnd thats it! We ran our algorithm. Now to understand it!\nLets first verify that indeed the histogram of our samples looks like the distribution we are trying to sample from!\n\nplt.figure(figsize=(10,7));\nplt.hist(x,bins=10);\n\n\n\n\nPerfect! Lets plot the path this chain took:\n\nplt.figure(figsize=(20,7));\nplt.plot(x,'-o');\n# plt.scatter(range(len(x)),x);\n\n\n\n\n\nü§∑üèΩ‚Äç‚ôÇÔ∏èPause-and-ponder: What are we looking at here? How is this related to the figure above?"
  },
  {
    "objectID": "posts/prob/prob-mcmc-gibbs.html#mcmc-for-bayesian-sampling",
    "href": "posts/prob/prob-mcmc-gibbs.html#mcmc-for-bayesian-sampling",
    "title": "An Introduction to Markov Chains, MCMC and The Gibbs Sampler",
    "section": "MCMC for Bayesian Sampling",
    "text": "MCMC for Bayesian Sampling\nRecall, that in Bayesian statistics, we want to estimate the posterior distribution, but this is often intractable due to the high-dimensional integral in the denominator (marginal likelihood).\n\np(X|Y) = \\frac{p(X)p(Y|X)}{p(Y)} = \\frac{p(X)p(Y|X)}{\\sum p(X_i)p(Y|X_i)}\n\nAs we saw above when we discussed Monte Carlo integration, we can approximate the posterior p(X|Y), if we can somehow draw many samples that come from the posterior distribution.\nWith vanilla Monte Carlo integration, we need the samples to be independent draws from the posterior distribution, which is a problem if we do not actually know what the posterior distribution is, because we cannot integrate the marginal likelihood in the first place!.\nWith MCMC, we draw samples from a simple proposal distribution, so that each draw depends only on the state of the previous draw (i.e.¬†the samples form a Markov chain!).\nUnder certain conditions, the Markov chain will have a unique stationary distribution which will be posterior! (Note: We have ignored those technical conditions in this document).\nNote however, that not all samples are used! Instead we must set up acceptance criteria for each draw based on comparing successive states with respect to a target distribution that ensure that the stationary distribution is the posterior distribution!\nThe nice thing is that this target distribution only needs to be proportional to the posterior distribution, which means we don‚Äôt need to evaluate the potentially intractable marginal likelihood! (which is just the normalizing constant).\nWe can find such a target distribution easily!\n\nü§∑üèΩ‚Äç‚ôÇÔ∏èPause-and-ponder: Can you think of a distribution that is proportional to our posterior but easy to sample from?\n\nIf you suggested the numerator of Bayes rule, you are correct!\n\np(X|Y) \\propto p(X) P(Y|X)\n\nAfter some time, the Markov chain of accepted draws will converge to the stationary distribution, and we can use those samples as correlated draws from the posterior distribution, and find functions of the posterior distribution in the same way as for vanilla Monte Carlo integration.\nLets instantiate this problem with our favorite simple Bayesian example: the coin flips!\nWe want to estimate the bias of a coin (the probability of flipping heads, say), given a sample consisting of n tosses:\n\np(\\theta | X^n)\n\nwhere X^n is all of our tosses. Note: we will drop the (\\cdot)^n notation below, as it is clear what we mean.\nTo compare our estimator, we can use a beta distribution as our conjugate prior, letting us write the posterior distribution in closed form\n\nnum_flips = 100\nheads = 61\np = heads/num_flips\n\n# setup distributions\nthetas = np.linspace(0, 1, 200)\n\na, b = 10, 10\nprior = stats.beta(a, b)\nlikelihood = stats.binom(num_flips, thetas)\npost = stats.beta(heads+a, num_flips-heads+b)\n\n\nplt.figure(figsize=(15,10))\nplt.plot(thetas, prior.pdf(thetas), label=r'Prior: $p(\\theta)$', c='blue')\nplt.plot(thetas, post.pdf(thetas), label=r'Posterior: $p(\\theta|X)$', c='red')\nplt.plot(thetas, num_flips*likelihood.pmf(heads), label=r'Likelihood: $p(X|\\theta)$', c='green')\nplt.xlim([0, 1])\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel('Density', fontsize=16)\nplt.legend(loc='upper left',fontsize=20);\n\n\n\n\nOk! Now we can get sampling!\nLets give the same Metropolis Algorithm above, in a Bayesian setting:\nWe first pick the same ‚Äúrandom walk‚Äù transition kernel/proposal distribution as above:\n\nQ(y|x) = \\mathcal{N}(x,1)\n\nTo rewrite it in terms of our coin example, we have:\n\nQ(\\theta_p|\\theta_t) = \\mathcal{N}(\\theta_t,\\sigma)\n\nwhere \\sigma needs to be smaller than 1 (more on this below).\nFor the target distribution, one way to describe it is to think of our target distribution as the actual posterior: \\pi(\\cdot) = \\text{post}(\\cdot).\nTake an initial guess for \\theta, and proceed as follows:\n\npropose a new \\theta_p: \n\\theta_p = \\theta_t + \\mathcal{N}(0,\\sigma)\n\nCalculate your acceptance ratio:\n\n\nA = \\frac{\\text{post}(\\theta_p | X)}{\\text{post}(\\theta_t | X)}\n\nBefore continuing, lets pause and think about this acceptance ratio!\n\nü§∑üèΩ‚Äç‚ôÇÔ∏èPause-and-ponder: What do you notice about this ratio?!\n\nIndeed! The model evidence/marginal likelihood cancels!\nSo we do not need to use the actual posterior in this ratio! Instead, we can any distribution that is proportional to it!. For example, the Bayes numerator!\nLets restate the algorithm now:\nTake an initial guess for \\theta, and proceed as follows:\n\npropose a new \\theta_p: \n\\theta_p = \\theta_t + \\mathcal{N}(0,1)\n\nCalculate your acceptance ratio:\n\n\nA = \\frac{p(\\theta_p) p(X|\\theta_p)}{p(\\theta_t) p(X|\\theta_t)}\n\n\nWith probability A, accept the proposed value and set \\theta_{t+1} = \\theta_p. Otherwise reject it and keep \\theta_{t+1} = \\theta_t.\n\n\nnum_flips = 100\nheads = 61\n\n# setup distributions\na = 10\nb = 10\nlikelihood = lambda curr_theta: stats.binom(num_flips, curr_theta).pmf(heads)\nprior = lambda curr_theta: stats.beta(a, b).pdf(curr_theta)\n\nnum_accepted = 0\n\ntheta_0 = 0.1\nnum_steps = 10000\nsigma = 0.3\nsamples = np.zeros(num_steps)\nsamples[0] = theta_0\n\nfor t in range(1,num_steps):\n    # propose a theta\n    theta_t = samples[t-1]\n    theta_p = theta_t + np.random.randn()*sigma\n    \n    # calculate acceptance ratio\n    A = (likelihood(theta_p)*prior(theta_p))/(likelihood(theta_t)*prior(theta_t))\n    A = np.amin([1,A])\n    \n    # decice to accept:\n    if np.random.rand() < A:\n        samples[t] = theta_p\n        num_accepted += 1\n    else:\n        samples[t] = theta_t\n    \nprint(\"Efficiency = \", num_accepted/num_steps)\n\nEfficiency =  0.1843\n\n\nWoah! We only accepted very few of our proposed samples!\nLets plot the histogram:\n\npost = stats.beta(heads+a, num_flips-heads+b)\n\nnmcmc = len(samples)//2\n\nplt.figure(figsize=(15,10))\nplt.hist(stats.beta(a, b).rvs(nmcmc), 40, histtype='step', density=True,linewidth=4, label='Prior');\nplt.hist(samples[nmcmc:], 20, histtype='step', density=True, linewidth=4, label='Posterior');\nplt.plot(thetas, post.pdf(thetas), c='red', linestyle='--', linewidth=4, label='True posterior')\nplt.xlim([0,1]);\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel('Density', fontsize=16)\nplt.legend(loc='upper left',fontsize=20);\n\n\n\n\nAs we did above, lets plot the path this chain took!\n\nplt.figure(figsize=(20,7))\nplt.plot(samples[::50], '-o');\n\n\n\n\nThese kinds of trace plots are often used to informally assess for stochastic convergence.\nThis simple idea is just to run multiple chains and check that they converge to similar distributions. Lets repeat the above for multiple chains:\n\ndef target(lik, prior, n, h, theta):\n    if theta < 0 or theta > 1:\n        return 0\n    else:\n        return lik(n, theta).pmf(h)*prior.pdf(theta)\n    \ndef mh_coin(niters, n, h, theta, lik, prior, sigma):\n    samples = [theta]\n    while len(samples) < niters:\n        theta_p = theta + stats.norm(0, sigma).rvs()\n        rho = min(1, target(lik, prior, n, h, theta_p)/target(lik, prior, n, h, theta ))\n        u = np.random.uniform()\n        if u < rho:\n            theta = theta_p\n        samples.append(theta)\n    return samples\n\n\nnum_flips = 100\nheads = 61\nlik = stats.binom\nprior = stats.beta(a, b)\nsigma = 0.05\nnum_iters = 100\n\nsampless = [mh_coin(num_iters, num_flips, heads, theta, lik, prior, sigma) for theta in np.arange(0.1, 1, 0.2)]\n\n\nplt.figure(figsize=(20,7))\nfor samples in sampless:\n    plt.plot(samples, '-o')\nplt.xlim([0, num_iters])\nplt.ylim([0, 1]);\n\n\n\n\n\nüßêPause-and-ponder: Debrief on what we‚Äôve seen above. Questions to discuss:\n\nWhere exactly is the Markov Chain in MCMC?\nWhy does this chain sample from a distribution?\nWhat can we say about the resulting samples?\nWhat about its efficiency?\nWhat about how long it takes for the chain to ‚Äúsettle in‚Äù to a good setting? (also called burn in)\n\n\nAbove, we‚Äôve described the Metropolis Algorithm for MCMC. There is also its natural extension the Metropolis-Hastings Algorithm, which changes the acceptance ratio formula, to account for non-symmetric proposal distributions.\n\nü§∑üèΩ‚Äç‚ôÇÔ∏èPause-and-ponder: Before looking it up, how do you think this will change the acceptance ratio?"
  },
  {
    "objectID": "posts/prob/prob-mcmc-gibbs.html#gibbs-sampler",
    "href": "posts/prob/prob-mcmc-gibbs.html#gibbs-sampler",
    "title": "An Introduction to Markov Chains, MCMC and The Gibbs Sampler",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\nThe other major MCMC algorithm is the Gibbs Sampler. This is a technique for construction a Markov Chain from a join distribution, by sampling from the conditional distributions sequentially!\nNote: For this part, I slightly modified the wording and code from this really nice blog post!.\nMany times in the ‚Äúreal world‚Äù you will find yourself in a situation where it is incredibly difficult to sample from a joint distribution, but much easier to sample from a conditional distribution.\nThe Gibbs sampler uses samples from the conditional distributions, to approximate samples from this joint!\nNote: If its not, then we can also use Metropolis-Hastings to sample from this conditional, and then use Gibbs use to sample from the joint! This is called:\n\nMetropolis-Inside-Gibbs‚Ñ¢üòé\n\nThe algorithm for the Gibbs sampler is actually very straightforward.\nSuppose we have a vector of parameters \\theta=(\\theta_1,\\theta_2,\\ldots,\\theta_n), and we want to estimate the joint posterior distribution p(\\theta|X).\nSuppose we can find and draw random samples from all the conditional distributions:\n\np(\\theta_1 | \\theta_2, \\ldots, \\theta_n,X) \\\\\np(\\theta_2 | \\theta_1, \\ldots, \\theta_n,X) \\\\\n\\cdots \\\\\np(\\theta_n | \\theta_1, \\theta_2,\\ldots,X)\n\nWith Gibbs sampling, the Markov chain is constructed by sampling from the conditional distribution for each parameter \\theta_i in turn, treating all other parameters as observed.\nWhen we have finished iterating over all parameters, we are said to have completed one cycle of the Gibbs sampler.\nSince hierarchical models are typically set up as products of conditional distributions, the Gibbs sampler is ubiquitous in Bayesian modeling.\n\nExample Setup\nImagine we want to sample from a 2D Gaussian distribution over a variable of interest:\n\n\\mathbf{X} = [x_0,x_1]^T \\sim \\mathcal{N}(\\mathbf{\\mu},\\mathbf{\\Sigma})\n\nwhere:\n\n\\mathbf{\\mu} =\n\\begin{bmatrix}\n\\mu_0 \\\\\n\\mu_1\n\\end{bmatrix} \\quad\\quad\n\\mathbf{\\Sigma} =\n\\begin{bmatrix}\n\\sigma_{00} & \\sigma_{01} \\\\\n\\sigma_{10} & \\sigma_{11}\n\\end{bmatrix}\n\nThis is our target, that we‚Äôre pretending its difficult to sample from.\nThe Gibbs sampler uses samples from the conditional distributions, to approximate samples from this joint.\nIn other words, we only need to sample from p(x_0|x_1) and p(x_1|x_0)!\nMultivariate Gaussians have the property that the conditional distributions are also Gaussian (and the marginals too - for a proof of this property, see Wikipedia or any ML textbook).\nSo our conditionals are given by:\n\np(x_0|x_1) \\sim \\mathcal{N} \\left(\\mu_0 + \\frac{\\sigma_{01}}{\\sigma_{11}}(x_1 -\\mu_1),\\sigma_{00} - \\frac{\\sigma_{01}^2}{\\sigma_{11}}\\right) \\\\\np(x_1|x_0) \\sim \\mathcal{N} \\left(\\mu_1 + \\frac{\\sigma_{01}}{\\sigma_{00}}(x_0 -\\mu_0),\\sigma_{11} - \\frac{\\sigma_{01}^2}{\\sigma_{00}}\\right)\n\nWow these look verbose! But, they are just the formula‚Äôs for 1D gaussians, in terms of the parameters of the 2D gaussian! Very simple to code up!\nIndeed: we give just one function that can simulate from both:\n\ndef conditional_sampler(sampling_index, current_x, mean, cov):\n    conditioned_index = 1 - sampling_index \n    # The above line works because we only have 2 variables, x_0 & x_1\n    a = cov[sampling_index, sampling_index]\n    b = cov[sampling_index, conditioned_index]\n    c = cov[conditioned_index, conditioned_index]\n  \n    mu = mean[sampling_index] + (b * (current_x[conditioned_index] - mean[conditioned_index]))/c\n    sigma = np.sqrt(a-(b**2)/c)\n    new_x = np.copy(current_x)\n    new_x[sampling_index] = np.random.randn()*sigma + mu\n    return new_x\n\nNow we can give the actual Gibbs sampler for this problem:\n\ndef gibbs_sampler(initial_point, num_samples, mean, cov, create_gif=True):\n\n    frames = []  # for GIF\n    point = np.array(initial_point)\n    samples = np.empty([num_samples+1, 2])  #sampled points\n    samples[0] = point\n    tmp_points = np.empty([num_samples, 2]) #inbetween points\n\n    for i in range(num_samples):\n\n        # Sample from p(x_0|x_1)\n        point = conditional_sampler(0, point, mean, cov)\n        tmp_points[i] = point\n        if(create_gif):\n            frames.append(plot_samples(samples, i+1, tmp_points, i+1, title=\"Num Samples: \" + str(i)))\n            \n        # Sample from p(x_1|x_0)\n        point = conditional_sampler(1, point, mean, cov)\n        samples[i+1] = point\n        if(create_gif):\n            frames.append(plot_samples(samples, i+2, tmp_points, i+1, title=\"Num Samples: \" + str(i+1)))\n            \n    if(create_gif):\n        return samples, tmp_points, frames\n    else:\n        return samples, tmp_points\n\nNow were ready to make some nice plots! Lets wrap up a lot of this ugly plotting code in functions:\n\n\nPlotting code\n\n# !pip install gif\nfrom matplotlib.patches import Ellipse\nimport gif\nimport matplotlib.transforms as transforms\nfrom IPython.display import Image\n\n\n# plotting code from matplotlib and:\n# https://github.com/mr-easy/Gibbs-Sampling-Visualized/blob/master/plotting_util.py\ndef plot_gaussian_from_points(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n    \"\"\"\n    Create a plot of the covariance confidence ellipse of *x* and *y*.\n    Parameters\n    ----------\n    x, y : array-like, shape (n, )\n        Input data.\n    ax : matplotlib.axes.Axes\n        The axes object to draw the ellipse into.\n    n_std : float\n        The number of standard deviations to determine the ellipse's radiuses.\n    Returns\n    -------\n    matplotlib.patches.Ellipse\n    Other parameters\n    ----------------\n    kwargs : `~matplotlib.patches.Patch` properties\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"x and y must be the same size\")\n    if len(x) < 2:\n        raise ValueError(\"Need more data.\")\n    cov = np.cov(x, y)\n    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n    # Using a special case to obtain the eigenvalues of this\n    # two-dimensionl dataset.\n    ell_radius_x = np.sqrt(1 + pearson)\n    ell_radius_y = np.sqrt(1 - pearson)\n    ellipse = Ellipse((0, 0),\n        width=ell_radius_x * 2,\n        height=ell_radius_y * 2,\n        facecolor=facecolor,\n        **kwargs)\n\n    # Calculating the stdandard deviation of x from\n    # the squareroot of the variance and multiplying\n    # with the given number of standard deviations.\n    scale_x = np.sqrt(cov[0, 0]) * n_std\n    mean_x = np.mean(x)\n\n    # calculating the stdandard deviation of y ...\n    scale_y = np.sqrt(cov[1, 1]) * n_std\n    mean_y = np.mean(y)\n\n    transf = transforms.Affine2D() \\\n        .rotate_deg(45) \\\n        .scale(scale_x, scale_y) \\\n        .translate(mean_x, mean_y)\n\n    ellipse.set_transform(transf + ax.transData)\n    return ax.add_patch(ellipse)\n\ndef plot_gaussian_from_parameters(mean, cov, ax, n_std=3.0, facecolor='none', **kwargs):\n    \"\"\"\n    Create a plot of the covariance confidence ellipse of *x* and *y*.\n    Parameters\n    ----------\n    mean : array-like, shape (2, )\n        Mean vector\n    cov : array-like, shape (2,2)\n        Covariance matrix\n    ax : matplotlib.axes.Axes\n        The axes object to draw the ellipse into.\n    n_std : float\n        The number of standard deviations to determine the ellipse's radiuses.\n    Returns\n    -------\n    matplotlib.patches.Ellipse\n    Other parameters\n    ----------------\n    kwargs : `~matplotlib.patches.Patch` properties\n    \"\"\"\n    if len(mean) != 2:\n        raise ValueError(\"Mean vector length should be 2.\")\n    if (cov.shape != (2, 2)):\n        raise ValueError(\"Covariance should be a 2x2 matrix.\")\n    #checking if cov is symmetric pos semidefinite\n    if(cov[0, 1] != cov[1, 0]):\n        raise ValueError(\"Covariance should be symmetric.\")\n    if(cov[0, 0] < 0 or cov[0, 0]*cov[1,1] - cov[0,1]**2 < 0):\n        raise ValueError(\"Covariance should be positive semidefinite.\")\n\n    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n    # Using a special case to obtain the eigenvalues of this\n    # two-dimensionl dataset.\n    ell_radius_x = np.sqrt(1 + pearson)\n    ell_radius_y = np.sqrt(1 - pearson)\n    ellipse = Ellipse((0, 0),\n        width=ell_radius_x * 2,\n        height=ell_radius_y * 2,\n        facecolor=facecolor,\n        **kwargs)\n\n    # Calculating the stdandard deviation of x from\n    # the squareroot of the variance and multiplying\n    # with the given number of standard deviations.\n    scale_x = np.sqrt(cov[0, 0]) * n_std\n    mean_x = mean[0]\n\n    # calculating the stdandard deviation of y ...\n    scale_y = np.sqrt(cov[1, 1]) * n_std\n    mean_y = mean[1]\n\n    transf = transforms.Affine2D() \\\n        .rotate_deg(45) \\\n        .scale(scale_x, scale_y) \\\n        .translate(mean_x, mean_y)\n\n    ellipse.set_transform(transf + ax.transData)\n    return ax.add_patch(ellipse)\n\n\n@gif.frame\ndef plot_samples(samples, num_samples, tmp_points, num_tmp, title=\"Gibbs Sampling\", xlims=(-11, 11), ylims=(-11, 11)):\n    fig = plt.figure(figsize=(15, 12))\n    ax = fig.gca()\n    \n    # Plot the true distribution\n    plot_gaussian_from_parameters(mean, cov, ax, n_std=2, edgecolor='g', alpha=0.5, label=\"True Distribution\")\n    \n    # Plot sampled points\n    ax.scatter(samples[:num_samples, 0], samples[:num_samples, 1], c='b', s=10, label=\"Sampled Points\")\n    ax.scatter(samples[0, 0], samples[0, 1], marker='*', c='g', s=60, label=\"Initial Point\")\n    \n    # Plot samples from conditional distribution\n    ax.scatter(tmp_points[:num_tmp, 0], tmp_points[:num_tmp, 1], c='r', alpha=0.4, s=5, label=\"Temporary Points\")\n    \n    # Keeping the axes scales same for good GIFS\n    ax.set_xlim(xlims)\n    ax.set_ylim(ylims)\n    \n    # Plot lines\n    if(num_tmp > 0):\n        ax.plot([samples[num_samples-1, 0], tmp_points[num_tmp-1, 0]], \n                [samples[num_samples-1, 1], tmp_points[num_tmp-1, 1]], c='k', alpha=0.25)\n        \n        # Plot estimated Gaussian, ignoring the starting point\n        if(num_samples > 2):\n            plot_gaussian_from_points(samples[1:num_samples, 0], samples[1:num_samples, 1], \n                                      ax, n_std=2, edgecolor='b', alpha=0.5, label=r\"p(x_1|x_0)\")\n        # Plot estimated Gaussian, ignoring the starting point\n        if(num_tmp > 2):\n            plot_gaussian_from_points(tmp_points[1:num_tmp, 0], tmp_points[1:num_tmp, 1], \n                                      ax, n_std=2, edgecolor='r', alpha=0.5, label=r\"p(x_0|x_1)\")\n    \n    \n    ax.legend(loc='upper left',fontsize=22)\n    ax.set_title(title,fontsize=22)\n\n\n\n\n\nResume Example\nOk! After all that plotting setup, we can actually run our example!\nLets specify the true distribution:\n\nmean = np.array([0, 0])\ncov = np.array([[10, 3], \n                [3, 5]])\n\n\n# Plot true distribution\nfig = plt.figure(figsize=(10, 7))\nax = fig.gca()\nplot_gaussian_from_parameters(mean, cov, ax, n_std=2, edgecolor='g', label=\"True Distribution\")\nax.scatter(mean[0], mean[1], c='g')\nax.set_xlim((-11, 11))\nax.set_ylim((-11, 11))\nax.legend(loc='upper left');\n\n\n\n\nThis is the distribution we want to sample from.\nLets run our Gibbs sampler:\n\ninitial_point = [-9.0, -9.0]\nnum_samples = 100\nsamples, tmp_points, frames = gibbs_sampler(initial_point, num_samples, mean, cov, create_gif=True)\n\nNow lets create the gif so we can visualize it!\n\n# Creating the GIF\nif reanimate_gifs:\n    gif.save(frames, \"gibbs.gif\", duration=150)\n\n\n\nüßêPause-and-ponder: Whoa! Lets break this down and discuss!"
  },
  {
    "objectID": "posts/nn/nn.html",
    "href": "posts/nn/nn.html",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "",
    "text": "In this notebook we will walk through the forward and backward computation directions (fowardprop/backprop) of neural networks.\nWe will start with a simple 1D network and work our way up to a fully connected neural network of arbitrary width and depth.\nIn so doing, we will have to ‚Äúlinear-algebrize‚Äù (vectorize) evertthing to make sure its fast and efficient.\nThis notebook aims to walk through all of these steps in detail, and by the end, we will generate the following animation to better understand how a networks ‚Äúhidden‚Äù representations evolve through training:\n\n\n\nThis document was influeced a great deal by these two fantastic resources:\n\n3blue1brown - Backpropagation calculus\nMichael Nielsen - Neural Networks and Deep Learning - Chapter 2\n\nThis document fully explains, in detail, and in simple (slow) python, and in vectorized numpy, what is going on forward/back prop, carefully explaining the required linear algebra along the way.\n\n\n\n\n2023-02-23: This notebook was first moved over to the quarto publishing system and placed online.\n\n\n\n\n\nimport numpy as np\nimport sklearn\nimport sklearn.datasets\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport seaborn as sns\n\n# for creating animations\nimport matplotlib.animation\nfrom IPython.display import HTML\n\n\n# styling additions\nfrom IPython.display import HTML\n# style = \"<style>div.warn{background-color: #fcf2f2;border-color: #dFb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;}</style>\"\nstyle = \"<style>div.info{padding: 15px; border: 1px solid transparent; border-left: 5px solid #dfb5b4; border-color: transparent; margin-bottom: 10px; border-radius: 4px; background-color: #fcf8e3; border-color: #faebcc;}</style>\"\nHTML(style)"
  },
  {
    "objectID": "posts/nn/nn.html#dataset",
    "href": "posts/nn/nn.html#dataset",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "Dataset",
    "text": "Dataset\nLets generate a simple 1-D toy dataset:\n\ndef toy():\n    N = 1000\n    inputs = np.linspace(-5,5,N).reshape(N,1)\n    labels = ((inputs>0).astype(int)*2-1).reshape(N,1)\n    return inputs,labels\n\n\ninputs,labels = toy()\ninputs[0:5],labels[0:5]\n\n(array([[-5.        ],\n        [-4.98998999],\n        [-4.97997998],\n        [-4.96996997],\n        [-4.95995996]]),\n array([[-1],\n        [-1],\n        [-1],\n        [-1],\n        [-1]]))"
  },
  {
    "objectID": "posts/nn/nn.html#d-network",
    "href": "posts/nn/nn.html#d-network",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "1-D Network",
    "text": "1-D Network\nLets give the simplest network we can imagine. One that consists of a few 1-D ‚Äúlayers‚Äù of size 1! We have an: * input node/layer, * hidden node/layer:h_1, * another hidden node/layer h_2 * output node/layer.\nNetworks are typically drawn with the weights on the wires. Our simple network can be given as: \n\nNote: This is sometimes confusing, as technically its the activations of the previous layer that ‚Äúflow‚Äù into the next layer. However, the weights are what we are tying to learn, and their relative strengths tells us something about the structure of the network.\n\nEach node has an associated: * weight * bias * activation function\nIn our example, we have w_1,b_1,w_2,b_2,w_3,b_3 as our parameters and we are using the sigmoid as our activation function.\nThe function of each node, is to apply its own weight and bias to a previous layers activation value, and then pass it through its activation function to produce its own activation. For example, h_1 is doing:\n\na_1 = \\sigma(w\\cdot x + b)\n\nThe input to a nodes activation function is useful to think about separately, so we can introduce an additional variable to denote it as: \nz_i = w\\cdot x + b, \\quad\\quad a_1 = \\sigma(z_i)\n\nSo, we can describe the behavior of each of our nodes as:\n\n\nImplementation\nNow we can actually implement this simple network. Lets start by recalling the sigmoid function:\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nplt.plot(np.linspace(-5,5),sigmoid(np.linspace(-5,5)));\n\n\n\n\nLets define the function of a node, being careful to match our notation:\n\ndef node(w,b,a):\n    z = w*a + b\n    return sigmoid(z)\n\nNow we are ready to define our actual network! Lets initialize our random weights and biases:\n\nw1,b1,w2,b2,w3,b3 = np.random.rand(6)\n\nNow, lets pick a training example and run it through our network:\n\ninput,label = inputs[0],labels[0]\n\n# h1 node - operates on our input\na1 = node(w1,b1,input)\n\n# h2 node - operates on a1\na2 = node(w2,b2,a1)\n\n# output node - operates on a2 - produces our output\noutput = node(w3,b3,a2)\ny_hat = output\n\nLets see how we did!\n\nprint(f\"for input: {input} with label: {label}, this network calculated: {y_hat}\")\n\nfor input: [-5.] with label: [-1], this network calculated: [0.721239]\n\n\nAs we know, to actually see how we did, we need to define a cost! Lets proceed with the usual average-MSE:\n\ndef mse(y_hat,y,average=True):\n    if average:\n        return np.mean((y_hat-y)**2)\n    else:\n        return ((y_hat-y)**2)\n\n\ndef cost_mse(y_hat,y):\n    return 0.5*np.linalg.norm(y-y_hat)**2\n\n\nprint(f\"for input: {input} with label: {label}, this network calculated: {output}, giving us a cost of: {mse(output,label)}\")\n\nfor input: [-5.] with label: [-1], this network calculated: [0.721239], giving us a cost of: 2.9626637101159607\n\n\nOk - so we‚Äôve demonstrated the entire forward direction, for one sample. Lets define a function for this simple network, so we can run our entire training set through!\n\ndef simple_network(inputs):\n    outputs = []\n    N = inputs.shape[0]\n    \n    # initialized weights\n    w1,b1,w2,b2,w3,b3 = np.random.rand(6)\n\n    for i in range(N):\n        input = inputs[i,:]\n        \n        # h1 node\n        a1 = node(w1,b1,input)\n\n        # h2 node\n        a2 = node(w2,b2,a1)\n\n        # output node\n        output = node(w3,b3,a2)\n        \n        # append to form output\n        outputs.append(output)\n    \n    return np.asarray(outputs)\n\nWe can now calculate our average loss over the entire training set:\n\nmse(simple_network(inputs),labels)\n\n1.6587087276822514"
  },
  {
    "objectID": "posts/nn/nn.html#multidimensional-network",
    "href": "posts/nn/nn.html#multidimensional-network",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "Multidimensional Network",
    "text": "Multidimensional Network\nWe‚Äôve looked at a very simple example above which captures the essence of what we are doing. However, our networks are really never actually composed of layers with one element each. Instead, each layer has multiple nodes.\nTechnically, we could continue in the same way as above and individually number our weights and biases but this quickly gets out of hand! As an exercise, try repeating the above analysis and implementation where each hidden layer is now of size 2!\nIronically, to avoid this notational complexity, it seems like we must introduce additional notation, and rewrite our problem in the language of linear algebra.\nTo introduce this notation, lets imagine a network the following structure: \nTo make our analysis simpler, lets zoom in on node node, and all of its input weights:\n\nWe‚Äôve highlighted the 0th node in our last hidden layer and all of its inputs from the previous layer in \\color{orange}{\\text{orange}}. We‚Äôve also numbered each of it input nodes with their layer-specific numbering and shown them in \\color{blue}{\\text{blue}}.\nWe‚Äôve also named each weight according to the following format:\n\nw_{\\;\\color{orange}{\\text{current node #}} \\;,\\; \\color{blue}{\\text{incoming node #}}}\n\nThis may seem a bit counter intuitive at first, as the tendency when reading from left to write is to want to write our weights as:\n\nw_{\\;\\color{blue}{\\text{incoming node #}} \\;,\\; \\color{orange}{\\text{current node #}} }\n\nYou absolutely can, but that will result in a bunch of transposes in later equations. To get rid of them now, we will number our weights as we did above. As is typically the case, people make seemingly weird/arbitrary decisions at the front to result in simplifications down the line.\nRecall, the function of a node is to apply its weight to the activation of its input/previous layer. In this case, we have three previous nodes/input nodes, so we will also write them in \\color{blue}{\\text{blue}} to make it clear that they are coming from the previous layer.\nSo our orange node is performing:\n\n\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{0}} \\cdot \\color{blue}{a_0} +\n\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{1}} \\cdot \\color{blue}{a_1} +\n\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{2}} \\cdot \\color{blue}{a_2} +\n\\color{orange}{b_0}\n\nAlready, our eyes should be screaming dot-product!\n\nNotation\nIndeed, we can form a vector of the previous layer‚Äôs activations as:\n\n\\color{blue}{\\mathbf{a}_{prev}} =\n\\begin{bmatrix}\n\\color{blue}{a_0 \\\\\na_1 \\\\\na_2}\n\\end{bmatrix}\n\nand a vector of the 0-th neurons weights in the current layer as:\n\n\\color{orange}{\\mathbf{w}_0} =\n\\begin{bmatrix}\n\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{0}} \\\\\n\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{1}} \\\\\n\\color{orange}{w}_{\\color{orange}{0},\\color{blue}{2}}\n\\end{bmatrix}\n\nwhere we have used color again to make it clear what layer we are talking about: the current or previous layer.\nThen we can rewrite what our orange node is calculating as: \n\\begin{align}\n\\color{orange}{z_0} &= \\color{orange}{\\mathbf{w}_0} \\cdot \\color{blue}{\\mathbf{a}_{prev}} + \\color{orange}{b_0} \\\\\n\\color{orange}{a_0} &= \\sigma(\\color{orange}{z_0})\n\\end{align}\n\nWell, we‚Äôve managed to rewrite the activation of one-node in slightly better notation. But we can we do better! Lets now reason about the entire layer!\nRecall, we already have a vector of the previous layer‚Äôs activations in \\color{blue}{\\mathbf{a}_{prev}}, although we never actually gave a formula for it. Based on the formula for the 0th-node in the current layer we just gave, lets try to give a vector of activations for the entire current layer.\n(Note: to prevent a color explosion, since we‚Äôre talking about the current layer, I will drop orange to refer to it most places. I will keep blue to refer to the previous layer).\nTo simplify our analysis, lets first note that: \n\\mathbf{a}_{curr} =\n\\begin{bmatrix}\n\\sigma (z_0) \\\\\n\\sigma (z_1) \\\\\n\\sigma (z_2) \\\\\n\\end{bmatrix} =\n\\sigma\\left(\\;\n\\begin{bmatrix}\nz_0 \\\\\nz_1 \\\\\nz_2 \\\\\n\\end{bmatrix}\\;\n\\right) =\n\\sigma (\\mathbf{z}_{curr})\n\nSo, lets focus on writing a formula for the vector \\mathbf{z}_{curr}:\n\n\\mathbf{z}_{curr} =\n\\begin{bmatrix}\n\\mathbf{w}_0 \\cdot \\color{blue}{\\mathbf{a}_{prev}} + b_0 \\\\\n\\mathbf{w}_1 \\cdot \\color{blue}{\\mathbf{a}_{prev}} + b_1 \\\\\n\\mathbf{w}_2 \\cdot \\color{blue}{\\mathbf{a}_{prev}} + b_2 \\\\\n\\end{bmatrix}\n\nLets make it a bit clearer by writing our biases for the entire layer as a separate vector \\mathbf{b}:\n\n\\mathbf{z}_{curr} =\n\\begin{bmatrix}\n\\mathbf{w}_0 \\cdot \\color{blue}{\\mathbf{a}_{prev}} \\\\\n\\mathbf{w}_1 \\cdot \\color{blue}{\\mathbf{a}_{prev}} \\\\\n\\mathbf{w}_2 \\cdot \\color{blue}{\\mathbf{a}_{prev}}  \\\\\n\\end{bmatrix}\n+\n\\mathbf{b}\n\nJust like we saw when we discussed linear regression, this vector of dot products is exactly the matrix-vector product of the weight matrix and the previous layers activation vector!\n\nDefinition: The current layers weight matrix: \\mathbf{W} is a matrix of k-many rows, and j-many columns, where k is the number of nodes in the current layer, and j is the number of nodes in the previous layer:\n\n\\mathbf{W} \\in \\mathbb{R}^{k,j}=\n\\begin{bmatrix}\n\\mathbf{w}_0^T \\\\\n\\mathbf{w}_1^T \\\\\n\\ldots \\\\\n\\mathbf{w}_k^T\n\\end{bmatrix} =\n\\begin{bmatrix}\nw_{0,0} & w_{0,1} & \\ldots & w_{0,j} \\\\\n\\ldots \\\\\nw_{k,0} & w_{0,1} & \\ldots & w_{k,j} \\\\\n\\end{bmatrix}\n\nNote: each row of the weight matrix represents all inputs to a specific node in the current layer.\n\nNow, we can finally write a complete linear algebraic equation for the function of a current layer on a previous layer:\n\n\\begin{align}\n\\mathbf{z}_{curr} &= \\mathbf{W}\\color{blue}{\\mathbf{a}_{prev}}+\\mathbf{b} \\\\\n\\mathbf{a}_{curr} &= \\sigma(\\mathbf{z}_{curr})\n\\end{align}\n\nNow, neural networks do this sequentially, so the last piece of the puzzle is to be able to refer to a specific layer by number. We now introduce the final piece of notation to let us do this: a superscript to designate the layer number:\n\nThe activation of layer L is given by:\n\n\\begin{align}\n\\mathbf{z}^L &= \\mathbf{W}^L \\mathbf{a}^{L-1}+\\mathbf{b}^L \\\\\n\\mathbf{a}^L &= \\sigma(\\mathbf{z}^L)\n\\end{align}\n\nThis is often written succinctly as:\n\n\\boxed{\\mathbf{a}^L = \\sigma(\\mathbf{W}\\mathbf{a}^{L-1} + \\mathbf{b})}\n\nwhere the specific \\mathbf{W},\\mathbf{b} we are talking about is implied.\n\n Wow we‚Äôve come a long way! We‚Äôve given a very clear and succinct linear algebraic equation for the entire forward direction for a network of any number of layers and size of each layer!\nLets perform a size sanity check: * \\mathbf{W} is of size k \\times j, where j is the number of neurons in the previous layer. * \\mathbf{a}^{L-1} is a vector of size j \\times 1, the activations of the previous layer. * Their multiplication results in a vector of size k \\times 1, where k is the number of neurons in the current layer. * Our bias vector is also k \\times 1 (as we expect!).\nSo everything works as expected!\nThis is why we decided to write our weight matrix to be of size k \\times j or \\text{# neurons in prev layer} \\times \\text{# neurons in current layer} instead of the other way around. If we had, we‚Äôd need a transpose in the equation above.\n\n\n\nImplementation\nArmed with our new notation, lets write an implementation of the network we gave above:\n\n\nüßêPause-and-ponder: What should the sizes of each W be for this network? Lets go through it together!\n\nOk! Now lets implement it!\n\ninputs,labels = toy()\n\n\ndef simple_network2(inputs):\n    outputs = []\n    N = inputs.shape[0]\n    \n    # initialize weight matrices - notice the dimensions\n    W_1 = np.random.randn(3,1)\n    W_2 = np.random.randn(2,3)\n    W_3 = np.random.randn(1,2)\n    \n    # and our biases\n    b_1 = np.random.randn(3,1)\n    b_2 = np.random.randn(2,1)\n    b_3 = np.random.randn(1,1)\n    \n    # loop through training data\n    for i in range(N):\n        \n        # correct size for current input\n        input = inputs[i,:]\n        \n        # layer 1\n        a_1 = sigmoid(W_1*input + b_1)\n\n        # layer 2\n        a_2 = sigmoid(W_2.dot(a_1)+b_2)\n\n        # output layer\n        output = sigmoid(W_3.dot(a_2)+b_3)\n        \n        # append to form output\n        outputs.append(output)\n    \n    return np.squeeze(np.asarray(outputs)).reshape(-1,1)\n\n\noutputs = simple_network2(inputs)\nplt.scatter(inputs,outputs,c=labels);"
  },
  {
    "objectID": "posts/nn/nn.html#mini-batch-notation",
    "href": "posts/nn/nn.html#mini-batch-notation",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "Mini-Batch Notation",
    "text": "Mini-Batch Notation\nNote, this implementation runs an example through the network one-by-one! Instead, we can imagine feeding our entire dataset through at once! This formalism will pave the way for us later to feed in a mini-batch at a time.\nLets just reason our way through this one from first-principles (fancy way to say: lets match our matrix sizes!), seeing how we get our entire dataset through the first hidden layer.\nThe first weight matrix is of size:\n\n\\mathbf{W}_1: (\\text{size of hidden layer} \\times \\text{dimension of input})\n\nwhich in this case is: 3 \\times 1. If our input was 2-D, it would be 3 \\times 2. So what we dot it with, needs to be of size: \\text{dimension of input} \\times \\text{??}.\nIts in that dimension that we can place our entire dataset!\nSo, now we‚Äôre going to be shuffling activation matrices around! In these activation matrices, each column is an activation for the last layer on a different training example!\n\nSo we expect the first activation matrix to be of size: \\text{dimension of input} \\times \\text{number of samples}. This means this must also be the size of the initial input matrix for the first hidden layer.\nSo, we can rewrite our layer propagation equation above for our entire dataset:\n\n\\boxed{\\mathbf{A}^L = \\sigma(\\mathbf{W}\\mathbf{A}^{L-1} + \\mathbf{b})}\n\nwhere we use broadcasting rules to let us add a vector \\mathbf{b} to a matrix.\n\nLets make a special note about the first hidden layer, and how it processes our input matrix.\nTypically, we imagine our data matrix such that the first dimension is the batch_size:\n\nThis means each row of this matrix, corresponds to one sample.\nSo if our data matrix is of size \\text{batch_size} \\times \\text{dimension}, in order for our first layer to calculate correctly, we have to make the sizes work! Meaning, our first layer should be:\n\n\\mathbf{A}^1 = \\sigma(\\mathbf{W}^1\\mathbf{X}^T + \\mathbf{b})\n\nwhere X^T is:\n\n\nNote: Here we define the input matrix X to be of size: N \\times d. That is why we transpose it in the first layer. This is by no means universal, and different numerical libraries do it differently. You might come across libraries or papers to talks, where the input matrix is defined to be of size d \\times N. If that is the case, the first layer does not need an X^T!\n\nNow we can implement this simple network, using this notation!\n\ndef simple_network2_batch(inputs):\n    # assume inputs is of shape Nxd\n    \n    # initialize weight matrices - notice the dimensions\n    W_1 = np.random.randn(3,1)\n    W_2 = np.random.randn(2,3)\n    W_3 = np.random.randn(1,2)\n    \n    # and our biases\n    b_1 = np.random.randn(3,1)\n    b_2 = np.random.randn(2,1)\n    b_3 = np.random.randn(1,1)\n    \n    # NOTE - there is no for loop here! \n    \n    # layer 1\n    a_1 = sigmoid(W_1.dot(X.T) + b_1)\n\n    # layer 2\n    a_2 = sigmoid(W_2.dot(a_1)+b_2)\n\n    # output layer\n    output = sigmoid(W_3.dot(a_2)+b_3)\n    \n    return np.squeeze(np.asarray(outputs)).reshape(-1,1)\n\nNow lets actually run our entire dataset through (since we aren‚Äôt yet dealing with batches), to generate outputs:\n\ninputs,labels = toy()\ny_hat = outputs = simple_network2(inputs)\n\nJust for fun, lets plot it!\n\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,7))\nax1.set_title('Original poits and labels')\nax1.scatter(inputs,labels);\n\nax1.set_title('Original poits and Y_Hat')\nax2.scatter(inputs,outputs,color='orange');\n\n\n\n\nNote the huge difference in our y-axis! This is without and training so its bound to be bad!\n\nPause-and-ponder: Go back and re run the network above and re-plot the final representation. Notice how random it is! This is because we initialize with random weights, but never actually do any training!"
  },
  {
    "objectID": "posts/nn/nn.html#dealing-with-biases",
    "href": "posts/nn/nn.html#dealing-with-biases",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "Dealing with Biases",
    "text": "Dealing with Biases\nAnother very common notational trick people do, as we saw in linear regression, was to add ‚Äúanother dimension‚Äù to ‚Äúautomatically‚Äù deal with our bias.\nHere, that means adding another ‚Äúnode‚Äù to each layer. This node has no input, only outputs, and its activation is always the value 1.\n\n\nQuestion: How does this act as a bias?\n\nWell, lets look at what this means for a particular node. Lets once again highlight the orange node:\n\nWhat effect does this have for this orange nodes update? Well, lets write out what it is:\n\nSo, \\color{orange}{w}_{\\color{orange}{0},\\color{blue}{3}} is always being multiplied by the value 1. This is exactly the role of the bias!\n\nAs we can see, the addition of a constant node in a layer gives an extra weight to each node in the next layer. This weight, multiplied by 1, acts as the bias for each node.\nSo all we have to do, is add an extra column to each weight matrix in our network. (Note: often, this bias is omitted from the final layer).\nNow, for a layer L, the weight matrix is of size: k \\times j+1, where k is the number of actual/real hidden nodes in layer L, and j is the number of actual/real hidden nodes in layer L-1.\nNote: often this is drawn with the bias nodes on top of the others, not below.\n\nSo we would think of the 0-th weight acting as the bias, so we would add an extra column to the left/right of each weight matrix. Its ultimately the same thing\nAs a sanity check that this works, lets compare the output of the previous implementation with this new notation. To do so, we need to make sure they use the same initialization. Lets take it out of the function and ‚Äúmake it flat‚Äù so we can easily compare:\n\n# W and b network\n# initialize weight matrices - notice the dimensions\nW_1 = np.random.randn(3,1)\nW_2 = np.random.randn(2,3)\nW_3 = np.random.randn(1,2)\n\n# and our biases\nb_1 = np.random.randn(3,1)\nb_2 = np.random.randn(2,1)\nb_3 = np.random.randn(1,1)\n\n\n# W network - adding biases to the right:\nW_1_b = np.hstack((W_1,b_1))\nW_2_b = np.hstack((W_2,b_2))\nW_3_b = np.hstack((W_3,b_3))\n\nLets run the ‚Äúold network‚Äù notation:\n\n# layer 1\na_1_old = sigmoid(W_1.dot(inputs.T) + b_1)\n\n# layer 2\na_2_old = sigmoid(W_2.dot(a_1_old)+b_2)\n\n# output layer\noutput_old = sigmoid(W_3.dot(a_2_old)+b_3)\noutput_old = np.squeeze(np.asarray(output_old)).reshape(-1,1)\n\nAnd now the ‚Äúnew‚Äù network notation. Note: in order to run the inputs through, we need to add the ‚Äúextra dimension of 1‚Äôs‚Äù, as we‚Äôve done many times before!\n\ninputs_new = np.c_[inputs,np.ones(inputs.shape[0])]\n\nNow we can go ahead and ‚Äúfeed it in‚Äù\n\n# layer 1\na_1_new = sigmoid(W_1_b.dot(inputs_new.T))\n# append the 1 to the end of the previous layers activations:\na_1_new = np.r_[a_1_new, np.ones((1,a_1_new.shape[1]))]\n\n# layer 2\na_2_new = sigmoid(W_2_b.dot(a_1_new))\n# append the 1 to the end of the previous layers activations:\na_2_new = np.r_[a_2_new, np.ones((1,a_2_new.shape[1]))]\n\n# output layer\noutput_new = sigmoid(W_3_b.dot(a_2_new))\noutput_new = np.squeeze(np.asarray(output_new)).reshape(-1,1)\n\nLets verify they are both equal:\n\nnp.equal(output_new,output_old).all()\n\nFalse\n\n\n\nNote: This might seem like a strange hack - We have to reshape each layers weight matrix, and each layers activation matrix to account for this extra ‚Äú1‚Äù flying around everywhere.\nI will not try to convince you one way or the other which makes the most sense. I‚Äôm just explaining it here in case it is useful to you to think of the bias as ‚Äúbeing wrapped up‚Äù in our weight matrix, as it was when we discussed linear regression.\nMoving forward, we will not be using this notation in the rest of the notebook."
  },
  {
    "objectID": "posts/nn/nn.html#activation-functions",
    "href": "posts/nn/nn.html#activation-functions",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "Activation Functions",
    "text": "Activation Functions\nLets imagine revisiting the previous network structure under different activation functions:\n\nLinear\nLets start with a linear activation function:\n\ndef linear(z):\n    return z\n\nThis is almost always called a linear activation, but is better thought of as the identity activation - that is, it just returns what it was given, unaltered.\nWhat does this mean for the computation our network can perform?\nOnce again, lets flatten our implementation to take a look:\n\ninputs,labels = toy()\n\n\n# initialize weight matrices - notice the dimensions\nW_1 = np.random.randn(3,1)\nW_2 = np.random.randn(2,3)\nW_3 = np.random.randn(1,2)\n\n# and our biases\nb_1 = np.random.randn(3,1)\nb_2 = np.random.randn(2,1)\nb_3 = np.random.randn(1,1)\n\n\n# layer 1\na_1 = linear(W_1.dot(inputs.T) + b_1)\n\n# layer 2\na_2 = linear(W_2.dot(a_1)+b_2)\n\n# output layer\noutputs = linear(W_3.dot(a_2)+b_3)\noutputs = np.squeeze(np.asarray(outputs)).reshape(-1,1)\n\nLets plot it:\n\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,7))\nax1.set_title('Original poits and labels')\nax1.scatter(inputs,labels);\n\nax1.set_title('Original poits and Y_Hat')\nax2.scatter(inputs,outputs,color='orange');\n\n\n\n\n\nüßê Pause-and-ponder: Go back and re-run the above a few times. What does this mean?\n\nRemember, what we are visualizing is the inputs and the corresponding output value - what our network has been able to map to.\nYou might be noticing that this is strangely linear! Indeed! With linear activation functions, all we are able to do is calculate some linear combination of our inputs!\nLets dig in a bit more, and show what we are actually calculating at Layer L:\n\nA^L = W_3((W_2(W_1X^T + b_1) + b_2) + b_3\n\nWell, lets distribute this out: \nA^L = W_3W_2W_1X^T + W_3W_2b_1 + W_3b_2 + b_3\n\nOk, wait a second‚Ä¶ this is getting confusing - lets check to make sure the sizes work out!\n\n\nLets go through this part together in class!\n\nOk! We can see that the sizes do in fact work out!\n\nüßêPause-and-ponder: In your resulting equation, perform some clarifying substitutions. What do we discover?\n\nAfter some substitutions, we can write an equation like the following:\n\n\\hat{\\mathbf{Y}} = \\mathbf{W}^*X^T + \\mathbf{b}^*\n\nWhich tells us that a NN with only linear activations, is ultimately just another linear function of its inputs! It doesn‚Äôt matter how deep or wide it is!\n\n\nA Final Sigmoid\nGiven our understanding above, what would happen if we only add a sigmoid at the end? Something like:\n\n\nüßêPause-and-ponder: What do you think it represents? What is its representational capacity?"
  },
  {
    "objectID": "posts/nn/nn.html#latex-definitions",
    "href": "posts/nn/nn.html#latex-definitions",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "Latex definitions",
    "text": "Latex definitions\nThis cell just creates some latex definitions we will need later.\n\n\\newcommand{\\bx}[1]{\\color{purple}{b^{#1}}}\n\\newcommand{\\bl}{\\color{purple}{b^L}}\n\\newcommand{\\wx}[1]{\\color{blue}{w^{#1}}}\n\\newcommand{\\wl}{\\color{blue}{w^L}}\n\\newcommand{\\wone}{\\color{blue}{w_1}}\n\\newcommand{\\ax}[1]{\\color{green}{a^{#1}}}\n\\newcommand{\\al}{\\color{green}{a^L}}\n\\newcommand{\\zx}[1]{\\color{orange}{z^{#1}}}\n\\newcommand{\\zl}{\\color{orange}{z^L}}\n\\newcommand{\\ap}{\\color{green}{a^{L-1}}}\n\\newcommand{\\czero}{\\color{red}{C_0}}\n\\newcommand{\\dc}{\\color{red}{\\partial C_0}}\n\\newcommand{\\dw}[1]{\\color{blue}{\\partial \\wx{#1}}}\n\\newcommand{\\dz}[1]{\\color{orange}{\\partial \\zx{#1}}}\n\\newcommand{\\da}[1]{\\color{green}{\\partial \\ax{#1}}}\n\\newcommand{\\db}[1]{\\color{purple}{\\partial \\bx{#1}}}\n\\newcommand{\\dap}{\\color{green}{\\partial \\ax{L-1}}}\n\\newcommand{\\dcdw}[1]{\\frac{\\dc}{\\dw{#1}}}\n\\newcommand{\\dcdz}[1]{\\frac{\\dc}{\\dz{#1}}}\n\\newcommand{\\dzdb}[1]{\\frac{\\dz{#1}}{\\db{#1}}}\n\\newcommand{\\dzdw}[1]{\\frac{\\dz{#1}}{\\dw{#1}}}\n\\newcommand{\\dadz}[1]{\\frac{\\da{#1}}{\\dz{#1}}}\n\\newcommand{\\dcda}[1]{\\frac{\\dc}{\\da{#1}}}\n\\newcommand{\\dcdb}[1]{\\frac{\\dc}{\\db{#1}}}\n\\newcommand{\\dcdap}{\\frac{\\dc}{\\dap}}\n\\newcommand{\\deltal}{\\delta^L}\n\\newcommand{\\deltax}[1]{\\delta^{#1}}\n\nNote: Make sure you run this cell!"
  },
  {
    "objectID": "posts/nn/nn.html#d-case",
    "href": "posts/nn/nn.html#d-case",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "1-D Case",
    "text": "1-D Case\nNote: This presentation follows very closely these two fantastic resources * 3blue1brown - Backpropagation calculus * Michael Nielsen - Neural Networks and Deep Learning - Chapter 2\nLets revisit the simplest network we started with at the beginning of this notebook:\n\nNotation Note: we‚Äôre introducing a new notation \\color{blue}{w_3 = w^L, w_2 = w^{L-1}}, \\ldots and we will be using them interchangeably.\nTo focus our discussion, lets just focus on the last two levels, and label their activation values:\n\nThe output layer defines the ‚Äúrepresentation‚Äù our network has created for a specific input:\n\n\\begin{align}\n\\zl &= \\wl\\ap+\\bl \\\\\n\\al &= \\sigma(\\zl)\n\\end{align}\n\nWhere now we are using color to denote the kind of variable we are talking about. For example, activations for any layer are green.\nSo, its with this layers activation that we want to measure our cost, on a specific example x_0 run through our network:\n\n\\czero = (\\al - y)^2\n\nAnother way to think about this process, is as this computational graph:\n\nThis tells a ‚Äúcausal‚Äù story, about what variables are needed to compute other variables. Note: this could be carried even further back through the network, all the way to the inputs!\n\nNote: This is a ‚Äúlight weight‚Äù/conceptual computational graph. Its a way to introduce the concept of backpropagating partial derivatives through a graph using the chain rule.\n\nLets try to understand exactly what a ‚Äúpartial derivative‚Äù like \\dcdw{L} is telling us, by associating a little number line with each of these variables:\n\nPictorially, this is telling us that a little nudge to a weight, results in a nudge to the ‚Äúactivity‚Äù/\\zl of the neuron (Q: how big/how small of a nudge?), which then results in a nudge to the activation/\\al of the neuron (Q: how big/how small of a nudge?) which then results in a nudge to the total cost of the network (how big/how small of a nudge?).\nSo conceptually, the partial \\dcdw{L} is capturing:\n\n\nSimulation\nWe actually go ahead and simulate this to better understand what its telling us! Lets start by running an example through our network:\n\ninputs,outputs = toy()\ninput,output = inputs[0],outputs[0]\n\n\n# initialize weights\nw_1 = np.random.randn()\nw_2 = np.random.randn()\nw_3 = 1 #exagerating for plotting purposes\n\n# and our biases\nb_1 = np.random.randn()\nb_2 = np.random.randn()\nb_3 = np.random.randn()\n\n\n# layer 1\nz_1 = w_1*input+b_1\na_1 = sigmoid(z_1)\n\n# layer 2\nz_2 = w_2*a_1+b_2\na_2 = sigmoid(z_2)\n\n# output layer\nz_3 = w_3*a_2+b_3\na_3 = sigmoid(z_3)\n\n\ncost_0 = (output - a_3)**2\n\n\n\nNudge \\color{blue}{w^L}\nNow lets give \\wl a ‚Äúlittle nudge‚Äù, and see how it propagates through the network!\nNote: This is not really a little nudge. Ive greatly exaggerated it to provide a good looking plot.\n\nnudge = np.linspace(-5,5,51)\nz_3_nudged = (w_3 + nudge)*a_2 + b_3\na_3_nudged = sigmoid(z_3_nudged)\ncost_0_nudged = (a_3_nudged - output)**2\n\nLets plot it!\n\nfig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,7),sharex=True)\nax1.plot(w_3+nudge,z_3_nudged,c=\"orange\");\nax1.scatter(w_3,w_3*a_2+b_3,s=100,c='orange');\nax1.set_xlabel('w_3 + nudge');\nax1.set_ylabel('z_3');\nax1.set_title(\"How a nudge in w_3 affects z_3\")\n\nax2.plot(w_3+nudge, a_3_nudged,c='green');\nax2.scatter(w_3,sigmoid(w_3*a_2+b_3),s=100,c='green');\nax2.set_xlabel('w_3 + nudge');\nax2.set_ylabel('a_3');\nax2.set_title(\"How a nudge in a_3 affects z_3\")\n\nax3.plot(w_3+nudge, cost_0_nudged,c='red');\nax3.scatter(w_3,(sigmoid(w_3*a_2+b_3)-output)**2,s=100,c='red');\nax3.set_xlabel('w_3 + nudge');\nax3.set_ylabel('C_0');\nax3.set_title(\"How a nudge in w_3 affects C_0\");\n\n\n\n\nWhat is this telling us? That a little nudge in w_3 results in very different changes to each the variables down our computational graph!\n\n\nNudge \\color{blue}{w_1}\nLets repeat this simulation, except go more back/backer to see how a nudge at w_1 affects our output layer and our cost!\n\nnudge = np.linspace(-5,5,51)\nz_1_nudged = (w_1 + nudge)*input + b_1\na_1_nudged = sigmoid(z_1_nudged)\n\nz_2_nudged = w_2*a_1_nudged + b_2\na_2_nudged = sigmoid(z_2_nudged)\n\nz_3_nudged = w_3*a_2_nudged + b_3\na_3_nudged = sigmoid(z_3_nudged)\n\ncost_0_nudged = (a_3_nudged - output)**2\n\nPlot it!\n\nfig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,7),sharex=True)\n\nax1.plot(w_1+nudge,z_1_nudged,c=\"orange\");\nax1.scatter(w_1,w_1*input+b_1,s=100,c='orange');\nax1.set_xlabel('W+nudge');\nax1.set_ylabel('z_1');\nax1.set_title(\"How a nudge in w_1 affects z_1\")\n\nax2.plot(w_1+nudge, a_3_nudged,c='green');\nax2.scatter(w_1,sigmoid(w_3*(sigmoid(w_2*sigmoid(w_1*input+b_1)+b_2))+b_3),s=100,c='green');\nax2.set_xlabel('W+nudge');\nax2.set_ylabel('a_3');\nax2.set_title(\"How a nudge in w_1 affects a_3\")\n\nax3.plot(w_1+nudge, cost_0_nudged,c='red');\nax3.scatter(w_1,(sigmoid(w_3*(sigmoid(w_2*sigmoid(w_1*input+b_1)+b_2))+b_3)-output)**2,s=100,c='red');\nax3.set_xlabel('W+nudge');\nax3.set_ylabel('C_0');\nax3.set_title(\"How a nudge in w_1 affects our cost\");\n\n\n\n\nAh, these graphs have are a bit more interesting, so we can point out the following:\nImplicit in all of these graphs are the following idea:\n\n\\frac{\\text{the amount of change in a dependent variable}}{\\text{the amount of change in a variable that it depends on}}\n\nWell, this is just the rate of change of the graph! Which we might remember! Lets restate this idea for each subplot:\nThe specific rate of change for the first subplot is:\n\n\\frac{\\text{resulting change in }\\color{orange}{z_1}}{\\text{some amount of change in } \\color{blue}{w_1}} = \\frac{\\color{orange}{\\Delta z_1}}{\\color{blue}{\\Delta w_1}}\n\nThe specific rate of change for the second subplot is:\n\n\\frac{\\text{resulting change in }\\color{green}{a_3}}{\\text{some amount of change in } \\color{blue}{w_1}} = \\frac{\\color{green}{\\Delta a_3}}{\\color{blue}{\\Delta w_1}}\n\nThe specific rate of change for the third subplot is:\n\n\\frac{\\text{resulting change in } \\color{red}{C_0}}{\\text{some amount of change in } \\color{blue}{w_1}} = \\frac{\\color{red}{\\Delta C_0}}{\\color{blue}{\\Delta w_1}}\n\nAha! That last subplot is telling us something about how sensitive the cost \\czero is to changes in \\wone. This is what we want!\nWe can see that a little change to \\wone to the left/right (depends on random vals), results in a big change to our final cost on this example!\nThis rate of change/derivative tells us the direction we need to change \\wone in order to reduce our costs!\n\n\n\nWe could do better\nNow, from the plotting code for the last subplot up there you might notice that to generate that last subplot, we basically had to run the entire nudged \\wone + \\Delta all the way through our entire network!\nWe are also reasoning about the derivative by looking at a graph, instead of actually calculating it so that we can use it.\nLets see if we can think of a clever scheme of actually calculating our partials with respect to our weights: \\ldots,\\dcdw{L-1},\\dcdw{L}, by starting with\n\n\\dcdw{L} = ?\n\nLets bring back our computational graph:\n\nLets use the following idea to decompose our graph starting from the end/bottom:\n\n\\frac{\\text{the amount of change in a dependent variable}}{\\text{the amount of change in a variable that it depends on}}\n\n\nStarting at \\czero, we have:\n\n\\dcda{L}\n\nThis object tells us how the cost changes with respect to the final layers activation. Lets calculate it:\n\n\\begin{align}\n\\czero &= (\\al - y)^2 \\\\\n\\dcda{L}  &= 2(\\al -y)\n\\end{align}\n\nWhat happens if \\al is small? Well then it contributes less to our error!\nSo we should focus on making sure that when \\al is large, its correct!\n\nüßêPause-and-ponder: What about \\frac{\\czero}{\\partial y} ? What can we say about this?\n\n\nAccording to our computational graph, thats all of the immediate ‚Äúindependent‚Äù variables \\czero has. So lets now switch our focus to \\al:\n\n\\begin{align}\n\\al &= \\sigma (\\zl) \\\\\n\\dadz{L} &= \\sigma ^{\\color{red}{\\prime}}(\\zl)\n\\end{align}\n\nwhere \\sigma^{\\color{red}{\\prime}}(\\cdot) is the derivative of our sigmoid activation function:\n\n\\sigma^{\\color{red}{\\prime}}(\\cdot) = \\sigma(\\cdot)(1-\\sigma(\\cdot))\n\nThis lets us rewrite the above as:\n\n\\begin{align}\n\\dadz{L} &= \\sigma(\\zl)(1-\\sigma(\\zl))\n\\end{align}\n\n\nWe now have the following partials:\n\n\\begin{align}\n\\dcda{L} &: \\text{how changing } \\al \\text{ changes } \\czero \\\\ \\\\\n\\dadz{L} &: \\text{how changing } \\zl \\text{ changes } \\al\n\\end{align}\n\nIt shouldn‚Äôt require too much convincing that we can multiply these two objects together to create a new partial:\n\n\\begin{align}\n\\dcda{L} \\cdot \\dadz{L} &= \\dcdz{L} \\\\\n                        &= \\text{how changing } \\zl \\text{ changes } \\czero\n\\end{align}\n\nIndeed, the notation itself suggests this by allowing us to ‚Äúcancel out‚Äù partials that appear on top and on bottom:\n\n\n\nThe last step is to actually write this out to get an expression for \\dcdz{L}:\n\n\\begin{align}\n\\dcdz{L} &= \\dadz{L} \\cdot \\dcda{L} \\\\\n         &= \\underbrace{\\sigma(\\zl)(1-\\sigma(\\zl))}_{\\text{how changing }\\zl\\text{ affects }\\al} \\cdot  \\underbrace{2(\\al -y)}_{\\text{how changing }\\al\\text{ affects }\\czero}\n\\end{align}\n\n\nüßêPause-and-ponder: We have just discovered and applied the chain rule! We have used it to backpropagate a change at the output of our last layer: \\dcda{L}, to a change at the input of the activation function of our last layer: \\dadz{L}.\n\n\nüìñSemi-Definition: Backpropagation can be thought of as applying the chain rule back across our computational graph!\n\n\n\n\nLets keep going!\nSo far we‚Äôve backpropagated once. Lets look at our map so far:\n\nLets keep backpropagating (chain-ruling)! We can now look at the inputs \\zl has. Lets focus on one of the most interesting for right now, \\wl:\n\n\\begin{align}\n\\zl &= \\wl\\ax{L-1}+\\bl \\\\\n\\dzdw{L} &= \\ax{L-1}\n\\end{align}\n\n\nüßêPause-and-ponder: This derivative has a particularly interesting interpretation. Its saying that the amount a nudge to our weight in the last layer influences the ‚Äúactivity‚Äù of the last layer, depends on how strong the previous neuron was firing! In other words, if the previous neuron was very active, even a small nudge could cause big changes! But if the previous neuron was always low activation/low firing, then this weight doesn‚Äôt really matter!\nNote: This observation is often stated as:\n\n‚ÄúNeurons that fire together, wire together‚Äù,\n‚ÄúWeights between low activation neurons learn very slowly‚Äù\n\n\nWith this, we can chain it together with our other partials to write an expression for \\dcdw{L}:\n\n\\begin{align}\n\\dcdw{L} &= \\dzdw{L}\\dcdz{L}\\\\ \\\\\n         &= \\dzdw{L}\\dadz{L}\\dcda{L} \\\\ \\\\\n         &= \\underbrace{\\ax{L-1}}_{\\text{how changing }\\wl\\text{ affects } \\zl} \\quad \\underbrace{\\sigma(\\zl)(1-\\sigma(\\zl))}_{\\text{how changing }\\zl\\text{ affects }\\al} \\quad \\underbrace{2(\\al -y)}_{\\text{how changing }\\al\\text{ affects }\\czero}\n\\end{align}\n\nOr simply:\n\n\\begin{align}\n\\dcdw{L} &= \\dzdw{L}\\dadz{L}\\dcda{L} \\\\\n         &= \\left(\\ax{L-1}\\right) \\left(\\sigma(\\zl)(1-\\sigma(\\zl))\\right) \\left(2(\\al -y)\\right)\n\\end{align}\n\n\nüßêPause-and-ponder: From here, can you guess the appropriate equation for \\dcdb{L}?\nHint:\n\n\\frac{\\dc}{?} = \\frac{\\dz{L}}{?}\\dadz{L}\\dcda{L}\n\n\n\nWow! We‚Äôve come a long way! We finally have an actual expression for how a change to one of our weights (\\wl) causes a change to our cost for a single example.\n\nLets finish up by using the hint I gave above\n\n\\frac{\\dc}{?} = \\frac{\\dz{L}}{?}\\dadz{L}\\dcda{L}\n\nto find an expression for how the cost changes with respect to the previous layers activity \\dcda{L-1}:\n\n\\dcda{L-1} = \\underbrace{\\frac{\\dz{L}}{\\da{L-1}}}_{\\text{need to calculate}}\\underbrace{\\dadz{L}\\dcda{L}}_{\\text{already have this}}\n\nSo all we need is an expression for \\frac{\\dz{L}}{\\da{L-1}}:\n\n\\begin{align}\n\\zl &= \\wl\\ax{L-1}+\\bl \\\\\n\\frac{\\dz{L}}{\\da{L-1}} &= \\wl\n\\end{align}\n\nLetting us write:\n\n\\dcda{L-1} = \\left(\\wl\\right) \\left(\\sigma(\\zl)(1-\\sigma(\\zl))\\right) \\left(2(\\al -y)\\right)\n\nFor completion, and as an answer to a previous pause-and-ponder, lets write out the expression for \\dcdb{L}:\n\n\\dcdb{L} = \\dzdb{L}\\dadz{L}\\dcda{L}\n\nWe can get an expression for \\dzdb{L}:\n\n\\begin{align}\n\\zl      &= \\wl\\ax{L-1}+\\bl \\\\\n\\dzdb{L} &= 1\n\\end{align}\n\nAnd so \n\\begin{align}\n\\dcdb{L} &= 1\\cdot\\dadz{L}\\dcda{L}\\\\\n         &= \\left(\\sigma(\\zl)(1-\\sigma(\\zl))\\right) \\left(2(\\al -y)\\right)\n\\end{align}\n\nLets group this level all together to write all partials for this level:\n\n\\begin{align}\n\\dcdw{L}   &= \\left(\\ax{L-1}\\right) &&\\cdot&\\left(\\sigma(\\zl)(1-\\sigma(\\zl))\\right) \\left(2(\\al -y)\\right) \\\\\n\\dcdb{L}   &= 1 &&\\cdot&\\left(\\sigma(\\zl)(1-\\sigma(\\zl))\\right) \\left(2(\\al -y)\\right) \\\\\n\\dcda{L-1} &=\\left(\\wl\\right) &&\\cdot&\\left(\\sigma(\\zl)(1-\\sigma(\\zl))\\right) \\left(2(\\al -y)\\right)\n\\end{align}\n\n\n\n\nSpeaking of Error‚Ä¶\nAha! We can notice they all have something in common:\n\n\\dcdz{L}=\\dadz{L}\\dcda{L}\n\n\nDefinition: The error associated with layer L is given by:\n\n\\deltal = \\dcdz{L}=\\dadz{L}\\dcda{L}\n\noften stated simply as:\n\n\\deltal = \\dcdz{L}\n\nand completely as:\n\n\\begin{align}\n\\deltal &= \\sigma(\\zl)(1-\\sigma(\\zl)) (\\al -y) \\\\\n        &= \\sigma^{\\color{red}{\\prime}}(\\zl) (\\al-y)\n\\end{align}\n\nNote: We‚Äôve discarded that 2 from the cost as is often done.\n\n\nüßêPause-and-ponder: Why is this called the error?\n\nThis lets rewrite our partials as:\n\n\\begin{align}\n\\dcdw{L}   &= \\ax{L-1} \\deltal \\\\\n\\dcdb{L}   &= \\deltal \\\\\n\\dcda{L-1} &= \\wl \\deltal\n\\end{align}\n\n\n\n\nWhat about the rest?\nAnd thats it! We‚Äôve finally finished out how our cost function changes with respect to one layer: L. But we‚Äôve only done one layer! Not to worry! We‚Äôve discovered backpropagation and the chain rule! So the rest is easy!\nLets look at our map\n\nWe‚Äôve boxed in what we know how to do so far. This also tells us its very straight forward to now give the equations for layer L-1:\n\n\\begin{align}\n\\dcdw{L-1} &= \\dzdw{L-1} \\dadz{L-1} \\dcda{L-1} \\\\\n\\dcdb{L-1} &= \\dzdb{L-1} \\dadz{L-1} \\dcda{L-1} \\\\\n\\dcda{L-2} &= \\frac{\\dz{L-1}}{\\da{L-2}} \\dadz{L-1} \\dcda{L-1}\n\\end{align}\n\n\n\n\nFully recursive definition\nWow! We again see they have something in common!\n\nDefinition: The error for layer L-1 is:\n\n\\deltax{L-1} = \\dcdz{L-1}=\\dadz{L-1}\\dcda{L-1}\n\noften stated simply as:\n\n\\deltax{L-1} = \\dcdz{L-1}\n\n\nWe can now restate the above as:\n\n\\begin{align}\n\\dcdw{L-1} &= \\ax{L-2} \\deltax{L-1} \\\\\n\\dcdb{L-1} &=  \\deltax{L-1} \\\\\n\\dcda{L-2} &= \\wx{L-1} \\deltax{L-1}\n\\end{align}\n\nThat is a recursive definition that serves for the rest of the graph!\n\n\n\nBut wait‚Ä¶\nTheres more! Lets bring back the definitions of error for layer L-1, and the partials for layer L:\nPartials at layer L:\n\n\\begin{align}\n\\dcdw{L}   &= \\ax{L-1} \\deltal \\\\\n\\dcdb{L}   &= \\deltal \\\\\n\\dcda{L-1} &= \\wl \\deltal\n\\end{align}\n\nError at layer L-1:\n\n\\deltax{L-1} = \\dcdz{L-1}=\\dadz{L-1}\\dcda{L-1}\n\nAha! We see something in common again!\n\n\\boxed{\\dcda{L-1} = \\wl \\deltal}\n\n\n\\deltax{L-1} = \\dcdz{L-1}=\\dadz{L-1}\\boxed{\\dcda{L-1}}\n\nLets sub that in and write:\n\n\\deltax{L-1} = \\dcdz{L-1}=\\dadz{L-1}\\wl \\deltal\n\nWe‚Äôve discovered another major equation!:\n\nüìñ Definition: The error at any layer L-1 can be written as a function of the next layer L:\n\n\\deltax{L-1} = \\wl \\deltal \\cdot \\sigma^{\\color{red}{\\prime}}(\\zx{L-1})\n\n\nFor example, for layer L-2:\n\n\\begin{align}\n\\deltax{L-2} &= \\wx{L-1}\\deltax{L-1}\\sigma^{\\color{red}{\\prime}}(\\zx{L-2}) \\\\\n             &= \\wx{L-1}\\left[  \\wl \\deltal \\cdot \\sigma^{\\color{red}{\\prime}}(\\zx{L-1}) \\right]\\sigma^{\\color{red}{\\prime}}(\\zx{L-2})\n\\end{align}\n\n\n\n\nThe Four Fundamental Backpropagation Equations‚Ñ¢\nWe did it! We‚Äôve completed backprop on this simple 1D network!\n\nLets cap this off by rewriting scalar versions of the four fundamental backpropagation equations:\n\nüìñDefinition: Scalar versions of the Four Fundamental Backpropagation Equations‚Ñ¢ are given by:\n\n\\begin{align}\n\\delta^L &= (\\al-y) \\cdot \\sigma^{\\color{red}{\\prime}}(\\zl)  \\tag{BP1} \\\\\n\\deltax{\\ell} &= \\wx{l+1} \\delta^{\\ell+1} \\cdot \\sigma^{\\color{red}{\\prime}}(\\zx{\\ell}) \\tag{BP2} \\\\\n\\dcdb{\\ell} &= \\delta^{\\ell} \\tag{BP3} \\\\\n\\dcdw{\\ell} &= \\ax{\\ell-1}\\delta^{\\ell} \\tag{BP4}\n\\end{align}\n\nwhere \\ell is any layer.\n\nSome explanation of each:\n\nBP1 defines the error for the last layer. This is the first thing we calculate to perform backprop.\nBP2 defines the error for any layer \\ell in terms of the error at the next level \\ell+1.\nBP3 defines the bias update at any layer \\ell\nBP4 defines the weight update at any layer \\ell\n\nBP2 and BP4 have interesting interpretations which will become more salient when we get to matrices/vectors, but that we can first describe here:\n\nLets start with BP2:\n\n\\deltax{\\ell} = \\wx{l+1} \\delta^{\\ell+1} \\cdot \\sigma^{\\color{red}{\\prime}}(\\zx{\\ell})\n\nWe can think of this as the backprop equation, as it clearly captures the backward flow of errors from outputs back through the network!\nYou can also think of BP2 as saying the following: Just as \\wx{\\ell+1} acted on the activation of the previous layer \\ax{l} to bring it forward to the current layer, it acts on the error of the current layer to bring it back to the previous layer!\nNote: This will become a bit more intutive when we get to matrices, as we will see this equation will have a weight matrix in the forward direction, and a transpose in the reverse direction\n\nLets rewrite BP4 as:\n\n\\dcdw{} = \\color{green}{a_{in}}\\delta_{out}\n\nwhere it‚Äôs understood that \\color{green}{a_{in}} is the activation of the neuron input to the weight w, and \\delta_{out} is the error of the neuron output from the weight w.\nClearly, when \\color{green}{a_{in}} is small, then \\dcdw{} is small. This is another way to say that this weight learns slowly, meaning that it‚Äôs not changing much during gradient descent. In other words, one consequence of BP4 is that weights output from low-activation neurons learn slowly.\n\nüßêPause-and-ponder: What else can we say? Think about what these equations are telling us!\n\n\n\nImplementation\nWe‚Äôre now ready to go ahead and implement backprop ourselves!\n\nüí™üèΩExercise: Implement this procedure on our toy example!\n\n\ndef backprop_1D():\n    # EDIT HERE\n    return\n\n\n# Step 0: fix all FP values: a1, a2, a3 and all parameters\n# Step 1: calculate delta_L\n\n\nüßêPause-and-ponder: Once we‚Äôve implemented these equations and calculated our partials all the way back through our network, what do we do?!"
  },
  {
    "objectID": "posts/nn/nn.html#multidimensional-case",
    "href": "posts/nn/nn.html#multidimensional-case",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "Multidimensional case",
    "text": "Multidimensional case\nNow things are going to get interesting! In our simple toy example, we only had one neuron per layer. This means in our resulting equations everything was just scalar values.\n\nLatex definitions\nThis cell contains more latex definitions. $\n$Note: remember to run it!\n\n\nNotation\nNow, we have to be a bit more careful! Lets remind ourselves of the notation we used above:\n\nHere, we‚Äôve labeled each neuron by its activation, layer number, and number in layer. For example, \\color{green}{a_j^L} is neuron {j}, in layer L, and \\color{blue}{w_{{j},{k}}^L} is the weight that connects neuron {k} in layer L-1 to neuron {j} in layer L.\nNow, the cost of a single example \\czero is a sum over the activations of all neurons in the last layer:\n\n\\czero = \\frac{1}{2}\\sum_{{j}} (\\color{green}{a_j^L}- y_{{j}})^2\n\nwhich in vector notation is:\n\n\\czero = \\frac{1}{2}\\|\\color{green}{\\mathbf{a}}^L - \\mathbf{y}\\|^2\n\nBefore going into vector/matrix notation, lets still deal with an individual weight in this network: \\color{blue}{w_{{j},{k}}^L}, and write out our backprop equation for this single weight:\n\n\\frac{\\dc}{\\color{blue}{\\partial w_{{j},{k}}^L}}= \\frac{\\color{orange}{\\partial z_j^L}}{\\color{blue}{\\partial w_{{j},{k}}^L}}\\frac{\\color{green}{\\partial \\color{green}{a_j^L}}}{\\color{orange}{\\partial z_j^L}}\\frac{\\dc}{\\color{green}{\\partial \\color{green}{a_j^L}}}\n\nFor completion, lets write expressions for each of its pieces.\nFor \\frac{\\dc}{\\color{green}{\\partial \\color{green}{a_j^L}}}:\n\n\\begin{align}\n\\czero &= \\frac{1}{2}\\sum_{{j}} (\\color{green}{a_j^L}- y_{{j}})^2 \\\\\n\\frac{\\dc}{\\color{green}{\\partial \\color{green}{a_j^L}}}&= (\\color{green}{a_j^L}- y_j)\n\\end{align}\n\nFor \\frac{\\color{green}{\\partial \\color{green}{a_j^L}}}{\\color{orange}{\\partial z_j^L}}: \n\\begin{align}\n\\frac{\\color{green}{\\partial \\color{green}{a_j^L}}}{\\color{orange}{\\partial z_j^L}}= \\sigma(\\color{orange}{z_j^L})(1-\\color{orange}{z_j^L})\n\\end{align}\n\nFor \\frac{\\color{orange}{\\partial z_j^L}}{\\color{blue}{\\partial w_{{j},{k}}^L}}: \n\\begin{align}\n\\color{orange}{z_j^L}&= \\ldots + \\color{blue}{w_{{j},{k}}^L}\\color{green}{a_k^{L-1}} + \\ldots \\\\\n\\frac{\\color{orange}{\\partial z_j^L}}{\\color{blue}{\\partial w_{{j},{k}}^L}}& =  \\color{green}{a_k^{L-1}}\n\\end{align}\n\nNote: Pay particular attention to the k index in the equation above! Remember, our weights are applied to the activations of the previous layer, which we are using k to index!\nJust as we did above, we can define a neuron specific error measure for neuron j in layer L as:\n\n\\begin{align}\n\\delta_{j}^L &= \\frac{\\dc}{\\color{orange}{\\partial}\\color{orange}{z_j^L}} \\\\\n             &= \\frac{\\color{green}{\\partial \\color{green}{a_j^L}}}{\\color{orange}{\\partial z_j^L}}\\frac{\\dc}{\\color{green}{\\partial \\color{green}{a_j^L}}}\\\\\n             &= \\sigma(\\color{orange}{z_j^L})\\sigma(1-\\color{orange}{z_j^L}) (\\color{green}{a_j^L}- y_j)\n\\end{align}\n\nNote: pay particular attention to the j index above! This is with respect to the current layer, which we are using j to index!\nThis lets us rewrite the above as:\n\n\\frac{\\dc}{\\color{blue}{\\partial w_{{j},{k}}^L}}= \\color{green}{a_k^{L-1}} \\delta_{j}^L\n\nAbove we‚Äôve given the equation for how the cost changes with a specific weight in our multi dimensional network. Notice how close it is to the 1D case!\n\nüí™üèΩExercise: Give the equation for \\frac{\\dc}{\\color{purple}{\\partial b^L_j}}\n\nNow, lets look at \\frac{\\dc}{\\color{green}{\\partial \\color{green}{a_k^{L-1}}}}. This is asking how the cost varies when we change the activation of a neuron k in layer L-1.\nJust using pattern matching, we might want to write:\n\n\\frac{\\dc}{\\color{green}{\\partial \\color{green}{a_k^{L-1}}}} \\stackrel{\\color{red}{?}}{=} \\color{blue}{w_{{j},{k}}^L}\\delta_j^L\n\nBut this is incorrect! To see why, lets draw a picture showing how nueron k in layer L-1 affects the current layer (and therefore the cost):\n\nAha! The activation of neuron k in layer L-1: \\color{green}{a_k^{L-1}} does not just flow through neuron j in layer L, it flows through every single neuron in layer L!\nSo if we want to account for the effect a small nudge to this neuron‚Äôs activation value has on our cost, we need to account for each neuron in layer L, and each associated weight!\nOur correct equation is then:\n\n\\frac{\\dc}{\\color{green}{\\partial \\color{green}{a_k^{L-1}}}} = \\sum_j \\left[ \\color{blue}{w_{{j},{k}}^L}\\delta_j^L \\right]\n\n\n\nImplementation\nOk! We‚Äôve gone to the multidimensional case, but still given equations for each individual parameter. So really, its almost exactly as it was in the 1D case!\n\nüí™üèΩExercise: Implement the backprop equations above!\n\nAs a hint, some pseudo-code for the implementation would be:\n# skipping setup\n# skipping forward pass\n\nfor sample in inputs:\n    # last layer\n    for neuron_j in layer[-1].neurons:\n        delta_j = # calculate according to formula above\n        dc_dwj = # calculate according to formula above\n        dc_dbj = # calculate according to formula above\n        dc_dak^{L-1} = # calculate according to formula above\n    \n    # other layers\n    for layer in layers-1:\n        for neuron_j in layer[l].neurons:\n            delta_j = # calculate according to formula above\n            dc_dwj = # calculate according to formula above\n            dc_dbj = # calculate according to formula above\n            dc_dak^{L-1} = # calculate according to formula above"
  },
  {
    "objectID": "posts/nn/nn.html#n-d-revisited---vector-notation",
    "href": "posts/nn/nn.html#n-d-revisited---vector-notation",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "N-D Revisited - Vector Notation",
    "text": "N-D Revisited - Vector Notation\nAbove, we gave specific equations for each parameter in a multi dimensional network. This will work, as your implementation should prove!\nHowever, it leaves much to be desired in terms of efficiency, and conciseness, and it doesn‚Äôt allow us to make full use of the magic of our numerical linear algebra libraries!\nLets begin our analysis with our error vector \\delta_{j}^L:\n\n\\begin{align}\n\\delta_{j}^L &= \\frac{\\dc}{\\color{orange}{\\partial}\\color{orange}{z_j^L}} \\\\\n             &= \\frac{\\color{green}{\\partial \\color{green}{a_j^L}}}{\\color{orange}{\\partial z_j^L}}\\frac{\\dc}{\\color{green}{\\partial \\color{green}{a_j^L}}}\\\\\n             &= \\sigma(\\color{orange}{z_j^L})\\sigma(1-\\color{orange}{z_j^L}) (\\color{green}{a_j^L}- y_j) \\\\\n             &= \\sigma^{\\color{red}{\\prime}}(\\color{orange}{z_j^L}) (\\color{green}{a_j^L}- y_j)\n\\end{align}\n where we brought back the \\sigma^{\\color{red}{\\prime}}(\\cdot) notation.\nWe would like to write a vector for our error \\delta^L, where each component is:\n\n\\delta^L = \\begin{bmatrix}\n\\sigma^{\\color{red}{\\prime}}(\\color{orange}{z_1^L}) (\\color{green}{a_1^L} - y_1) \\\\\n\\cdots \\\\\n\\sigma^{\\color{red}{\\prime}}(\\color{orange}{z_j^L}) (\\color{green}{a_j^L} - y_j)\\\\\n\\cdots\n\\end{bmatrix}\n\nThis is exactly the definition of element-wise product of the following vectors: \\color{orange}{\\mathbf{z}^L},\\color{green}{\\mathbf{a}^L} and \\mathbf{y}:\n\n\\delta^L = \\sigma^{\\color{red}{\\prime}}(\\color{orange}{\\mathbf{z}^L}) \\odot (\\color{green}{\\mathbf{a}^L} - \\mathbf{y})\n\nwhere we introduce another piece of notation:\n\nüìñDefinition: The Hadamard product between two vectors is the element-wise product:\n\n\\mathbf{a} \\odot \\mathbf{b} =\n\\begin{bmatrix}\na_1 \\cdot b_1 \\\\\n\\cdots \\\\\na_n \\cdot b_n\n\\end{bmatrix}\n\n\nLets define a function to implement this element-wise product:\n\ndef hadamard(a,b):\n    result = np.zeros_like(a)\n    for i in range(a.shape[0]):\n        result[i] = a[i] * b[i]\n    \n    return result\n\nLets test it!\n\na,b = np.random.randint(1,10,(3,1)),np.random.randint(1,10,(3,1))\na,b\n\n(array([[3],\n        [7],\n        [8]]),\n array([[5],\n        [6],\n        [7]]))\n\n\n\nhadamard(a,b)\n\narray([[15],\n       [42],\n       [56]])\n\n\nIt works!\nHowever, Python actually already does this element-wise product for us! Using the * operator!\n\na*b\n\narray([[15],\n       [42],\n       [56]])\n\n\nSo we didn‚Äôt have to implement our own. But thats ok, because we know how to. Lets get back to business!\n\nWe‚Äôve rewritten BP1 in vector notation. Lets now focus on BP2: moving the error ‚Äúbackward‚Äù.\nWell, we can probably already guess its going to involve the hadamard product with \\sigma^{\\color{red}{\\prime}}(\\cdot) as it did previously. We just don‚Äôt know with what yet!\n\n\\begin{align}\n\\delta^\\ell = ?? \\odot \\sigma^{\\color{red}{\\prime}}(\\mathbf{\\zx{\\ell}})\n\\end{align}\n\nWell, lets try to reason through it from ‚Äúfirst principles‚Äù (recall: this means make the sizes work!).\n\\delta^\\ell is a vector, which should be of size k, the number of elements in layer \\ell. We know its going to be formed with the weight matrix and \\delta^{\\ell+1}, so lets write their sizes:\n\n\\color{blue}{W^{\\ell+1}}: (j \\times k), \\quad\\quad \\delta^{\\ell+1}: (j \\times 1)\n\nHow do we multiply these out to get a vector of size k \\times 1 out?\nüßê\nIndeed! We need to transpose our weight matrix, and then take the usual matrix-vector product with \\delta^{\\ell+1}!\nNow we can write the vectorized equation for the error at any layer:\n\nüìñVectorized BP2: The error at any layer \\ell can be written as as function of the next layer as:\n\n\\delta^{\\ell} = (\\color{blue}{W^{\\ell+1}})^T \\delta^{\\ell+1} \\odot \\sigma^{\\color{red}{\\prime}}(\\mathbf{\\zx{\\ell}})\n\n\n\nüßêPause-and-ponder: What can we say about our previous interpretation in light of this new equation?\n\nLets quickly tackle BP3:\n\nüìñVectorized-BP3:\n\n\\frac{\\dc}{\\color{purple}{\\partial \\mathbf{b}^\\ell}} = \\delta^\\ell\n\n\nWell, that was easy! Its just as it was above, but as a vector.\nHm. Now we get to the tough one: BP4. This is now a partial derivative of our cost with respect to weight matrix!\nLets start at the last layer. \\color{blue}{W^L} is of size j \\times k, where j is the size of the last layer.\nSo, we should expect its gradient to be of the same size: a matrix. If its not clear why, just imagine how were going to use this partial. We‚Äôre going to add it the current value of \\color{blue}{W} in gradient descent, so it better be of the same size!\nTo form this matrix, we have \\color{green}{\\mathbf{a}^{L-1}}, which is of size k \\times 1, the size of the previous layer, and \\delta^L, which is of size j \\times 1.\nSo how do we form this matrix using these two vectors? We take the outer product:\n\n\\delta^L(\\color{green}{\\mathbf{a}^{L-1}})^T\n\nLets make sure the sizes work!\n\n(j \\times 1)(1 \\times k) = (j \\times k)\n\nPerfect!\n\nüìñVectorized BP4:\n\n\\frac{\\dc}{ \\color{blue}{ \\partial \\mathbf{W^\\ell} } } = \\delta^L(\\color{green}{\\mathbf{a}^{L-1}})^T\n\n\nFinally! Lets give fully vectorized forms of the batchprop equations:\n\n\nF.F.B.E - Vectorized‚Ñ¢\nLets cap this off by writing vectorized versions of the four fundamental backpropagation equations:\n\nüìñDefinition: Vectorized versions of the Four Fundamental Backpropagation Equations‚Ñ¢ are given by:\n\n\\begin{align}\n\\delta^L &= (\\color{green}{\\mathbf{a}^L} - \\mathbf{y})  \\odot  \\sigma^{\\color{red}{\\prime}}(\\color{orange}{\\mathbf{z}^L}) \\tag{BP1}\\\\\n\\delta^{\\ell} &= (\\color{blue}{W^{\\ell+1}})^T \\delta^{\\ell+1} \\odot \\sigma^{\\color{red}{\\prime}}(\\mathbf{\\zx{\\ell}})\\tag{BP2} \\\\\n\\frac{\\dc}{\\color{purple}{\\partial \\mathbf{b}^\\ell}} &= \\delta^\\ell \\tag{BP3} \\\\\n\\frac{\\dc}{ \\color{blue}{ \\partial \\mathbf{W^\\ell} } } &= \\delta^\\ell(\\color{green}{\\mathbf{a}^{\\ell-1}})^T \\tag{BP4}\n\\end{align}\n\n\n\n\n\nImplementation\nWow! Now we‚Äôve really come a long way!\n\nüí™üèΩExercise: Implement the backprop equations above!\n\nAs you‚Äôre implementing this, think about the following:\n\nüßêPause-and-ponder: How do you deal with multiple samples?\n\nInitialize our toy problem as always:\n\nX,y = toy()\n\nSince we‚Äôre still dealing with a flat implementation, lets go ahead and initialize our weights and biases here:\n\n# initialize weight matrices - notice the dimensions\nW_1_init = np.random.randn(3,1)\nW_2_init = np.random.randn(2,3)\nW_3_init = np.random.randn(1,2)\nweights = [W_1_init,W_2_init,W_3_init]\n\n# and our biases\nb_1_init = np.random.randn(3,1)\nb_2_init = np.random.randn(2,1)\nb_3_init = np.random.randn(1,1)\nbiases = [b_1_init,b_2_init,b_3_init]\n\nNow we can use the feedforward batch implementation we gave above:\n\ndef feedforward_batch(X, weights, biases):\n    # assume inputs is of shape Nxd\n    W_1,W_2,W_3 = weights[0],weights[1],weights[2]\n    b_1,b_2,b_3 = biases[0],biases[1],biases[2]\n    \n    activities = []\n    activations = []\n    \n    # NOTE - there is no for loop here! \n    \n    # treat activations like layer_0 activations:\n    activations.append(X.T)\n    \n    # layer 1\n    z_1 = W_1.dot(X.T) + b_1\n    a_1 = sigmoid(z_1)\n    activities.append(z_1)\n    activations.append(np.squeeze(np.asarray(a_1)))\n    \n    # layer 2\n    z_2 = W_2.dot(a_1)+b_2\n    a_2 = sigmoid(z_2)\n    activities.append(z_2)\n    activations.append(np.squeeze(np.asarray(a_2)))\n    \n    # output layer\n    z_3 = W_3.dot(a_2)+b_3\n    y_hat = sigmoid(z_3)\n    activities.append(z_3)\n    activations.append(np.asarray(y_hat))\n    \n    return activities, activations\n\nLets actually feed-forward our entire dataset:\n\nactivities, activations = feedforward_batch(X,weights,biases)\n\nLets check the sizes of these:\n\nprint(activities[0].shape,activities[1].shape,activities[2].shape)\n\n(3, 1000) (2, 1000) (1, 1000)\n\n\n\nprint(activations[0].shape,activations[1].shape,\n      activations[2].shape, activations[3].shape)\n\n(1, 1000) (3, 1000) (2, 1000) (1, 1000)\n\n\nNote: We‚Äôre calling our input our first activity, which is why there is one extra.\nNow we‚Äôre ready to run backprop on each sample in our dataset, to generate bias and weight updates for each sample.\nLets start with a sigmoid prime function we will need:\n\ndef sigmoid_prime(z):\n    return sigmoid(z)*(1-sigmoid(z))\n\n\nplt.plot(sigmoid(np.linspace(-10,10)));\nplt.plot(sigmoid_prime(np.linspace(-10,10)));\n\n\n\n\nLets continue our work:\n\nN,d = inputs.shape\n\nweight_updates = []\nbias_updates = []\n\n\nlayer_num = len(activations)\nweight_updates = []\nbias_updates = []\n\nNow our equations are above are only for one sample! So lets pick one and proceed:\n\nsample_idx = 0\n\nOk, so lets run backprop on that one sample, by starting with our last layer:\n\n########################################\n#           Last Level\n########################################\n# get last layer specific variables\nz_L = activities[-1][:,sample_idx].reshape(-1,1)\na_L = activations[-1][:,sample_idx].reshape(-1,1)\na_L_minus_1 = activations[-2][:,sample_idx].reshape(-1,1)\n\n\n(z_L.shape,a_L.shape,a_L_minus_1.shape)\n\n((1, 1), (1, 1), (2, 1))\n\n\nAre these what we expect?\nLets use them in the formulas we gave above!\n\n# calculate delta_L for the last level\ndelta_L = (a_L-y[sample_idx])*sigmoid_prime(z_L)\n\n\ndelta_L.shape\n\n(1, 1)\n\n\nLets make a list to store all these delta values!\n\ndeltas = []\ndeltas.append(delta_L)\n\nNow lets calculate the bias update:\n\n# calculate bias update\ndc_db_l = delta_L\nbias_updates = [dc_db_l] + bias_updates\ndc_db_l.shape\n\n(1, 1)\n\n\nAnd the weight update:\n\n# calcualte weight updates\ndc_dw_l = delta_L.dot((a_L_minus_1).T)\nweight_updates = [dc_dw_l] + weight_updates\ndc_dw_l.shape\n\n(1, 2)\n\n\nWow! We‚Äôve run backprop across our last layer! Now all we have to do, is apply this to the rest of our layers!\n\n########################################\n# loop through each layer, from 2 to L-1\n######################################## \nfor layer in range(2,layer_num):\n    # using level as a **negative index**\n    l = -layer\n\n    # uncomment this print statement \n    # to help you understand how negative indexing works\n    print(f\"Loop variable: {layer}, l={l}, corresponding to layer: l{l}\")\n\n    # calculate delta_l for each layer\n    z_l = activities[l][:,sample_idx].reshape(-1,1)\n    a_l = activations[l][:,sample_idx].reshape(-1,1)\n    delta_l = weights[l+1].T.dot(deltas[l+1]) * sigmoid_prime(z_l)\n    deltas = [delta_l] + deltas\n\n    # calculate bias update\n    dc_db_l = delta_l\n    bias_updates = [dc_db_l] + bias_updates\n\n    # calcualte weight updates\n    a_l_minus_1 = activations[l-1][:,sample_idx].reshape(-1,1)\n    dc_dw_l = delta_l.dot((a_l_minus_1).T)\n    weight_updates = [dc_dw_l] + weight_updates\n    \n    print(f'=========Layer:{layer_num+l}=========')\n    print(f'Using negative index: {l}')\n    print(f'z_l:{z_l.shape}, a_l:{a_l.shape}, a_l_minus_1:{a_l_minus_1.shape}')\n    print(f'delta_l:{delta_l.shape}, dc_db_l:{dc_db_l.shape},dc_dw_l:{dc_dw_l.shape}')\n    print()\n\nLoop variable: 2, l=-2, corresponding to layer: l-2\n=========Layer:2=========\nUsing negative index: -2\nz_l:(2, 1), a_l:(2, 1), a_l_minus_1:(3, 1)\ndelta_l:(2, 1), dc_db_l:(2, 1),dc_dw_l:(2, 3)\n\nLoop variable: 3, l=-3, corresponding to layer: l-3\n=========Layer:1=========\nUsing negative index: -3\nz_l:(3, 1), a_l:(3, 1), a_l_minus_1:(1, 1)\ndelta_l:(3, 1), dc_db_l:(3, 1),dc_dw_l:(3, 1)\n\n\n\nAll thats left to do, is wrap this up in a reusable function!\n\ndef backprop_1_example(activities,activations,y,sample_idx,quiet=True):\n    layer_num = len(activations)\n    weight_updates = []\n    bias_updates = []\n    \n    ########################################\n    #           Last Level\n    ########################################\n    # get last layer specific variables\n    a_L = activations[-1][:,sample_idx].reshape(-1,1)\n    a_L_minus_1 = activations[-2][:,sample_idx].reshape(-1,1)\n    z_L = activities[-1][:,sample_idx].reshape(-1,1)\n    \n    # calculate delta_L for the last level\n    delta_L = (a_L-y)*sigmoid_prime(z_L)\n    deltas = []\n    deltas.append(delta_L)\n\n    # calculate bias update\n    dc_db_L = delta_L\n    bias_updates = [dc_db_L] + bias_updates\n\n    # calcualte weight updates\n    dc_dw_L = delta_L.dot((a_L_minus_1).T)\n    weight_updates = [dc_dw_L] + weight_updates\n    \n    if not quiet:\n        print(f'=========Layer:{layer_num+-1}=========')\n        print(f'Using negative index: -1')\n        print(f'z_l:{z_L.shape}, a_l:{a_L.shape}, a_l_minus_1:{a_L_minus_1.shape}')\n        print(f'delta_l:{delta_L.shape}, dc_db_l:{dc_db_L.shape},dc_dw_l:{dc_dw_L.shape}')\n        print()\n    \n    ########################################\n    # loop through each layer, from 2 to L-1\n    ######################################## \n    for layer in range(2,layer_num):\n        # using level as a **negative index**\n        l = -layer\n        \n        # uncomment this print statement \n        # to help you understand how negative indexing works\n        # print(f\"Loop variable: {layer}, l={l}, corresponding to layer: l{l}\")\n\n        # calculate delta_l for each layer\n        a_l = activations[l][:,sample_idx].reshape(-1,1)\n        z_l = activities[l][:,sample_idx].reshape(-1,1)\n        delta_l = weights[l+1].T.dot(deltas[l+1]) * sigmoid_prime(z_l)\n        deltas = [delta_l] + deltas\n\n        # calculate bias update\n        dc_db_l = delta_l\n        bias_updates = [dc_db_l] + bias_updates\n\n        # calcualte weight updates\n        a_l_minus_1 = activations[l-1][:,sample_idx].reshape(-1,1)\n        dc_dw_l = delta_l.dot((a_l_minus_1).T)\n        weight_updates = [dc_dw_l] + weight_updates\n        \n        if not quiet:\n            print(f'=========Layer:{layer_num+l}=========')\n            print(f'Using negative index: {l}')\n            print(f'z_l:{z_l.shape}, a_l:{a_l.shape}, a_l_minus_1:{a_l_minus_1.shape}')\n            print(f'delta_l:{delta_l.shape}, dc_db_l:{dc_db_l.shape},dc_dw_l:{dc_dw_l.shape}')\n            print()\n    \n    return weight_updates, bias_updates\n\nWe now have the following: * feedforward_batch: a function which feeds entire batches through our network * backprop_1_example: a function which backpropagates the erorr associated with a single example through the network\nLets now revisit:\n\nüßêPause-and-ponder: How do you deal with multiple samples?\n\nAnd try to think of how we can implement this algorithm:"
  },
  {
    "objectID": "posts/nn/nn.html#more-interesting-example",
    "href": "posts/nn/nn.html#more-interesting-example",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "More interesting example",
    "text": "More interesting example\n\n# Generate the dataset\nX, y = sklearn.datasets.make_circles(\n    n_samples=1000, shuffle=False, factor=0.3, noise=0.1)\n\n# Separate the red and blue samples for plotting\nx_red = X[y==0]\nx_blue = X[y==1]\n\n# correct size of y\ny = y.reshape(-1,1)\n\nprint('shape of X: {}'.format(X.shape))\nprint('shape of y: {}'.format(y.shape))\n\nshape of X: (1000, 2)\nshape of y: (1000, 1)\n\n\n\n# Plot both classes on the x1, x2 plane\nplt.figure(figsize=(10, 7))\nplt.plot(x_red[:,0], x_red[:,1], 'r*', \n         label='class: red star', alpha=0.75)\nplt.plot(x_blue[:,0], x_blue[:,1], 'bo', \n         label='class: blue circle', alpha=0.75)\nplt.legend(loc=1)\nplt.xlabel('$x_1$', fontsize=20)\nplt.ylabel('$x_2$', fontsize=20)\nplt.axis([-1.5, 1.5, -1.5, 1.5])\nplt.title('red star vs blue circle classes in the input space', fontsize=20);\n\n\n\n\n\nTrain our Network!\nNow we can actually train our network using gradient descent as we have before!\n\n# dataset\nN = X.shape[0]\nd = X.shape[1]\nbatch_size = 50\nepochs = 10\neta=1e-2\ndataset=X\nquiet=False\n\n# initialize weight matrices - notice the dimensions\nW_1_init = np.random.rand(3,2)\nW_2_init = np.random.rand(3,3)\nW_3_init = np.random.randn(1,3)\nweights = [W_1_init,W_2_init,W_3_init]\n\n# and our biases\nb_1_init = np.random.rand(3,1)\nb_2_init = np.random.rand(3,1)\nb_3_init = np.random.rand(1,1)\nbiases = [b_1_init,b_2_init,b_3_init]\n\n# network\nW_1,W_2,W_3 = weights[0],weights[1],weights[2]\nb_1,b_2,b_3 = biases[0],biases[1],biases[2]\n\n# mini-batch params \nnum_batches = int(N/batch_size)\niterations = 0\n\n# debugging lists\nloss = []\ngrad_norm_1 = []\ngrad_norm_2 = []\ngrad_norm_3 = []\nnorm_1 = []\nnorm_2 = []\nnorm_3 = []\n\n\nfor epoch in range(epochs):\n\n    # work on current batch \n    for batch_num in range(num_batches):\n\n        # get current batch\n        batch_start = batch_num*batch_size\n        batch_end = (batch_num+1)*batch_size\n        batch = dataset[batch_start:batch_end,:]\n        batch_labels = labels[batch_start:batch_end,:].reshape(-1,1)\n\n        # feedforward on batch\n        activities, activations = feedforward_batch(batch, weights,biases)\n        \n\n        # setup matrices to hold gradients\n        grad_W_1 = np.zeros_like(W_1)\n        grad_b_1 = np.zeros_like(b_1)\n        grad_W_2 = np.zeros_like(W_2)\n        grad_b_2 = np.zeros_like(b_2)\n        grad_W_3 = np.zeros_like(W_3)\n        grad_b_3 = np.zeros_like(b_3)\n        \n\n        # loop through each example in the batch\n        for idx in range(batch.shape[0]):\n            current_sample = batch[idx,:]\n            current_label = batch_labels[idx]\n\n            # get current weight and bias updates\n            weight_updates, bias_updates = backprop_1_example(activities,activations,current_label,idx)\n\n            # aggregate them \n            grad_W_1 = grad_W_1 + weight_updates[0]\n            grad_b_1 = grad_b_1 + bias_updates[0]\n\n            grad_W_2 = grad_W_2 + weight_updates[1]\n            grad_b_2 = grad_b_2 + bias_updates[1]\n\n            grad_W_3 = grad_W_3 + weight_updates[2]\n            grad_b_3 = grad_b_3 + bias_updates[2]\n            \n            \n        grad_norm_1.append(np.linalg.norm(grad_W_1))\n        grad_norm_2.append(np.linalg.norm(grad_W_2))\n        grad_norm_3.append(np.linalg.norm(grad_W_3))\n\n\n        # take your steps:\n        W_1 = W_1 - eta/batch_size*(grad_W_1)\n        b_1 = b_1 - eta/batch_size*(grad_b_1)\n\n        W_2 = W_2 - eta/batch_size*(grad_W_2)\n        b_2 = b_2 - eta/batch_size*(grad_b_2)\n\n        W_3 = W_3 - eta/batch_size*(grad_W_3)\n        b_3 = b_3 - eta/batch_size*(grad_b_3)\n        \n        norm_1.append(np.linalg.norm(W_1))\n        norm_2.append(np.linalg.norm(W_2))\n        norm_3.append(np.linalg.norm(W_3))\n        \n        # save new weights and biases to be used in the next feedforward step\n        weights = [W_1,W_2,W_3]\n        biases = [b_1,b_2,b_3]\n        \n        # calculate current loss \n        loss.append(mse(activations[-1],batch_labels))\n    \n    print(f\"Epoch:{epoch+1}/{epochs}\")\n\nEpoch:1/10\nEpoch:2/10\nEpoch:3/10\nEpoch:4/10\nEpoch:5/10\n\n\nEpoch:6/10\nEpoch:7/10\nEpoch:8/10\nEpoch:9/10\nEpoch:10/10\n\n\nüßê Hm. Wait a minute - how do we know we‚Äôre doing the right thing? We should check it!"
  },
  {
    "objectID": "posts/nn/nn.html#gradient-checking",
    "href": "posts/nn/nn.html#gradient-checking",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "Gradient Checking",
    "text": "Gradient Checking\nBackprop implementations are notoriously difficult to get right! That means we should implement some kind of numerical check for correctness.\n\nüßêPause-and-ponder: Can you think of a way we could numerically check that our analytical gradients are correct?\n\n\nNumerical Gradients!\nHaving thought about it a bit, you might have thought to check our numerical approximations for the derivative!\nRecall, we could write the cost function as a function of one long vector of all of our parameters:\n\n\\czero(\\color{blue}{\\mathbf{W}_1,\\mathbf{W}_2,\\mathbf{W}_3},\\color{purple}{\\mathbf{b}_1,\\mathbf{b}_2,\\mathbf{b}_3}) = \\czero(\\color{blue}{w_{0,0},w_{0,1},\\ldots,w_{1,0},w_{1,1}},\\ldots,\\color{purple}{b_{0,0},b_{0,1}},\\ldots)\n\nTo abstract this away, we can write it out as:\n\n\\czero (\\theta_1,\\theta_2,\\ldots,\\theta_i,\\ldots)\n\nWell, then one way we could check for correctness, is to calculate the two sided difference \\Delta_i:\n\n\\begin{align}\n\\Delta_i &= \\frac{\\czero (\\theta_1,\\theta_2,\\ldots,\\color{red}{\\theta_i+\\epsilon},\\ldots) - \\czero (\\theta_1,\\theta_2,\\ldots,\\color{red}{\\theta_i-\\epsilon},\\ldots)}{2\\epsilon}\n\\end{align}\n\nWe know this is an approximation for our desired partial when \\epsilon is very small:\n\n\\Delta_i \\approx \\frac{\\dc}{\\partial \\theta_i}\n\nSo we can use this approximation to see how close it is to our analytical gradient!\nSpecifically, we want to calculate:\n\n\\frac{\\|\\Delta_i - \\frac{\\dc}{\\partial \\theta_i}  \\|_2}{\\|\\Delta_i\\|_2 + \\|\\frac{\\dc}{\\partial \\theta_i}\\|_2  }\n\nAnd verify that this is very small for all of our parameters! Specifically, we can choose \\epsilon = 10^{-7}, and expect this difference to be <10^{-7}. If so, we can call our backprop code:\n\nnumerically verified‚Ñ¢ üòé\n\nA note before we proceed, its very common to use the ravel function to do this flattening out, and then use concatenate to join our newly flattened arrays‚Äù\n\na,b = np.random.randint(1,10,(3,2)),np.random.randint(1,10,(4,3))\na,b\n\n(array([[8, 9],\n        [6, 8],\n        [3, 4]]),\n array([[1, 1, 6],\n        [2, 6, 7],\n        [6, 7, 7],\n        [6, 2, 7]]))\n\n\n\na.ravel()\n\narray([8, 9, 6, 8, 3, 4])\n\n\n\nnp.concatenate((a.ravel(),b.ravel()))\n\narray([8, 9, 6, 8, 3, 4, 1, 1, 6, 2, 6, 7, 6, 7, 7, 6, 2, 7])\n\n\nSo now, we can use these to flatten all of our parameters, and join them into one big vector which we can then perturb.\nLets define a couple helper functions which will help us unravel and then reform them.\nThe first is get_params, which will return a single vector of all our parameters, using python‚Äôs list comprehension:\n\ndef get_params(weights,biases):\n    weight_params = [weight.ravel() for weight in weights]\n    bias_params = [bias.ravel() for bias in biases]\n    all_params = weight_params+bias_params\n    return np.concatenate(all_params)\n\n\nget_params(weights,biases)\n\narray([ 0.52803977,  0.19727827,  0.73571915,  0.01607195,  0.45943066,\n        0.41666659,  0.76040374,  0.1461289 ,  0.80321117,  0.58307994,\n        0.15582433,  0.91136596,  0.31535448,  0.46250472,  0.06992028,\n       -0.95136624,  1.08528127, -0.6451534 ,  0.79075908,  0.45143977,\n        0.24539134,  0.38100885,  0.81534446,  0.7344555 , -0.15842374])\n\n\nVerify that this is of the correct length, based on our network structure!\n\nget_params(weights,biases).shape\n\n(25,)\n\n\nWe can also define a set_params function, which is responsible for re-forming our parameters into the correct shapes, given one long vector:\n\ndef set_params(param_vector,orig_weights,orig_biases):\n    # set weights\n    new_weights=[]\n    pointer = 0\n    for weight_mat in orig_weights:\n        curr_len = (weight_mat.shape[0]*weight_mat.shape[1])\n        new_mat = param_vector[pointer:pointer+curr_len].reshape(weight_mat.shape)\n        new_weights.append(new_mat)\n        pointer += curr_len\n    \n    # set biases\n    new_biases=[]\n    for weight_vec in orig_biases:\n        curr_len = (weight_vec.shape[0]*weight_vec.shape[1])\n        new_vec = param_vector[pointer:pointer+curr_len].reshape(weight_vec.shape)\n        new_biases.append(new_vec)\n        pointer += curr_len\n        \n    return new_weights,new_biases\n\nNote: This function takes an original set of weights and biases, but only uses it for the sizes of each element!\nLets run both of our new functions and verify they work as expected:\n\nnew_weights,new_biases = set_params(get_params(weights,biases),weights,biases)\n\nNow we can once again use list comprehension to quickly check if new_weights is equal to weights, and new_biases is equal to biases at each element:\n\n[np.array_equal(i,j) for i,j in zip(weights,new_weights)]\n\n[True, True, True]\n\n\n\n[np.array_equal(i,j) for i,j in zip(biases,new_biases)]\n\n[True, True, True]\n\n\nWe should see all True there! Perfect, our unraveling and raveling works as we expect. Now we can use this to perform gradient checking.\n\n\nImplementation\nWe can now begin to implement our numerical gradient checking, by defining a function that lets us do this:\n\ndef gradient_checking(weights,biases,batch,idx,current_label,weight_updates,bias_updates,epsilon=1e-7):\n\n    # unravel the current params\n    params = get_params(weights,biases)\n    diff_vec = np.zeros_like(params)\n\n    # loop through, perturbing one parameter at a time:\n    for p_index in range(params.shape[0]):\n        perturb = np.zeros_like(params)\n        perturb[p_index] = epsilon\n\n        # feedforward at each side\n        _, activations_p = feedforward_batch(batch,*set_params(params + perturb,weights,biases))\n        _, activations_m = feedforward_batch(batch,*set_params(params - perturb,weights,biases))\n\n        # calculate cost of each side\n        cost_plus = cost_mse(activations_p[-1][:,idx],current_label)\n        cost_minus = cost_mse(activations_m[-1][:,idx],current_label)\n\n        # calcualte \\Delta_i\n        diff_vec[p_index] = (cost_plus - cost_minus)/(2*epsilon)\n\n    # calculate the difference for this round of backprop\n    grad_vec = get_params(weight_updates, bias_updates)\n    grad_error = np.linalg.norm(diff_vec - grad_vec)/(np.linalg.norm(diff_vec) + np.linalg.norm(grad_vec))\n   \n    if grad_error > epsilon:\n        print(\"Error in gradient calculation!\")\n   \n    return grad_error\n\nThis function takes the following:\n\nweights: the current weights of our network\nbiases: the current biases of our network\nbatch: the current batch of data\nidx: the index of our current sample,\ncurrent_label: the current label, of the current sample\nweight_updates: the list of gradients of the weights we calculated\nbias_updates: the list of gradients of the biases we calculated\nepsilon=1e-7: a default epsilon value\n\nIts a bit verbose, but thats because we‚Äôre going for a flat implementation. Once we‚Äôve understood all the pieces, we can wrap everything up nice and neat.\nWith this, lets re-run our training but this time perform gradient checking!\n\ndef backprop_1_example1(activities,activations,y,quiet=True):\n    my_bp['act'].append(activations)\n    \n    layer_num = len(activations)\n    weight_updates = []\n    bias_updates = []\n    \n    ########################################\n    #           Last Level\n    ########################################\n    # get last layer specific variables\n    a_L = activations[-1].reshape(-1,1)\n    a_L_minus_1 = activations[-2].reshape(-1,1)\n    z_L = activities[-1].reshape(-1,1)\n    \n    # calculate delta_L for the last level\n    delta_L = (a_L-y)*sigmoid_prime(z_L)\n    my_bp['delta'].append(delta_L)\n    \n    deltas = []\n    deltas.append(delta_L)\n\n    # calculate bias update\n    dc_db_L = delta_L\n    bias_updates = [dc_db_L] + bias_updates\n\n    # calcualte weight updates\n    dc_dw_L = delta_L.dot((a_L_minus_1).T)\n    my_bp['w'].append(dc_dw_L)\n    weight_updates = [dc_dw_L] + weight_updates\n    \n    if not quiet:\n        print(f'=========Layer:{layer_num+-1}=========')\n        print(f'Using negative index: -1')\n        print(f'z_l:{z_L.shape}, a_l:{a_L.shape}, a_l_minus_1:{a_L_minus_1.shape}')\n        print(f'delta_l:{delta_L.shape}, dc_db_l:{dc_db_L.shape},dc_dw_l:{dc_dw_L.shape}')\n        print()\n    \n    ########################################\n    # loop through each layer, from 2 to L-1\n    ######################################## \n    for layer in range(2,layer_num):\n        # using level as a **negative index**\n        l = -layer\n        \n        # uncomment this print statement \n        # to help you understand how negative indexing works\n        # print(f\"Loop variable: {layer}, l={l}, corresponding to layer: l{l}\")\n\n        # calculate delta_l for each layer\n        a_l = activations[l].reshape(-1,1)\n        z_l = activities[l].reshape(-1,1)\n        delta_l = weights[l+1].T.dot(deltas[l+1]) * sigmoid_prime(z_l)\n        my_bp['delta'].append(delta_l)\n        deltas = [delta_l] + deltas\n\n        # calculate bias update\n        dc_db_l = delta_l\n        bias_updates = [dc_db_l] + bias_updates\n\n        # calcualte weight updates\n        a_l_minus_1 = activations[l-1].reshape(-1,1)\n        dc_dw_l = delta_l.dot((a_l_minus_1).T)\n        my_bp['w'].append(dc_dw_l)\n        weight_updates = [dc_dw_l] + weight_updates\n        \n        if not quiet:\n            print(f'=========Layer:{layer_num+l}=========')\n            print(f'Using negative index: {l}')\n            print(f'z_l:{z_l.shape}, a_l:{a_l.shape}, a_l_minus_1:{a_l_minus_1.shape}')\n            print(f'delta_l:{delta_l.shape}, dc_db_l:{dc_db_l.shape},dc_dw_l:{dc_dw_l.shape}')\n            print()\n    \n    return weight_updates, bias_updates\n\n\n# dataset\nN = X.shape[0]\nd = X.shape[1]\nbatch_size = 50\nepochs = 10\neta=1e-2\ndataset=X\nlabels=y\nquiet=False\n\n# gradient checking\nepsilon = 1e-7\n\n# initialize weight matrices - notice the dimensions\nW_1_init = np.random.rand(3,2)\nW_2_init = np.random.rand(3,3)\nW_3_init = np.random.randn(1,3)\nweights = [W_1_init,W_2_init,W_3_init]\n\n# and our biases\nb_1_init = np.random.rand(3,1)\nb_2_init = np.random.rand(3,1)\nb_3_init = np.random.rand(1,1)\nbiases = [b_1_init,b_2_init,b_3_init]\n\n# network\nW_1,W_2,W_3 = weights[0],weights[1],weights[2]\nb_1,b_2,b_3 = biases[0],biases[1],biases[2]\n\n# mini-batch params \nnum_batches = int(N/batch_size)\niterations = 0\n\n# various debugging lists\nloss = []\ngrad_norm_1 = []\ngrad_norm_2 = []\ngrad_norm_3 = []\nnorm_1 = []\nnorm_2 = []\nnorm_3 = []\ngrad_errors = []\ndiff_vecs = []\ngrad_vecs = []\nmy_act = []\nmy_w = []\nmy_b = []\nmy_bp = {'x':[],'y':[],'act':[],'delta':[],'w':[]}\n\nfor epoch in range(epochs):\n    # shuffle the dataset - note we do this by shuffling indices\n#     indices = np.random.permutation(N)\n#     shuffled_dataset = dataset[indices,:]\n#     shuffled_labels = labels[indices,:]\n    shuffled_dataset = dataset\n    shuffled_labels = labels\n        \n    # work on current batch \n    for batch_num in range(num_batches):\n\n        # get current batch\n        batch_start = batch_num*batch_size\n        batch_end = (batch_num+1)*batch_size\n        batch = shuffled_dataset[batch_start:batch_end,:]\n        batch_labels = shuffled_labels[batch_start:batch_end,:].reshape(-1,1)\n\n        # feedforward on batch\n        activities, activations = feedforward_batch(batch, weights,biases)\n        my_act.append(activations[-1])\n\n        # setup matrices to hold gradients\n        grad_W_1 = np.zeros_like(W_1)\n        grad_b_1 = np.zeros_like(b_1)\n        grad_W_2 = np.zeros_like(W_2)\n        grad_b_2 = np.zeros_like(b_2)\n        grad_W_3 = np.zeros_like(W_3)\n        grad_b_3 = np.zeros_like(b_3)\n        \n\n        # loop through each example in the batch\n        for idx in range(batch.shape[0]):\n            current_sample = batch[idx,:].reshape(1,-1)\n            current_label = batch_labels[idx]\n            my_bp['x'].append(current_sample)\n            my_bp['y'].append(current_label)\n            \n            curr_act, curr_activ = feedforward_batch(current_sample, weights,biases)\n            \n            # get current weight and bias updates\n            weight_updates, bias_updates = backprop_1_example1(curr_act,curr_activ,\n                                                              current_label)\n            my_w.append(weight_updates)\n            my_b.append(bias_updates)\n\n            # perform gradient cehcking\n#             grad_error = gradient_checking(weights,biases,batch,idx,current_label,\n#                                            weight_updates,bias_updates,epsilon)\n#             grad_errors.append(grad_error)\n            \n            # aggregate our updates\n            grad_W_1 = grad_W_1 + weight_updates[0]\n            grad_b_1 = grad_b_1 + bias_updates[0]\n\n            grad_W_2 = grad_W_2 + weight_updates[1]\n            grad_b_2 = grad_b_2 + bias_updates[1]\n\n            grad_W_3 = grad_W_3 + weight_updates[2]\n            grad_b_3 = grad_b_3 + bias_updates[2]\n            \n            \n        grad_norm_1.append(np.linalg.norm(grad_W_1))\n        grad_norm_2.append(np.linalg.norm(grad_W_2))\n        grad_norm_3.append(np.linalg.norm(grad_W_3))\n\n\n        # take your steps:\n        W_1 = W_1 - eta/batch_size*(grad_W_1)\n        b_1 = b_1 - eta/batch_size*(grad_b_1)\n\n        W_2 = W_2 - eta/batch_size*(grad_W_2)\n        b_2 = b_2 - eta/batch_size*(grad_b_2)\n\n        W_3 = W_3 - eta/batch_size*(grad_W_3)\n        b_3 = b_3 - eta/batch_size*(grad_b_3)\n        \n        norm_1.append(np.linalg.norm(W_1))\n        norm_2.append(np.linalg.norm(W_2))\n        norm_3.append(np.linalg.norm(W_3))\n        \n        # save new weights and biases to be used in the next feedforward step\n        weights = [W_1,W_2,W_3]\n        biases = [b_1,b_2,b_3]\n        \n  \n    \n    print(f\"Epoch:{epoch+1}/{epochs}\")\n\nEpoch:1/10\nEpoch:2/10\nEpoch:3/10\n\n\nEpoch:4/10\nEpoch:5/10\nEpoch:6/10\nEpoch:7/10\n\n\nEpoch:8/10\nEpoch:9/10\nEpoch:10/10\n\n\nNow, lets plot our estimated gradient errors:\n\nplt.plot(grad_errors);\n\n\n\n\nWow! Look at that y axis! Pretty good! We cal also verify:\n\n(np.array(grad_errors) < epsilon).all()\n\nTrue\n\n\nPerfect! Our implementation of backprop is now completely:\n\nnumerically verified‚Ñ¢ üòé"
  },
  {
    "objectID": "posts/nn/nn.html#compare-to-nielsen-implementation",
    "href": "posts/nn/nn.html#compare-to-nielsen-implementation",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "Compare to Nielsen implementation:",
    "text": "Compare to Nielsen implementation:\n\nclass Network(object):\n\n    def __init__(self, sizes, weights=None, biases=None):\n        \"\"\"The list ``sizes`` contains the number of neurons in the\n        respective layers of the network.  For example, if the list\n        was [2, 3, 1] then it would be a three-layer network, with the\n        first layer containing 2 neurons, the second layer 3 neurons,\n        and the third layer 1 neuron.  The biases and weights for the\n        network are initialized randomly, using a Gaussian\n        distribution with mean 0, and variance 1.  Note that the first\n        layer is assumed to be an input layer, and by convention we\n        won't set any biases for those neurons, since biases are only\n        ever used in computing the outputs from later layers.\"\"\"\n        self.num_layers = len(sizes)\n        self.sizes = sizes\n        if weights is None:\n            self.weights = [np.random.randn(y, x)\n                            for x, y in zip(sizes[:-1], sizes[1:])]\n            print(\"random weights\")\n        else:\n            self.weights = weights\n\n        if biases is None:\n            self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n            print(\"random biases\")\n        else:\n            self.biases = biases\n\n    def feedforward(self, a):\n        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n        for b, w in zip(self.biases, self.weights):\n            a = sigmoid(np.dot(w, a)+b)\n        return a\n\n    def SGD(self, training_data, epochs, mini_batch_size, eta,\n            test_data=None):\n        \"\"\"Train the neural network using mini-batch stochastic\n        gradient descent.  The ``training_data`` is a list of tuples\n        ``(x, y)`` representing the training inputs and the desired\n        outputs.  The other non-optional parameters are\n        self-explanatory.  If ``test_data`` is provided then the\n        network will be evaluated against the test data after each\n        epoch, and partial progress printed out.  This is useful for\n        tracking progress, but slows things down substantially.\"\"\"\n        if test_data: n_test = len(test_data)\n        n = len(training_data)\n        for j in range(epochs):\n#             random.shuffle(training_data)\n            mini_batches = [\n                training_data[k:k+mini_batch_size]\n                for k in range(0, n, mini_batch_size)]\n            for mini_batch_idx in range(len(mini_batches)):\n                mini_batch = mini_batches[mini_batch_idx]\n                self.update_mini_batch(mini_batch, eta)\n            if test_data:\n                print(\"Epoch {0}: {1} / {2}\".format(\n                    j, self.evaluate(test_data), n_test))\n            else:\n                print(\"Epoch {0} complete\".format(j))\n\n    def update_mini_batch(self, mini_batch, eta):\n        \"\"\"Update the network's weights and biases by applying\n        gradient descent using backpropagation to a single mini batch.\n        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n        is the learning rate.\"\"\"\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        for x, y in mini_batch:\n            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n            nielsen_w.append(delta_nabla_w)\n            nielsen_b.append(delta_nabla_b)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n        self.weights = [w-(eta/len(mini_batch))*nw\n                        for w, nw in zip(self.weights, nabla_w)]\n        self.biases = [b-(eta/len(mini_batch))*nb\n                       for b, nb in zip(self.biases, nabla_b)]\n\n    def backprop(self, x, y):\n        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n        gradient for the cost function C_x.  ``nabla_b`` and\n        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n        to ``self.biases`` and ``self.weights``.\"\"\"\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        nielsen_bp['x'].append(x)\n        nielsen_bp['y'].append(y)\n        \n        # feedforward\n        activation = x\n        activations = [x] # list to store all the activations, layer by layer\n        zs = [] # list to store all the z vectors, layer by layer\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation)+b\n            zs.append(z)\n            activation = sigmoid(z)\n            activations.append(activation)\n        nielsen_bp['act'].append(activations)\n        # backward pass\n        delta = self.cost_derivative(activations[-1], y) * \\\n            sigmoid_prime(zs[-1])\n        nielsen_bp['delta'].append(delta)\n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n        nielsen_bp['w'].append(nabla_w[-1])\n        # Note that the variable l in the loop below is used a little\n        # differently to the notation in Chapter 2 of the book.  Here,\n        # l = 1 means the last layer of neurons, l = 2 is the\n        # second-last layer, and so on.  It's a renumbering of the\n        # scheme in the book, used here to take advantage of the fact\n        # that Python can use negative indices in lists.\n        for l in range(2, self.num_layers):\n            z = zs[-l]\n            sp = sigmoid_prime(z)\n            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n            nabla_b[-l] = delta\n            nielsen_bp['delta'].append(delta)\n            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n            nielsen_bp['w'].append(nabla_w[-1])\n        return (nabla_b, nabla_w)\n\n    def evaluate(self, test_data):\n        \"\"\"Return the number of test inputs for which the neural\n        network outputs the correct result. Note that the neural\n        network's output is assumed to be the index of whichever\n        neuron in the final layer has the highest activation.\"\"\"\n        test_results = [(np.argmax(self.feedforward(x)), y)\n                        for (x, y) in test_data]\n        return sum(int(x == y) for (x, y) in test_results)\n\n    def cost_derivative(self, output_activations, y):\n        \"\"\"Return the vector of partial derivatives \\partial C_x /\n        \\partial a for the output activations.\"\"\"\n        return (output_activations-y)\n\n#### Miscellaneous functions\ndef sigmoid(z):\n    \"\"\"The sigmoid function.\"\"\"\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    \"\"\"Derivative of the sigmoid function.\"\"\"\n    return sigmoid(z)*(1.0-sigmoid(z))\n\n\nnielsen_w = []\nnielsen_b = []\nnielsen_bp = {'x':[],'y':[],'act':[],'delta':[],'w':[]}\n\nnet = Network([2,3,3,1],weights=[W_1_init, W_2_init, W_3_init],\n              biases=[b_1_init, b_2_init, b_3_init])\n\n\ny_nielsen = net.feedforward(X.T) \n_,act = feedforward_batch(X,weights=[W_1_init, W_2_init, W_3_init],\n                           biases=[b_1_init, b_2_init, b_3_init])\ny_mine = act[-1]\n\n\ny_nielsen[:,1:10]\n\narray([[0.24102498, 0.24030022, 0.24219846, 0.24222601, 0.2414524 ,\n        0.2416362 , 0.2422655 , 0.24063667, 0.24248245]])\n\n\n\ny_mine[:,1:10]\n\narray([[0.24102498, 0.24030022, 0.24219846, 0.24222601, 0.2414524 ,\n        0.2416362 , 0.2422655 , 0.24063667, 0.24248245]])\n\n\n\n((y_nielsen - y_mine)<0.001).all()\n\nTrue\n\n\n\nnp.array_equal(y_nielsen,y_mine)\n\nTrue\n\n\nThey are the same before training with he same weights, so the problem isnt in feedforward.\n\nnet.SGD([(lx.reshape(-1,1),ly.reshape(-1,1)) for lx,ly in zip(X,y)], epochs,batch_size,eta)\n\nEpoch 0 complete\nEpoch 1 complete\nEpoch 2 complete\nEpoch 3 complete\n\n\nEpoch 4 complete\nEpoch 5 complete\nEpoch 6 complete\nEpoch 7 complete\n\n\nEpoch 8 complete\nEpoch 9 complete\n\n\n\nnet.weights\n\n[array([[0.70809966, 0.22304797],\n        [0.67298385, 0.29395349],\n        [0.49821938, 0.05956363]]),\n array([[0.56422281, 0.04320534, 0.70577901],\n        [0.92917503, 0.15010985, 0.10178283],\n        [0.03982933, 0.78097305, 0.35281214]]),\n array([[-0.00879669, -0.81116172, -0.85877154]])]\n\n\n\nweights\n\n[array([[0.70809966, 0.22304797],\n        [0.67298385, 0.29395349],\n        [0.49821938, 0.05956363]]),\n array([[0.56422281, 0.04320534, 0.70577901],\n        [0.92917503, 0.15010985, 0.10178283],\n        [0.03982933, 0.78097305, 0.35281214]]),\n array([[-0.00879669, -0.81116172, -0.85877154]])]\n\n\n\n[np.array_equal(i,j) for i,j in zip(weights,net.weights)]\n\n[True, True, True]\n\n\n\n[np.array_equal(i,j) for i,j in zip(biases,net.biases)]\n\n[True, True, True]"
  },
  {
    "objectID": "posts/nn/nn.html#fully-vectorized-implementation",
    "href": "posts/nn/nn.html#fully-vectorized-implementation",
    "title": "A Bottom-Up Introduction to Neural Networks",
    "section": "Fully vectorized implementation",
    "text": "Fully vectorized implementation\nOk! We‚Äôve done a lot of hard work! Now we‚Äôve come to the final linear algebraic thing we need to do to make this fast üèéüí®.\nWe want to be able to backprop across batches, instead of across individual samples in a batch.\nTo do so, we will have to introduce 3D matrices!. In math these are called tensors!\n\nüìñ Definition: For our purposes, a tensor is defined as a multidimensional matrix.\nNote: In python, these objects are called ‚ÄúN-D arrays‚Äù or just ‚Äúarrays‚Äù. Vectors, and matrices are just 1D and 2D versions of these tensors/arrays.\n\nLets create one right now!\n\na = np.random.randint(1,10,(3,3,3))\na\n\narray([[[4, 8, 4],\n        [6, 9, 8],\n        [6, 6, 3]],\n\n       [[3, 7, 3],\n        [5, 6, 8],\n        [3, 5, 9]],\n\n       [[7, 8, 2],\n        [7, 8, 9],\n        [5, 9, 2]]])\n\n\nü§î\nThis doesn‚Äôt look that special. Thats just because were printing this 3D object on a 2D notebook. Pay special attention to the [ and ] characters and you‚Äôll notice its almost like its a list of a 2D matrices. Indeed, for right now, we can just think of it like that.\nLets grab the first matrix in the list:\n\na[0,:,:]\n\narray([[4, 8, 4],\n       [6, 9, 8],\n       [6, 6, 3]])\n\n\nAnd the second:\n\na[1,:,:]\n\narray([[3, 7, 3],\n       [5, 6, 8],\n       [3, 5, 9]])\n\n\nAnd the last:\n\na[-1,:,:]\n\narray([[7, 8, 2],\n       [7, 8, 9],\n       [5, 9, 2]])\n\n\nOk, so far so good. Lets understand one more thing we need to do with these objects.\nWe want to be able to take this list/stack of matrices, and multiply the stack with another vector matrix. In other words, if I have a tensor T, which contains four matrices which we will call A, B, C, and D I want to be able to do something like:\nT*v = R\nwhere R is now a stack which contains Av,Bv, Cv, Dv.\nAnd then at the end, I can average across the stack dimension, to produce one final 2D matrix which is just:\n(Av+Bv+Cv+Dv)/4\nOk. We‚Äôve described in words the procedure we want to perform, so lets go about coding it up! To make this clear, lets define our tensor T as we described above:\n\nA = np.ones((3,3))\nA\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\n\nB = np.ones((3,3))*2\nB\n\narray([[2., 2., 2.],\n       [2., 2., 2.],\n       [2., 2., 2.]])\n\n\n\nC = np.ones((3,3))*3\nC\n\narray([[3., 3., 3.],\n       [3., 3., 3.],\n       [3., 3., 3.]])\n\n\n\nD = np.ones((3,3))*4\nD\n\narray([[4., 4., 4.],\n       [4., 4., 4.],\n       [4., 4., 4.]])\n\n\n\nT = np.zeros((4,3,3))\nT[0,:,:] = A\nT[1,:,:] = B\nT[2,:,:] = C\nT[3,:,:] = D\nT.shape,T\n\n((4, 3, 3),\n array([[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n \n        [[2., 2., 2.],\n         [2., 2., 2.],\n         [2., 2., 2.]],\n \n        [[3., 3., 3.],\n         [3., 3., 3.],\n         [3., 3., 3.]],\n \n        [[4., 4., 4.],\n         [4., 4., 4.],\n         [4., 4., 4.]]]))\n\n\nNote: we could have also used np.stack to do this for us:\n\nnp.stack((A,B,C,D)).shape\n\n(4, 3, 3)\n\n\nAnd now lets create that vector we wanted:\n\nv = np.ones((3,1))\nv\n\narray([[1.],\n       [1.],\n       [1.]])\n\n\nand verify how it interacts with each matrix in our stack:\n\nA.dot(v)\n\narray([[3.],\n       [3.],\n       [3.]])\n\n\n\nB.dot(v)\n\narray([[6.],\n       [6.],\n       [6.]])\n\n\n\nC.dot(v)\n\narray([[9.],\n       [9.],\n       [9.]])\n\n\n\nD.dot(v)\n\narray([[12.],\n       [12.],\n       [12.]])\n\n\nOk! So now we know what we want R to look like. It should be the stack of those vectors.\nWe can create this using the matmul function, which stands for matrix multiply. We can check its documentation:\n\n# np.matmul?\n\nThe line that we are interested in is the following:\nIf either argument is N-D, N > 2, it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly.\nThats exactly what we want! So lets use it!\n\nR = np.matmul(T,v)\nR\n\narray([[[ 3.],\n        [ 3.],\n        [ 3.]],\n\n       [[ 6.],\n        [ 6.],\n        [ 6.]],\n\n       [[ 9.],\n        [ 9.],\n        [ 9.]],\n\n       [[12.],\n        [12.],\n        [12.]]])\n\n\nPerfect! Thats what we expected! Lets stop and realize this:\nYou can think of this one function as having handled the three separate multiplications for us! In one function! And it did so as fast as possible because its relying on the underlying numerical libraries!\nHint: In a few cells from now, we will see this is exactly what we want to do for our batch!\nThe matmul function is also represented by the handy @ symbol, meaning we could also just type:\n\nR = T@v\nR\n\narray([[[ 3.],\n        [ 3.],\n        [ 3.]],\n\n       [[ 6.],\n        [ 6.],\n        [ 6.]],\n\n       [[ 9.],\n        [ 9.],\n        [ 9.]],\n\n       [[12.],\n        [12.],\n        [12.]]])\n\n\nNote: matmul/@ work as we expect in simpler cases:\n\na,b = np.random.randint(1,10,(3,2)),np.random.randint(1,10,(2,3))\na,b\n\n(array([[7, 8],\n        [8, 3],\n        [9, 1]]),\n array([[7, 7, 5],\n        [9, 7, 8]]))\n\n\n\na@b, a.dot(b)\n\n(array([[121, 105,  99],\n        [ 83,  77,  64],\n        [ 72,  70,  53]]),\n array([[121, 105,  99],\n        [ 83,  77,  64],\n        [ 72,  70,  53]]))\n\n\nOne final thing which might be useful, is the einsum function:\n\n# np.einsum?\n\nThis is a very powerful function, but we can use it in a straightforward manner to just describe to numpy the shapes of each of our matrices, and the shape we want afterwards!\nLets revisit the T/R example above\n\nnp.einsum('ijk,kl->ikl',T,v)\n\narray([[[ 3.],\n        [ 3.],\n        [ 3.]],\n\n       [[ 6.],\n        [ 6.],\n        [ 6.]],\n\n       [[ 9.],\n        [ 9.],\n        [ 9.]],\n\n       [[12.],\n        [12.],\n        [12.]]])\n\n\nü§Ø\nWhat?! What just happened! Well, we just described the operation we wanted directly in terms of sizes using the Einstein summation convention.\nWe said T is of size i x j x k, and that v is of size k x l, and I want to turn this into something that is of size i x k x l. And thats exactly what we got!\nThe last thing we want to do is ‚Äúcollapse‚Äù across one of our dimensions to form the average. This is easy in numpy, we just tell numpy across what axis we want to average:\n\nR = T@v\nR\n\narray([[[ 3.],\n        [ 3.],\n        [ 3.]],\n\n       [[ 6.],\n        [ 6.],\n        [ 6.]],\n\n       [[ 9.],\n        [ 9.],\n        [ 9.]],\n\n       [[12.],\n        [12.],\n        [12.]]])\n\n\n\nR.shape\n\n(4, 3, 1)\n\n\n\nnp.mean(R,axis=0)\n\narray([[7.5],\n       [7.5],\n       [7.5]])\n\n\n\nnp.mean(R,axis=0).shape\n\n(3, 1)\n\n\nIt worked! We told numpy to take the average across the first dimension (axis=0), so the result is the 3x1 matrix/vector which is just the average of all of the members of our stack!\n\nLets point out one more potentially confusing aspect of our implementations. Lets take an activation matrix as our example. The tranpose of this matrix is of size batch_size x nodes in our layer.\nWe‚Äôve been using this 2D matrix to represent our collection of vectors, implicitly.\nWe could make this more explicit, by adding on the missing dimension, making it: batch_size x nodes in our layer x 1 - giving us a clearer interpretation of this as a stack of vectors.\n\na = np.random.randint(1,10,(10,3))\na\n\narray([[5, 2, 2],\n       [3, 7, 1],\n       [9, 6, 1],\n       [8, 7, 9],\n       [1, 2, 7],\n       [5, 8, 6],\n       [5, 9, 6],\n       [5, 6, 5],\n       [8, 3, 3],\n       [6, 5, 1]])\n\n\n\na = a[:,:,np.newaxis]\n\nNow, a is a stack of column vectors! Lets grab the first vector:\n\na[0,:,:]\n\narray([[5],\n       [2],\n       [2]])\n\n\n\nImplementation\nThis is the final piece of the puzzle we need. We are now ready to use this in our backprop implementation! Lets look at our old implementation:\n\nactivities, activations = feedforward_batch(X,weights,biases)\n\n\nweight_updates = []\nbias_updates = []\nlayer_num = len(activations)\n\n\nfor activity in activities:\n    print(activity.shape) \n\n(3, 1000)\n(3, 1000)\n(1, 1000)\n\n\n\nfor activation in activations:\n    print(activation.shape) \n\n(2, 1000)\n(3, 1000)\n(3, 1000)\n(1, 1000)\n\n\nThis is like having a batch_size of 1000. Lets continue! We want to move the batch size to the front so that its out of the way!\n\nfor activation in activations:\n    print(activation.T.shape) \n\n(1000, 2)\n(1000, 3)\n(1000, 3)\n(1000, 1)\n\n\nIts to these that we want to add our ‚Äúnew dimension‚Äù to clearly make these a stack:\n\nfor activation in activations:\n    print((activation.T)[:,:,np.newaxis].shape) \n\n(1000, 2, 1)\n(1000, 3, 1)\n(1000, 3, 1)\n(1000, 1, 1)\n\n\nThis is saying: I have a batch_size-many stack, composed of vectors, where each vector is ___ size.\nPerfect! Lets get cooking!\n\n########################################\n#           Last Level\n########################################\n# get last layer specific variables\nz_L = (activities[-1].T)[:,:,np.newaxis]\na_L = (activations[-1].T)[:,:,np.newaxis]\na_L_minus_1 = (activations[-2].T)[:,:,np.newaxis]\nz_L.shape, a_L.shape,a_L_minus_1.shape\n\n((1000, 1, 1), (1000, 1, 1), (1000, 3, 1))\n\n\n\n# calculate delta_L for the last level\ndelta_L = (a_L-y[:,:,np.newaxis])*sigmoid_prime(z_L)\ndelta_L.shape\n\n(1000, 1, 1)\n\n\n\ndeltas = []\ndeltas.append(delta_L)\n\nHere, remember we need to collapse it down across the batch axis!\n\n# calculate bias update\ndc_db_l = delta_L.sum(axis=0)\nbias_updates = [dc_db_l] + bias_updates\ndc_db_l.shape\n\n(1, 1)\n\n\n\n# calcualte weight updates\ndc_dw_l = (delta_L @ a_L_minus_1.transpose(0,2,1)).sum(axis=0)\nweight_updates = [dc_dw_l] + weight_updates\ndc_dw_l.shape\n\n(1, 3)\n\n\n\n########################################\n# loop through each layer, from 2 to L-1\n######################################## \nfor layer in range(2,layer_num):\n    # using level as a **negative index**\n    l = -layer\n\n    # uncomment this print statement \n    # to help you understand how negative indexing works\n    print(f\"Loop variable: {layer}, l={l}, corresponding to layer: l{l}\")\n\n    # get layer values\n    z_l = (activities[l].T)[:,:,np.newaxis]\n    a_l = (activations[l].T)[:,:,np.newaxis]\n    a_l_minus_1 = (activations[l-1].T)[:,:,np.newaxis]\n    \n    # calculate delta_l for each layer\n    delta_l = (weights[l+1].T @ deltas[l+1]) * sigmoid_prime(z_l)\n    deltas = [delta_l] + deltas\n\n    # calculate bias update\n    dc_db_l = delta_l.sum(axis=0)\n    bias_updates = [dc_db_l] + bias_updates\n\n    # calcualte weight updates\n    dc_dw_l = (delta_l @ a_l_minus_1.transpose(0,2,1)).sum(axis=0)\n    weight_updates = [dc_dw_l] + weight_updates\n    \n    print(f'=========Layer:{layer_num+l}=========')\n    print(f'Using negative index: {l}')\n    print(f'z_l:{z_l.shape}, a_l:{a_l.shape}, a_l_minus_1:{a_l_minus_1.shape}')\n    print(f'delta_l:{delta_l.shape}, dc_db_l:{dc_db_l.shape}, dc_dw_l:{dc_dw_l.shape}')\n    print()\n\nLoop variable: 2, l=-2, corresponding to layer: l-2\n=========Layer:2=========\nUsing negative index: -2\nz_l:(1000, 3, 1), a_l:(1000, 3, 1), a_l_minus_1:(1000, 3, 1)\ndelta_l:(1000, 3, 1), dc_db_l:(3, 1), dc_dw_l:(3, 3)\n\nLoop variable: 3, l=-3, corresponding to layer: l-3\n=========Layer:1=========\nUsing negative index: -3\nz_l:(1000, 3, 1), a_l:(1000, 3, 1), a_l_minus_1:(1000, 2, 1)\ndelta_l:(1000, 3, 1), dc_db_l:(3, 1), dc_dw_l:(3, 2)\n\n\n\nWow! That was easy! We backproped across every single example in our batch!\nWe‚Äôre done! Lets define a new function that does the above:\n\ndef backprop_batch(activities,activations,y_batch,quiet=True):\n    weight_updates = []\n    bias_updates = []\n    layer_num = len(activations)\n    \n    ########################################\n    #           Last Level\n    ########################################\n    # get last layer specific variables\n    z_L = (activities[-1].T)[:,:,np.newaxis]\n    a_L = (activations[-1].T)[:,:,np.newaxis]\n    a_L_minus_1 = (activations[-2].T)[:,:,np.newaxis]\n    \n    # calculate delta_L for the last level\n    delta_L = (a_L-y_batch[:,:,np.newaxis])*sigmoid_prime(z_L)\n\n    deltas = []\n    deltas.append(delta_L)\n    \n    # calculate bias update\n    dc_db_L = delta_L.sum(axis=0)\n    bias_updates = [dc_db_L] + bias_updates\n    \n    # calcualte weight updates\n    dc_dw_L = (delta_L @ a_L_minus_1.transpose(0,2,1)).sum(axis=0)\n    weight_updates = [dc_dw_L] + weight_updates\n    \n\n    ########################################\n    # loop through each layer, from 2 to L-1\n    ######################################## \n    for layer in range(2,layer_num):\n        # using level as a **negative index**\n        l = -layer\n\n        # uncomment this print statement \n        # to help you understand how negative indexing works\n        if not quiet: print(f\"Loop variable: {layer}, l={l}, corresponding to layer: l{l}\")\n\n        # get layer values\n        z_l = (activities[l].T)[:,:,np.newaxis]\n        a_l = (activations[l].T)[:,:,np.newaxis]\n        a_l_minus_1 = (activations[l-1].T)[:,:,np.newaxis]\n\n        # calculate delta_l for each layer\n        delta_l = (weights[l+1].T @ deltas[l+1]) * sigmoid_prime(z_l)\n        deltas = [delta_l] + deltas\n\n        # calculate bias update\n        dc_db_l = delta_l.sum(axis=0)\n        bias_updates = [dc_db_l] + bias_updates\n\n        # calcualte weight updates\n        dc_dw_l = (delta_l @ a_l_minus_1.transpose(0,2,1)).sum(axis=0)\n        weight_updates = [dc_dw_l] + weight_updates\n    \n        if not quiet:\n            print(f'=========Layer:{layer_num+l}=========')\n            print(f'Using negative index: {l}')\n            print(f'z_l:{z_l.shape}, a_l:{a_l.shape}, a_l_minus_1:{a_l_minus_1.shape}')\n            print(f'delta_l:{delta_l.shape}, dc_db_l:{dc_db_l.shape}, dc_dw_l:{dc_dw_l.shape}')\n            print()\n    \n    return weight_updates,bias_updates\n\nAnd now lets use it!\n\n# dataset\nN = X.shape[0]\nd = X.shape[1]\nbatch_size = 50\nepochs = 10\neta=1e-2\ndataset=X\nlabels=y\nquiet=False\n\n# gradient checking\nepsilon = 1e-7\n\n# initialize weight matrices - notice the dimensions\nW_1_init = np.random.rand(3,2)\nW_2_init = np.random.rand(3,3)\nW_3_init = np.random.randn(1,3)\nweights = [W_1_init,W_2_init,W_3_init]\n\n# and our biases\nb_1_init = np.random.rand(3,1)\nb_2_init = np.random.rand(3,1)\nb_3_init = np.random.rand(1,1)\nbiases = [b_1_init,b_2_init,b_3_init]\n\n# network\nW_1,W_2,W_3 = weights[0],weights[1],weights[2]\nb_1,b_2,b_3 = biases[0],biases[1],biases[2]\n\n# mini-batch params \nnum_batches = int(N/batch_size)\niterations = 0\n\n# various debugging lists\nloss = []\ngrad_norm_1 = []\ngrad_norm_2 = []\ngrad_norm_3 = []\nnorm_1 = []\nnorm_2 = []\nnorm_3 = []\ngrad_errors = []\ndiff_vecs = []\ngrad_vecs = []\nmy_act = []\nmy_w = []\nmy_b = []\nmy_bp = {'x':[],'y':[],'act':[],'delta':[],'w':[]}\n\nfor epoch in range(epochs):\n    # shuffle the dataset - note we do this by shuffling indices\n#     indices = np.random.permutation(N)\n#     shuffled_dataset = dataset[indices,:]\n#     shuffled_labels = labels[indices,:]\n    shuffled_dataset = dataset\n    shuffled_labels = labels\n        \n    # work on current batch \n    for batch_num in range(num_batches):\n\n        # get current batch\n        batch_start = batch_num*batch_size\n        batch_end = (batch_num+1)*batch_size\n        batch = shuffled_dataset[batch_start:batch_end,:]\n        batch_labels = shuffled_labels[batch_start:batch_end,:].reshape(-1,1)\n\n        # feedforward on batch\n        activities, activations = feedforward_batch(batch,weights,biases)\n        my_act.append(activations[-1])\n\n        # backprop on batch\n        weight_grads,bias_grads = backprop_batch(activities,activations,batch_labels)\n\n        # take your steps:\n        W_1 = W_1 - eta/batch_size*(weight_grads[0])\n        b_1 = b_1 - eta/batch_size*(bias_grads[0])\n\n        W_2 = W_2 - eta/batch_size*(weight_grads[1])\n        b_2 = b_2 - eta/batch_size*(bias_grads[1])\n\n        W_3 = W_3 - eta/batch_size*(weight_grads[2])\n        b_3 = b_3 - eta/batch_size*(bias_grads[2])\n        \n        norm_1.append(np.linalg.norm(W_1))\n        norm_2.append(np.linalg.norm(W_2))\n        norm_3.append(np.linalg.norm(W_3))\n        \n        # save new weights and biases to be used in the next feedforward step\n        weights = [W_1,W_2,W_3]\n        biases = [b_1,b_2,b_3]\n        \n  \n    \n    print(f\"Epoch:{epoch+1}/{epochs}\")\n\nEpoch:1/10\nEpoch:2/10\nEpoch:3/10\n\n\nEpoch:4/10\n\n\nEpoch:5/10\nEpoch:6/10\nEpoch:7/10\nEpoch:8/10\nEpoch:9/10\nEpoch:10/10\n\n\n\nweights\n\n[array([[0.73240605, 0.38135829],\n        [0.69464115, 0.24507531],\n        [0.72734686, 0.53434862]]),\n array([[0.53256162, 0.06228779, 0.97612581],\n        [0.49255969, 0.80325147, 0.83507987],\n        [0.82934198, 0.82415587, 0.88704505]]),\n array([[ 0.37292057, -0.95255426,  0.69064931]])]\n\n\n\nnielsen_w = []\nnielsen_b = []\nnielsen_bp = {'x':[],'y':[],'act':[],'delta':[],'w':[]}\n\nnet = Network([2,3,3,1],weights=[W_1_init, W_2_init, W_3_init],\n              biases=[b_1_init, b_2_init, b_3_init])\n\n\nnet.SGD([(lx.reshape(-1,1),ly.reshape(-1,1)) for lx,ly in zip(X,y)], epochs,batch_size,eta)\n\nEpoch 0 complete\n\n\nEpoch 1 complete\nEpoch 2 complete\nEpoch 3 complete\n\n\nEpoch 4 complete\n\n\nEpoch 5 complete\nEpoch 6 complete\nEpoch 7 complete\n\n\nEpoch 8 complete\n\n\nEpoch 9 complete\n\n\n\nnet.weights\n\n[array([[0.73240605, 0.38135829],\n        [0.69464115, 0.24507531],\n        [0.72734686, 0.53434862]]),\n array([[0.53256162, 0.06228779, 0.97612581],\n        [0.49255969, 0.80325147, 0.83507987],\n        [0.82934198, 0.82415587, 0.88704505]]),\n array([[ 0.37292057, -0.95255426,  0.69064931]])]\n\n\n\nweights\n\n[array([[0.73240605, 0.38135829],\n        [0.69464115, 0.24507531],\n        [0.72734686, 0.53434862]]),\n array([[0.53256162, 0.06228779, 0.97612581],\n        [0.49255969, 0.80325147, 0.83507987],\n        [0.82934198, 0.82415587, 0.88704505]]),\n array([[ 0.37292057, -0.95255426,  0.69064931]])]"
  },
  {
    "objectID": "posts/opt/opt.html",
    "href": "posts/opt/opt.html",
    "title": "An Introduction to Optimization",
    "section": "",
    "text": "In this notebook we will walk through the several gradient-based optimization techniques from scratch.\nWe will start with a simple introduction to Gradient Descent, and work our way up to a vectorized implementation of Mini-Batch Stochastic Gradienet Descent with Momentum, and even ADAM.\nIn so doing, we will fully explain all the required numpy and linear algebra along the way.\nThis notebook aims to walk through all of these steps in detail, and by the end, we will generate the following animation which shows how various optimiation algorithms behave on a Multi-variate Polynomial Regression problem:\n\n\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport seaborn as sns\n\n# for creating animations\nimport matplotlib.animation\nfrom IPython.display import HTML\nfrom IPython.display import Image, Video\n\n\n# styling additions\nfrom IPython.display import HTML\n# style = \"<style>div.warn{background-color: #fcf2f2;border-color: #dFb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;}</style>\"\nstyle = \"<style>div.info{padding: 15px; border: 1px solid transparent; border-left: 5px solid #dfb5b4; border-color: transparent; margin-bottom: 10px; border-radius: 4px; background-color: #fcf8e3; border-color: #faebcc;}</style>\"\nHTML(style)\n\n\n\n\n\n\nThis notebook can (re)generate animations of optimization procedures, but that can take quite some time. In its place, you can use pre-generated GIFs/MP4s by togglnig this flag:\n\nregenerate_anim = False"
  },
  {
    "objectID": "posts/opt/opt.html#hypothesis-space",
    "href": "posts/opt/opt.html#hypothesis-space",
    "title": "An Introduction to Optimization",
    "section": "Hypothesis Space",
    "text": "Hypothesis Space\nNow this our dataset. Say we want to fit a line to this dataset, so our hypothesis takes the form:\nh(x) = mx+b \nThis model has two parameters that we have to learn: m and b.\nAlthogh this is not typical in linear regression, we can also call them weights:\nh(x) = w_0 + w_1x\nAnother notational thing that we can do now that will be useful to us later, is introduce a notational convenience: x_0 = 1. This lets us rewrite our model as:\nh(x) = w_0 x_0 + w_1x_1\nwhere our original variable x we‚Äôre now calling x_1.\nRemember! This is just a ‚Äútrick‚Äù of notation, because x_0 is defined to be 1, so we actually have:\n\n\\begin{align}\nh(x) &= w_0 x_0 + w_1x_1 \\\\\n     &= w_0 (1) + w_1x_1 \\\\\n     &= w_0 + w_1x_1 \\\\\n     &= b + mx\n\\end{align}\n\nThis notational trick now lets us write out our model in succinct linear algebraic notation as a dot product using vectors:\n\n\\begin{align}\nh(\\mathbf{x}) &= \\mathbf{w}\\cdot\\mathbf{x} \\\\\n              &= w_0x_0 + w_1x_1 \\\\\n              &= \\mathbf{w}^T \\mathbf{x}\n\\end{align}\n\nLets quickly remind ourselves about column vectors, row vectors and the dot product:\n\nWe usually assume all vectors are written as column vectors, meaning they look like this: \n\\mathbf{a} =\n\\begin{bmatrix}\na_0 \\\\ a_1\n\\end{bmatrix}\n\nSo when we transponse them, they become row vectors: \n\\mathbf{b}^T =\n\\begin{bmatrix}\nb_0 & b_1\n\\end{bmatrix}\n\nThis lets us write our dot product as a vector-vector multiplication of a column vector and a row vector: \n\\begin{align}\n\\mathbf{a} \\cdot \\mathbf{b} &= a_0b_0 + a_1b_1 \\\\\n                            &= \\mathbf{a}^T\\mathbf{b} \\\\\n                            &= \\mathbf{b}^T\\mathbf{a}\n\\end{align}\n\n\nSo lets summarize: we want to learn a linear model for our dataset:\n\\hat{y}=h(x)=mx+b\nmeaning we want a slope: m, and an intercept: b.\nWe have restated this problem in more linear algebraic notation, as wanting to find a 2-dimensional weight vector: \\mathbf{w} for our linear model:\nh(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}\nSo we can interpret the components of \\mathbf{w} as:\n\n\\mathbf{w} =\n\\begin{bmatrix}\nw_0 \\\\ w_1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nb \\\\ m\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nintercept \\\\ slope\n\\end{bmatrix}\n\n\nNow, lets go one step further with our notation! Note that as we‚Äôve specified it above,\nh(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}\nonly specifies a single \\hat{y}, for a single example of our dataset: \\mathbf{x}.\nSo, to abuse notation even futher, (trust me, I don‚Äôt want to but this is how its almost always presented so we might as well get used to it!) we can make this explicity by saying:\n\\hat{y}_i = h(\\mathbf{x}_i) = \\mathbf{w}^T\\mathbf{x}_i\nWe want to generate our estimates for each input example \\mathbf{x}_i. We can picture what we want like this:\n\n\\begin{align}\n    \\hat{y}_1 &= h(\\mathbf{x}_i) = \\mathbf{w}^T\\mathbf{x}_i \\\\\n    \\hat{y}_2 &= h(\\mathbf{x}_2) = \\mathbf{w}^T\\mathbf{x}_2 \\\\\n    \\ldots \\\\\n    \\hat{y}_i &= h(\\mathbf{x}_i) = \\mathbf{w}^T\\mathbf{x}_i \\\\\n    \\ldots \\\\\n    \\hat{y}_N &= h(\\mathbf{x}_N) = \\mathbf{w}^T\\mathbf{x}_N \\\\\n\\end{align}\n\nI‚Äôve laid out suggesting we can do this with linear algebra, and indeed we can! We want to form a vector of our estimates: \\mathbf{\\hat{y}} which of size (N \\times 1). To do so, we need to calculate the following operation:\n\n\\begin{bmatrix}\n    \\mathbf{w}^T\\mathbf{x}_i \\\\\n    \\mathbf{w}^T\\mathbf{x}_2 \\\\\n    \\mathbf{w}^T\\mathbf{x}_i \\\\\n    \\mathbf{w}^T\\mathbf{x}_N \\\\\n\\end{bmatrix}\n\nThis is exactly the result of a matrix-vector product of our dataset matrix X, with our weight vector \\mathbf{w}!\n\nX\\mathbf{w} =\n\\begin{bmatrix}\n    \\mathbf{w}^T\\mathbf{x}_i \\\\\n    \\mathbf{w}^T\\mathbf{x}_2 \\\\\n    \\mathbf{w}^T\\mathbf{x}_i \\\\\n    \\mathbf{w}^T\\mathbf{x}_N \\\\\n\\end{bmatrix}\n\nWe can quickly verify that sizes indeed work out: \n\\underbrace{X}_{(N \\times 2)} \\underbrace{\\mathbf{w}}_{(2 \\times 1)} =\n\\begin{bmatrix}\n    \\mathbf{w}^T\\mathbf{x}_i \\\\\n    \\mathbf{w}^T\\mathbf{x}_2 \\\\\n    \\mathbf{w}^T\\mathbf{x}_i \\\\\n    \\mathbf{w}^T\\mathbf{x}_N \\\\\n\\end{bmatrix}\n\nSo now we have a formula, which lets us a single weight vector \\mathbf{w} and apply it to our entire dataset at once, to generate all of our estimates at once:\n X\\mathbf{w} = \\mathbf{\\hat{y}}"
  },
  {
    "objectID": "posts/opt/opt.html#dataset",
    "href": "posts/opt/opt.html#dataset",
    "title": "An Introduction to Optimization",
    "section": "Dataset",
    "text": "Dataset\nLets take a second to talk about our dataset matrix X. Above, we did the following to look at our dataset:\n\nplt.scatter(x,y);\n\n\n\n\nAt first glance, we don‚Äôt see any matrix here! Our x‚Äôs are only 1 dimensional! Well, recall the convention that x_0=1! So our dataset matrix is actually:\n\nX =\n\\begin{bmatrix}\n1 & x_1 \\\\\n\\ldots \\\\\n1 & x_i \\\\\n\\ldots \\\\\n1 & x_N\n\\end{bmatrix}\n\nwhere now i is indexing from 1,\\ldots,N - the size of our dataset. So lets go ahead and form it:\n\nX = np.ones((N,2))\nX[:,1] = x\nX[0:5,:]\n\narray([[ 1.        , -2.        ],\n       [ 1.        , -1.95959596],\n       [ 1.        , -1.91919192],\n       [ 1.        , -1.87878788],\n       [ 1.        , -1.83838384]])\n\n\nNow lets assemble our output vector:\n\ny = y[:,None]\n# or y = y[:, np.newaxis]\nprint(y.shape)\ny[0:5,:]\n\n(100, 1)\n\n\narray([[-4.54437285],\n       [-6.75779464],\n       [-5.2087573 ],\n       [-4.73313111],\n       [-5.35793348]])\n\n\n\nBroadcasting\n\n\n\n\n\n\nImportant\n\n\n\nNote: Its important for this vector to be an actual column vector (i.e.¬†have its last dimension be 1. Otherwise, a somewhat non-intuitive (unless you are familiar with and love numpy‚Äôs array broadcasting rules) broadcast happens. As an example:\n\n\n\na = np.random.rand(3,1)\na.shape,a\n\n((3, 1),\n array([[0.09708829],\n        [0.71928046],\n        [0.79971499]]))\n\n\n\nb = np.random.rand(3)\nb.shape,b\n\n((3,), array([0.46586583, 0.67176581, 0.4836771 ]))\n\n\nAs we can see, at first glance these are both vectors of length 3‚Ä¶right? Lets substract them and see what happens:\n\nresult = a-b\nresult.shape,result\n\n((3, 3),\n array([[-0.36877755, -0.57467753, -0.38658881],\n        [ 0.25341463,  0.04751465,  0.23560336],\n        [ 0.33384915,  0.12794917,  0.31603789]]))\n\n\n\nYou-should-know: This is not what you might except as a mathematician, but you must familiarize yourself with and come to love broadcasting as a Data scientist!\n\nTo prevent having to squeeze things after a dot product, I think its just easier to ensure that y has the proper dimension."
  },
  {
    "objectID": "posts/opt/opt.html#cost-function",
    "href": "posts/opt/opt.html#cost-function",
    "title": "An Introduction to Optimization",
    "section": "Cost function",
    "text": "Cost function\nNow that we understand our model, we can talk about how exactly we‚Äôre going to learn what the best weights should be.\nIn order to learn, we need a measure of distance or cost or quality of our current guess, which tells us how good/bad were doing so far. As we‚Äôve discussed in class, a natural loss for regression is the regression loss, or the mean squared error (MSE) loss:\n\nC(\\mathbf{w}) = \\underbrace{\\frac{1}{N} \\sum_{i=1}^N}_{\\text{mean}} \\;\\; \\underbrace{(\\hat{y}_i-y_i)^2}_{\\text{squared error}}\n\n\nPause and Ponder: Often times, you‚Äôll see this cost function defined with a \\frac{1}{2N} instead of just \\frac{1}{N}. * Why do you think that is? * Do you think that will affect our learning procedure?\n\n\nThe cost function given above looks a bit strange at first. I‚Äôve written it as a function of our weight vector \\mathbf{w}, but \\mathbf{w} does not appear on the right hand side, but \\hat{y}_i does. Recall, that all of our individual \\hat{y}_i are generated from one weight vector. We could also rewrite the above as:\n\nC(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^N \\;\\; (\\mathbf{w}^T\\mathbf{x}_i-y_i)^2\n\nSo, we can now calculate this for out entire dataset! Lets do so below, for a random weight vector:\n\nw_rand = np.random.random(size=(2,1))\nw_rand\n\narray([[0.51374312],\n       [0.57865621]])\n\n\nNow, lets loop through all N data points and calculate this cost:\n\ncost = 0\nfor i in range(N):\n    x_i = X[i,:]\n    y_hat_i = w_rand.T.dot(x_i)\n    error_i = y_hat_i - y[i]\n    cost = cost + error_i**2\n\ncost = np.squeeze(cost/N)  # the squeeze just gets rid of empty dimensions\ncost\n\narray(10.13753696)\n\n\nSo this is our cost for a random weight vector. Lets define this as a fuction, so we can plot it over the space of w_0 and w_1.\n\nRecall: this is the same as plotting the cost over the sapce of intercepts (w_0) and slopes (w_1)\n\n\ndef cost(dataset,w):\n    N = dataset.shape[0]\n    cost = 0\n    for i in range(N):\n        x_i = dataset[i,:]\n        y_hat_i = w.T.dot(x_i)\n        error_i = y_hat_i - y[i]\n        cost = cost + error_i**2\n\n    return np.squeeze(cost/N)\n\nLets verify it works as we expect:\n\ncost(X,w_rand)\n\narray(10.13753696)\n\n\nPerfect - lets now calculate our costs over the entire grid of potential slope and interceps:\nNote: The below is slow! But right now Im purposefully not vectorizing anything so it follows our intuition\n\nlen_w = 100\nw_0 = np.linspace(0,2,len_w) # possible bias\nw_1 = np.linspace(2,4,len_w) # possible slopes\ncosts = np.zeros((len_w,len_w))\nfor i in range(len_w):\n    for j in range(len_w):\n        current_w = np.asarray([w_0[i],w_1[j]])\n        costs[i,j] = cost(X,current_w)\n\nWow! That took a while!\nNow lets actually plot our cost surface along with a contour plot and the real minimum highlighted.\nNote: you can skip the plotting details in this cell if you want.\n\n# plotting\nW_0, W_1 = np.meshgrid(w_0, w_1)\nCosts = costs.reshape(W_0.shape)\n\nfig = plt.figure(figsize=(15,7))\nax = fig.add_subplot(111,projection='3d')\nax.set_xlabel('w_0')\nax.set_ylabel('w_1')\nax.set_zlabel('cost')\nax.set_zlim([-2,3])\n\nax.plot_surface(W_0, W_1, Costs,cmap='inferno');\nax.contour(W_0, W_1, Costs, cmap='inferno', offset=-2,levels=15);\nax.scatter(1,3,-2,s=30,color='r');   # plot the known minimum\n\n\n\n\nSo now we can actually see our cost surface as a function of our weight vector!\n\nPause and Ponder: Why is it that our true minimum shown in red, is not the minimum of our cost surface?\n\n\n\nVectorized Cost function\nWe‚Äôve come a long way so far, and we haven‚Äôt even learned anything! Before we move forward with that, lets vectorize our cost function.\n\nVectorization is often used to mean ‚Äúrewrite in linear-algebraic notation‚Äù. This results in much faster code, since our numerical libraries have been optimized to operate on vectors/matrices and not for loops!\n\nThe function we just explored was:\n\nC(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^N \\;\\; (\\mathbf{w}^T\\mathbf{x}_i-y_i)^2\n\nBefore we proceed, lets take a quick asside to remind ourselves of the definition of the \\mathcal{L}_2-norm:\n\nThe \\mathcal{L}_2-norm of a vector \\mathbf{a} is:\n\n    \\|\\mathbf{a}\\|_2 = \\sqrt{a_1^2 + \\ldots + a_n^2}\n\nWe are often interested in the squared \\mathcal{L}_2-norm of a vector:\n\n    \\|\\mathbf{a}\\|_2^2 = a_1^2 + \\ldots + a_n^2\n\nWe are also often interested in the squared \\mathcal{L}_2-norm of some kind of distance or error vector:\n\n    \\|\\mathbf{a}-\\mathbf{b}\\|_2^2 = (a_1-b_1)^2 + \\ldots + (a_n-b_n)^2\n\n\nNow, lets use the fact that we can calculate our all our \\hat{y}_i at once and rewrite the cost in terms of the \\mathcal{L}_2-norm we just defined above\n\n\\begin{align}\n    C(\\mathbf{w}) &= \\frac{1}{N} \\| X\\mathbf{w} - \\mathbf{y} \\|_2^2 \\\\\n                  &= \\frac{1}{N} \\| \\mathbf{\\hat{y}} - \\mathbf{y} \\|^2_2 \\\\\n                  &= \\frac{1}{N} \\left[ (\\hat{y}_1 - y_1)^2 + \\ldots + (\\hat{y}_N-y_N)^2 \\right]\n\\end{align}\n\nwhich is exactly what we want! So now we can calculate our cost as:\n\ncost = np.linalg.norm(X.dot(w_rand)-y)**2/X.shape[0]\ncost\n\n10.137536961317753\n\n\nThis is exactly what we had above! Lets now re-define our vectorized cost function:\n\ndef cost(X, y, w):\n    return np.linalg.norm(X.dot(w)-y,axis=0)**2/X.shape[0]\n\nLets re-calculate our costs over a grid and replot it!\n\nlen_w = 100\nw_0 = np.linspace(0,2,len_w)\nw_1 = np.linspace(2,4,len_w)\nW_0, W_1 = np.meshgrid(w_0, w_1)\n\n\nW_0.shape, W_1.shape\n\n((100, 100), (100, 100))\n\n\n\n# combine and reshape mesh for calculation\nwgrid = np.array([W_0,W_1]).reshape(2, len(w_0)**2)\n\n\nPay attention to how much faster this cell is than the double for loop above!\n\n\ncost(X,y,wgrid)\n\narray([3.78144022, 3.74421484, 3.7078057 , ..., 3.3598471 , 3.40179731,\n       3.44456376])\n\n\n\n# calculate on grid, and reshape back for plotting\ncosts = cost(X,y,wgrid).reshape(W_0.shape)\n\nWow! That was fast! Now lets plot it!\n\n# plotting\nfig = plt.figure(figsize=(15,7))\nax = fig.add_subplot(111,projection='3d')\nax.set_xlabel('w_0')\nax.set_ylabel('w_1')\nax.set_zlabel('cost')\nax.set_zlim([-2,3])\n\nax.plot_surface(W_0, W_1, costs,cmap='inferno');\nax.contour(W_0, W_1, costs, cmap='inferno', offset=-2,levels=10);\nax.scatter(1,3,-2,s=30,color='r');   # plot the known minimum\n\n\n\n\nIt works!"
  },
  {
    "objectID": "posts/opt/opt.html#gradient-of-our-cost",
    "href": "posts/opt/opt.html#gradient-of-our-cost",
    "title": "An Introduction to Optimization",
    "section": "Gradient of our cost",
    "text": "Gradient of our cost\nNow, we can really get going! Lets start by giving the gradient of this cost function (recall: we calculated this in class!). Lets also call this cost what it is, the in-sample error:\n\nE_{in}(\\mathbf{w}) = C(\\mathbf{w}) = \\frac{1}{N} \\| X\\mathbf{w} - \\mathbf{y} \\|_2^2\n\nSo its gradient is:\n\n\\begin{align}\n    \\nabla E_{in}(\\mathbf{w}) &= \\frac{2}{N}(X^TX\\mathbf{w} - X^T\\mathbf{y})\n\\end{align}\n\nRecall an earlier pause-and-ponder where I asked about the definition of the cost function with a \\frac{1}{2N} vs \\frac{1}{N}!\nLets also note an underappreciated property of this function which will become relevant later:\n\nPause-and-ponder: This gradient is a function of our entire dataset! This means that at each point in weight space, to calculate the gradient we need to proces our entire dataset!\n\nKeep that point in mind as we will come back to it later!"
  },
  {
    "objectID": "posts/opt/opt.html#gradient-descent-for-linear-regression",
    "href": "posts/opt/opt.html#gradient-descent-for-linear-regression",
    "title": "An Introduction to Optimization",
    "section": "Gradient Descent for Linear Regression",
    "text": "Gradient Descent for Linear Regression\nNow we have all the pieces together to start talking about and implement gradient descent! Recall the weight update equation we gave in class:\n\\boldsymbol{w}(t+1) = \\boldsymbol{w}(t) + \\eta \\hat{\\boldsymbol{v}}\nwhere \\hat{\\boldsymbol{v}} is the unit-vector in the direction we want to step.\nFor gradient descent, this becomes:\n\nThe gradient descent algorithm is just a step in the direction of the negative gradient: \\boldsymbol{w}(t+1) = \\boldsymbol{w}(t) -\\eta\\cdot\\nabla f(\\boldsymbol{w}(t))\n\n\nLets look at this for special case of linear regression, by starting with the weight update equation:\n\n\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla E_{in}\n\nLets move \\mathbf{w}_t to the other side so we can focus on the change in our weights:\n\n\\begin{align}\n    \\Delta \\mathbf{w} &= \\mathbf{w}_{t+1} - \\mathbf{w}_t \\\\\n                      &= - \\eta \\nabla E_{in} \\\\\n                      &= - \\eta \\frac{2}{N} (X^TX\\mathbf{w} - X^T\\mathbf{y})\n\\end{align}\n\nWe can now write the GD algorithm for linear regression:\n\nThe gradient descent algorithm for linear regression is:\n\n\\begin{align}\n\\boldsymbol{w}(t+1) &= \\boldsymbol{w}(t) -\\eta \\nabla E_{in} \\\\\n                    &= - \\eta \\frac{2}{N} (X^TX\\mathbf{w} - X^T\\mathbf{y})\n\\end{align}\n\n\nLets first define a function to calculate this gradient:\n\ndef cost_grad(X,y,w):\n    N = X.shape[0]\n    return (2/N)*(X.T.dot(X).dot(w)-X.T.dot(y))\n\nOut of curiosity, lets visualize it!\n\nlen_w = 100\nw_0 = np.linspace(0,2,len_w)\nw_1 = np.linspace(2,4,len_w)\nW_0, W_1 = np.meshgrid(w_0, w_1)\n\n# combine and reshape mesh for calculation\nwgrid = np.array([W_0,W_1]).reshape(2, len(w_0)**2)\n\n\n# calculate on grid, and reshape back for plotting\ncosts = cost(X,y,wgrid).reshape(W_0.shape)\n\n\n# calculate grad on grid, and reshape back for plotting\ngrad_costs = cost_grad(X,y,wgrid).reshape(2,W_0.shape[0],W_0.shape[1])\n\n\n# plotting\nfig = plt.figure(figsize=(15,7))\nax1 = fig.add_subplot(131,projection='3d')\nax1.set_xlabel('w_0')\nax1.set_ylabel('w_1')\nax1.set_zlabel('cost')\nax1.set_zlim([-2,3])\nax1.set_title('Cost')\n\nax1.plot_surface(W_0, W_1, costs,cmap='inferno');\nax1.contour(W_0, W_1, costs, cmap='inferno', offset=-2,levels=10);\nax1.scatter(1,3,-2,s=30,color='r');   # plot the known minimum\n\nax2 = plt.subplot(132, projection='3d', sharex = ax1, sharey=ax1)\nax2.set_xlabel('w_0')\nax2.set_ylabel('w_1')\nax2.set_zlabel('grad')\nax2.set_title('Gradient wrt w_0')\nax2.plot_surface(W_0, W_1, grad_costs[0,:],cmap='inferno');\nax2.contour(W_0, W_1, costs, cmap='inferno', offset=-2,levels=10);\nax2.scatter(1,3,-2,s=30,color='r');   # plot the known minimum\n\nax3 = plt.subplot(133, projection='3d', sharex = ax1, sharey=ax1)\nax3.set_xlabel('w_0')\nax3.set_ylabel('w_1')\nax3.set_zlabel('grad')\nax3.set_title('Gradient wrt w_1')\nax3.plot_surface(W_0, W_1, grad_costs[1,:],cmap='inferno');\nax3.contour(W_0, W_1, costs, cmap='inferno', offset=-2,levels=10);\nax3.scatter(1,3,-2,s=30,color='r');   # plot the known minimum\n\n\n\n\n\nPause-and-ponder: What does this mean? üßê Think about this!\n\n\n\nImplementation\nLets actually implement our weight updates now!\n\ndef gradient_descent(dataset, labels, w_init=None, eta=None, max_iter=300, quiet=False):\n    if w_init is None:\n        w_init = np.random.rand(dataset.shape[1],1)\n    if eta is None:\n        eta = 0.01\n    \n    # clean up initial point sizes:\n    w_init = np.asarray(w_init).reshape(dataset.shape[1],1)\n    \n    # gradient descent params\n    delta = 0.001\n    delta_w = 1\n    iterations = 0\n    \n    # save weights and costs along path\n    wPath = [w_init]\n    cPath = [cost(dataset,labels,w_init)]\n        \n    # take first step\n    w_old = w_init\n    \n    # perform GD\n    while True:\n        # check to see if you should terminate\n        if iterations > max_iter:\n            if not quiet: print(\"terminating GD after max iter:\",max_iter)\n            break\n            \n        if delta_w < delta:\n            if not quiet: print(f\"terminating GD after {iterations} iterations with weight change of {delta}.\")\n            break\n             \n        # update your weights\n        # STEP 1 \n        w_new = w_old - eta* cost_grad(dataset,labels,w_old)\n        \n        # update paths\n        wPath.append(w_new)\n        cPath.append(cost(dataset,labels,w_new))\n        \n        # update iteration and weight change\n        iterations += 1\n        delta_w = np.linalg.norm(w_new - w_old)\n        \n        # STEP 2\n        # make your previous new weights the current weights\n        w_old = w_new\n    \n    # clean up path variables\n    wPath = np.squeeze(np.asarray(wPath))\n    cPath = np.squeeze(np.asarray(cPath))\n    \n    return w_new, wPath, cPath\n\n\nw_gd, wPath_gd, cPath_gd = gradient_descent(X, y, [0.25,2.4], eta = 0.2)\nw_gd\n\nterminating GD after 12 iterations with weight change of 0.001.\n\n\narray([[0.92994594],\n       [3.11226582]])\n\n\n\n\n\nPlot Path\nNow lets plot the path GD took!\n\n# plotting\nlen_w = 100\nw_0 = np.linspace(0,2,len_w)\nw_1 = np.linspace(2,4,len_w)\nW_0, W_1 = np.meshgrid(w_0, w_1)\n\n# combine and reshape mesh for calculation\nwgrid = np.array([W_0,W_1]).reshape(2, len(w_0)**2)\n\n# calculate on grid, and reshape back for plotting\ncosts = cost(X,y,wgrid).reshape(W_0.shape)\n\nfig = plt.figure(figsize=(15,7))\nax1 = fig.add_subplot(121,projection='3d')\nax1.set_xlabel('w_0')\nax1.set_ylabel('w_1')\nax1.set_zlabel('cost')\nax1.set_zlim([-2,3])\nax1.set_title(\"cost surface\")\n\nax1.scatter(wPath_gd[:,0],wPath_gd[:,1],-2,s=10,c=cPath_gd);\nax1.plot_surface(W_0, W_1, costs,cmap='inferno');\nax1.contour(W_0, W_1, costs, cmap='inferno', offset=-2,levels=10,alpha=0.5);\n\nax2 = fig.add_subplot(122);\nax2.plot(cPath_gd);\nax2.set_title(\"Cost during GD\")\nax2.set_xlabel(\"Iteration\");\n\n\n\n\nLets define a quick plotting function to make it easier to re-use:\n\ndef plot_path(wPath,cPath,colorby='cost'):\n    if colorby == 'cost':\n        colorer = cPath\n    else:\n        colorer = range(len(wPath))\n    # plotting\n    len_w = 100\n    w_0 = np.linspace(0,2,len_w)\n    w_1 = np.linspace(2,4,len_w)\n    W_0, W_1 = np.meshgrid(w_0, w_1)\n\n    # combine and reshape mesh for calculation\n    wgrid = np.array([W_0,W_1]).reshape(2, len(w_0)**2)\n\n    # calculate on grid, and reshape back for plotting\n    costs = cost(X,y,wgrid).reshape(W_0.shape)\n    \n    \n    fig = plt.figure(figsize=(15,7))\n    ax1 = fig.add_subplot(121,projection='3d')\n    ax1.set_xlabel('w_0')\n    ax1.set_ylabel('w_1')\n    ax1.set_zlabel('cost')\n    ax1.set_zlim([-2,3])\n    ax1.scatter(wPath[:,0],wPath[:,1],-2,s=10,c=colorer,cmap='inferno');\n    ax1.plot_surface(W_0, W_1, costs,cmap='inferno');\n    ax1.contour(W_0, W_1, costs, cmap='inferno', offset=-2,levels=10,alpha=0.5);\n    \n    ax2 = fig.add_subplot(122);\n    ax2.plot(cPath);\n    ax2.set_title(\"Cost during training\")\n    ax2.set_xlabel(\"Iteration\");\n\nGo back, and try rerunning gradient descent with different initial values and plot:\n\n_,wPath_gd,cPath_gd = gradient_descent(X,y,[1.9,3.9])\nplot_path(wPath_gd,cPath_gd)\n\nterminating GD after 151 iterations with weight change of 0.001.\n\n\n\n\n\n\nfrom ipywidgets import interact, interactive, fixed, interact_manual, FloatSlider\n\n# @interact(w_0=(0,2,0.1), w_1=(2,4,0.1))\ndef interactive_gd(w_0,w_1):\n    _,wPath_gd,cPath_gd = gradient_descent(X,y,[w_0,w_1])\n    plot_path(wPath_gd,cPath_gd)\n\ninteract(interactive_gd, w_0=FloatSlider(min=0,max=2,step=0.1,continous_update=False),\n                         w_1=FloatSlider(min=2,max=4,step=0.1,continous_update=False));\n\n\n\n\nOk. Now we have should have a much better grasp of Gradient Descent for Linear Regression!"
  },
  {
    "objectID": "posts/opt/opt.html#multivarite-linear-regression",
    "href": "posts/opt/opt.html#multivarite-linear-regression",
    "title": "An Introduction to Optimization",
    "section": "Multivarite Linear Regression",
    "text": "Multivarite Linear Regression\nThe best part of the formulism above, is that it now works for higher order models! For example, lets run the same analysis as above, except not we are tring to fit a 3rd-order polynomial:\n\ndef f(x):\n    return 4*x**3 + 4*x**2 + 3*x - 4\n\nx = np.linspace(-2,2,100)\n\ny = f(x) + np.random.randn(len(x))*5\n\nplt.figure(figsize=(12,7))\nplt.scatter(x,y);\nplt.plot(x,f(x),'--r',alpha=0.5);\n\n\n\n\nThe model we are trying to fit here can now be written as:\n\nh(\\mathbf{w}) = w_0 + w_1x + w_2x^2 + w_3x^3\n\nSo just like for a line, we were learning 2 parameters, for a 3-rd order polynomial we are learning 4:\n\nh(\\mathbf{w}) = \\mathbf{w}^T\\mathbf{x}\n\nFor\n\n\\mathbf{w} =\n\\begin{bmatrix}\nw_0 \\\\ w_1 \\\\ w_2 \\\\ w_3\n\\end{bmatrix},\n\\;\\;\\quad\\;\\;\n\\mathbf{x} =\n\\begin{bmatrix}\n1 \\\\ x \\\\ x^2 \\\\ x^3\n\\end{bmatrix}\n\nLets form our dataset/design matrix as above by stacking:\n\nX = np.hstack((x[:,np.newaxis]*0+1,\n               x[:,np.newaxis], \n               x[:,np.newaxis]**2, \n               x[:,np.newaxis]**3))\n\nSo now each row of X is a sample of our dataset, and each column is a dimension:\n\nprint(\"sample:\",X[1,:])\n\nsample: [ 1.         -1.95959596  3.84001632 -7.52488047]\n\n\nLets also redefine our y‚Äôs as a column vector:\n\ny = y[:,np.newaxis]\ny.shape\n\n(100, 1)\n\n\nBecause of how we defined our cost and cost_grad functions, we should be able to use them without modification!. Lets calculate the cost on a random 4 dimensional vector:\n\nw_rand = np.random.rand(4,1)*5\nw_rand\n\narray([[4.40486757],\n       [2.19318454],\n       [2.83438899],\n       [4.02340168]])\n\n\n\ncost(X,y,w_rand)\n\narray([77.98811223])\n\n\nPerfect! Unfortunately, we can‚Äôt easily visualize it, so lets instead plot the GD procedure as the polynomial we are fitting:\n\nw_gd,wPath_gd,cPath_gd = gradient_descent(X,y)\nw_gd\n\nterminating GD after max iter: 300\n\n\narray([[-3.4482133 ],\n       [ 3.50132037],\n       [ 3.62600224],\n       [ 4.17202869]])\n\n\nRecall, this weight vector now defines a polynomial!. Lets plot the polynomial given by our final weight vector:\nNote: poly1d expects the coefs in decreasing order, so we have to reverse our weight vector:\n\ncoefs = np.flip(np.squeeze(w_gd))\npoly_fin = np.poly1d(coefs)\n\n\npoly_fin\n\npoly1d([ 4.17202869,  3.62600224,  3.50132037, -3.4482133 ])\n\n\n\nfig = plt.figure(figsize=(10,7))\nax1 = fig.add_subplot(111)\nax1.scatter(x,y);\nax1.plot(x,poly_fin(x),'r');\n\n\n\n\nNot bad! Lets plot the path that GD took, in terms of our polynomials!\n\nif regenerate_anim:\n    fig = plt.figure(figsize=(10,7))\n\n    ax1 = fig.add_subplot(111)\n    ax1.scatter(x,y);\n    line, = ax1.plot([],[],'--r')\n\n    def animate(i):\n        coefs = np.flip(np.squeeze(wPath_gd[i]))\n        poly_fin = np.poly1d(coefs)\n        line.set_data(x,poly_fin(x));\n        ax1.set_title('Iteration: %d - %.2f x^3 + %.2f x^2 + %.2f x + %.2f' % (i, coefs[0], coefs[1], coefs[2], coefs[3]))\n        return line\n\n    plt.close()\n    anim = matplotlib.animation.FuncAnimation(fig, animate, frames=len(wPath_gd), interval=100)\n    # HTML(anim.to_html5_video())\n    anim.save('multi_lin_reg.mp4')\n\nVideo('multi_lin_reg.mp4',width=800)\n\n\n      Your browser does not support the video element."
  },
  {
    "objectID": "posts/opt/opt.html#implementation-1",
    "href": "posts/opt/opt.html#implementation-1",
    "title": "An Introduction to Optimization",
    "section": "Implementation",
    "text": "Implementation\nNow we are ready to describe the full SGD algorithm. Since we are estimating our gradient at each training sample, we now define a new term:\n\nDef: An epoch is a full run of a learning procedure through the entire training set.\n\nSo we can (and often must) take multiple epochs of training. The tradeoff is that each iteration is now must faster!\nNow we can describe the algorithm as:\n\nThe SGD algorithm is:\nfor each epoch:\n\nRandomly shuffle the training_set\nfor each sample in training_set\n\nw_{t+1} = w_{t} - \\eta \\; \\nabla C_i(\\mathbf{w}_t)\n\n\n\nLets implement that below\n\nA = np.arange(9).reshape(3,3)\nA\n\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\n\n\nindices = np.random.permutation(3)\nindices\n\narray([1, 2, 0])\n\n\n\nA[indices]\n\narray([[3, 4, 5],\n       [6, 7, 8],\n       [0, 1, 2]])\n\n\n\nA\n\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\n\n\ndef sgd(dataset, labels, w_init=None, eta=None, epochs=5):\n    if w_init is None:\n        w_init = np.random.rand(dataset.shape[1],1)\n    if eta is None:\n        eta = 1e-2\n        \n    # dataset\n    N = dataset.shape[0]\n    d = dataset.shape[1]\n    \n    # clean up initial point sizes:\n    w_init = np.asarray(w_init).reshape(d,1)\n    \n    # SGD params - Note their difference from GD\n    delta = 1e-10\n    max_iter = 1000\n    \n    # initialzie loop variables\n    delta_w = 1\n    iterations = 0\n\n    # save weights and costs along path\n    wPath = [w_init]\n\n    # calc cost at that initial value\n    idx = np.random.randint(N)\n    x_i = dataset[np.newaxis,idx,:].T\n    y_i = labels[idx,:]\n    cPath = [cost_sgd(x_i,y_i,w_init)]\n        \n    # take first step\n    w_old = w_init\n    \n    # epochs of training\n    for epoch in range(epochs):\n        \n        # shuffle the dataset - note we do this by shuffling indices\n        indices = np.random.permutation(N)\n        shuffled_dataset = dataset[indices,:]\n        shuffled_labels = labels[indices,:]\n        # NOTE: try commenting out this line above and seeing how it behaves\n        \n        # for each sample\n        for i in range(N):\n            # np.newaxis is there to deal with the 1 for the row size\n            curr_sample = shuffled_dataset[i,:]\n            curr_sample = curr_sample[np.newaxis].T\n            curr_label = shuffled_labels[i,:]\n            \n            # update your weights with the grad at one sample\n            grad = cost_grad_sgd(curr_sample,curr_label,w_old)\n            w_new = w_old - eta * grad\n            \n            # update paths\n            wPath.append(w_new)\n            cPath.append(cost_sgd(curr_sample,curr_label,w_new))\n            \n            # calculate weight change\n            # NOTE to me: ask a HW question here!! \n            delta_w = np.linalg.norm(w_new - w_old)\n            \n            # make your previous new weights the current weights\n            w_old = w_new\n        \n        # at end of epoch, check to see if you should terminate\n        if delta_w < delta:\n            break\n    \n    # clean up path variables\n    wPath = np.squeeze(np.asarray(wPath))\n    cPath = np.squeeze(np.asarray(cPath))\n    \n    print(f\"terminating SGD after {epoch} epochs with weight change of {delta_w:0.5f}.\")\n    \n    return w_new, wPath, cPath"
  },
  {
    "objectID": "posts/opt/opt.html#sgd-on-linear-regression",
    "href": "posts/opt/opt.html#sgd-on-linear-regression",
    "title": "An Introduction to Optimization",
    "section": "SGD on Linear Regression",
    "text": "SGD on Linear Regression\nLets re-define our simple linear regression from above and see how we do. This time, lets make a function to make it easier to setup comparisons below:\n\ndef lin_reg():\n    def f(x):\n        return 3*x + 1\n    N = 100\n    x = np.linspace(-2,2,N)\n    y = f(x) + np.random.randn(N) # our zero man gaussian noise\n    \n    X = np.ones((N,2))\n    X[:,1] = x\n    \n    y = y[:,np.newaxis]\n\n    return x,X,y,f\n\n\nx,X,y,f = lin_reg()\nplt.scatter(x,y);\n\n\n\n\n\nw_sgd, wPath_sgd, cPath_sgd = sgd(X,y,w_init=[0.75,2.5],eta=1e-2,epochs=100)\nw_sgd\n\nterminating SGD after 99 epochs with weight change of 0.03233.\n\n\narray([[1.04826202],\n       [3.13976719]])\n\n\n\nPlot\nLets plot it!\n\nw_sgd, wPath_sgd, cPath_sgd = sgd(X,y,w_init=[0.1,2],eta=1e-2,epochs=10)\nprint(w_sgd)\nplot_path(wPath_sgd,cPath_sgd,colorby='iter')\n\nterminating SGD after 9 epochs with weight change of 0.01851.\n[[1.16979153]\n [3.05990881]]\n\n\n\n\n\n\nPause-and-ponder: What do you notice about this procedure?\n\nGo back and re-run this procedure with different values for your parameters of interest!\n\n\nComparison\nLets compare both of them!\n\nw_init = [0,2]\nw_gd,wPath_gd,cPath_gd = gradient_descent(X,y,w_init=w_init)\nw_sgd, wPath_sgd, cPath_sgd = sgd(X,y,w_init=w_init,eta=1e-3/2,epochs=200)\n\nplot_path(wPath_gd,cPath_gd);\nplot_path(wPath_sgd,cPath_sgd);\n\nterminating GD after 161 iterations with weight change of 0.001.\nterminating SGD after 199 epochs with weight change of 0.00005.\n\n\n\n\n\n\n\n\nLets look at the contour plots:\n\nnum_points = 300\nw_0 = np.linspace(0.7,1.3,num_points)\nw_1 = np.linspace(2.75,3.25,num_points)\nW_0,W_1 = np.meshgrid(w_0,w_1)\nwgrid = np.array([W_0,W_1]).reshape(2, len(w_0)**2)\ncosts = cost(X,y,wgrid).reshape(W_0.shape)\n\nplt.figure(figsize=(15,7));\nax1 = plt.subplot(121)\nax1.contour(W_0, W_1, costs, cmap='inferno');\nax1.set_xlabel('w_0');\nax1.set_xlabel('w_1');\nax1.set_xlim([0.7,1.3])\nax1.set_ylim([2.75,3.25])\nax1.scatter(wPath_gd[:,0],wPath_gd[:,1], s=50,c=cPath_gd);\n\nax2 = plt.subplot(122, sharex=ax1, sharey=ax1)\nax2.contour(W_0, W_1, costs, cmap='inferno');\nax2.scatter(wPath_sgd[:,0],wPath_sgd[:,1], c=range(len(wPath_sgd)));\n\n\n\n\n\nPause-and-ponder: What do you notice?"
  },
  {
    "objectID": "posts/opt/opt.html#sgd-on-polynomial-regression",
    "href": "posts/opt/opt.html#sgd-on-polynomial-regression",
    "title": "An Introduction to Optimization",
    "section": "SGD on Polynomial Regression",
    "text": "SGD on Polynomial Regression\nAs above, lets redefine our polynomial regression problem. This time, lets make it a function to make it easier to compare below.\n\ndef poly_reg():\n    def f(x):\n        return 4*x**3 + 4*x**2 + 3*x - 4\n    x = np.linspace(-2,2,100)\n    y = f(x) + np.random.randn(len(x))*5\n    \n    X = np.hstack((x[:,np.newaxis]*0+1,\n               x[:,np.newaxis], \n               x[:,np.newaxis]**2, \n               x[:,np.newaxis]**3))\n    \n    y = y[:,np.newaxis]\n    \n    return x,X,y,f\n\n\nx,X,y,f = poly_reg()\nplt.figure(figsize=(12,7))\nplt.scatter(x,y);\nplt.plot(x,f(x),'--r',alpha=0.5);\n\n\n\n\nNow we can run them both and see!\n\nw_init = [0,0,0,0]\nw_gd,wPath_gd,cPath_gd = gradient_descent(X,y,w_init=w_init)\nw_sgd, wPath_sgd, cPath_sgd = sgd(X,y,w_init=w_init,eta=1e-2,epochs=100)\n\nw_gd,w_sgd\n\nterminating GD after max iter: 300\nterminating SGD after 99 epochs with weight change of 0.16115.\n\n\n(array([[-3.37519071],\n        [ 2.35485563],\n        [ 3.40590893],\n        [ 4.0224176 ]]),\n array([[-4.26754354],\n        [ 3.14637804],\n        [ 3.39303763],\n        [ 5.26249499]]))"
  },
  {
    "objectID": "posts/opt/opt.html#plot-it",
    "href": "posts/opt/opt.html#plot-it",
    "title": "An Introduction to Optimization",
    "section": "Plot it",
    "text": "Plot it\nBefore we plot the path, lets plot the overall costs:\n\nfig = plt.figure(figsize=(15,7))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\nax1.plot(cPath_gd);\nax2.plot(cPath_sgd);\n\n\n\n\n\nplt.figure(figsize=(15,10))\nplt.plot(cPath_sgd)\n\n\n\n\nNow we can plot the path:\n\nif regenerate_anim:\n    fig = plt.figure(figsize=(20,7))\n\n    ax1 = fig.add_subplot(121)\n    ax1.scatter(x,y);\n\n    ax2 = fig.add_subplot(122)\n    ax2.scatter(x,y);\n\n\n\n    line1, = ax1.plot([],[],'--r')\n    line2, = ax2.plot([],[],'--r')\n\n\n    num_frames = 50\n    gd_frames = np.linspace(0,len(wPath_gd)-1,num_frames).astype(int)\n    sgd_frames = np.linspace(0,len(wPath_sgd)-1,num_frames).astype(int)\n\n    # abs_cost_diff = np.abs(cPath_sgd[sgd_frames] - cPath_gd[gd_frames])\n\n    # # ax3 = fig.add_subplot(133)\n    # # ax3.plot(range(num_frames),abs_cost_diff)\n    # # scat3 = ax3.scatter([],[],s=100)\n\n    def animate(i):\n        gd_frame = gd_frames[i]\n        sgd_frame = sgd_frames[i]\n\n    #     abs_cost_diff[i] = np.abs(cPath_sgd[sgd_frame] - cPath_gd[gd_frame])\n\n        coefs_gd = np.flip(np.squeeze(wPath_gd[gd_frame]))\n        coefs_sgd = np.flip(np.squeeze(wPath_sgd[sgd_frame]))\n\n        poly_gd = np.poly1d(coefs_gd)\n        poly_sgd = np.poly1d(coefs_sgd)\n\n        line1.set_data(x,poly_gd(x));\n        ax1.set_title('GDIteration: %d - %.2f x^3 + %.2f x^2 + %.2f x + %.2f' % (i, coefs_gd[0], coefs_gd[1], coefs_gd[2], coefs_gd[3]))\n\n        line2.set_data(x,poly_sgd(x));\n        ax2.set_title('SGD Iteration: %d - %.2f x^3 + %.2f x^2 + %.2f x + %.2f' % (i, coefs_sgd[0], coefs_sgd[1], coefs_sgd[2], coefs_sgd[3]))\n\n\n        return line1, line2\n\n    plt.close()\n    anim = matplotlib.animation.FuncAnimation(fig, animate, frames=num_frames, interval=100)\n    # HTML(anim.to_html5_video())\n    anim.save('multi_lin_reg_sgd.mp4')\nVideo('multi_lin_reg_sgd.mp4',width=800)\n\n\n      Your browser does not support the video element.\n    \n\n\n\nPause-and-ponder: What do you observe? Go back and mess around with the setup and compare!\n\nNote: Ask for a plot of the ‚Äúerror-in-each-dimension‚Äù like we discussed in class\n\nfig = plt.figure(figsize=(15,7))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\nax1.plot(cPath_gd);\nax2.plot(cPath_sgd);"
  },
  {
    "objectID": "posts/opt/opt.html#implementation-2",
    "href": "posts/opt/opt.html#implementation-2",
    "title": "An Introduction to Optimization",
    "section": "Implementation",
    "text": "Implementation\nThe implementation of mini-batch GD is:\n\nThe mini-batch GD algorithm is:\nfor each epoch:\n\nseperate your data into N/batch_size-many batches\nfor each batch in batches\n\nw_{t+1} = w_{t} - \\eta \\; \\nabla C(\\mathbf{w}_t,batch)\n\n\n\nThis looks familiar! And it should! This is exactly GD, if we only had N/batch_size many examples! So the only difference in this algorithm and GD is that here, we process less examples before taking a gradient step!\n\ndef mini_batch_gd(dataset, labels, w_init=None, eta=None, batch_size=10, epochs=5, quiet=False):\n    if w_init is None:\n        w_init = np.random.rand(dataset.shape[1],1)\n    if eta is None:\n        eta = 1e-2\n        \n    # dataset\n    N = dataset.shape[0]\n    d = dataset.shape[1]\n\n    # calculat our batches\n    num_batches = int(N/batch_size)\n    \n    # clean up initial point sizes:\n    w_init = np.asarray(w_init).reshape(d,1)\n    \n    # mini-batch params \n    delta = 1e-5\n    delta_w = 1\n    iterations = 0\n\n    # save weights and costs along path\n    wPath_mbgd = []\n    cPath_mbgd = []\n    cPath_mbgd_epoch = []\n    \n    # take the first step\n    w_curr = w_init\n    \n    # loop through all epochs\n    for epoch in range(epochs):\n        \n        # loop through all batches\n        for batch_num in range(num_batches):\n\n            # get current batch\n            batch_start = batch_num*batch_size\n            batch_end = (batch_num+1)*batch_size\n            batch = dataset[batch_start:batch_end,:]\n            batch_labels = labels[batch_start:batch_end]\n\n            # run gradient descent on the batch! \n            w_curr,_,_ = gradient_descent(batch, batch_labels, w_init=w_curr, eta=eta, max_iter=1,quiet=True)\n \n            \n            wPath_mbgd.append(w_curr)\n            cPath_mbgd.append(cost(batch,batch_labels,w_curr))\n            \n            #if not quiet: print(f\"Batch:{batch_num+1}/{num_batches}, Loss:{cPath_mbgd[-1]}\")\n        \n        \n        if not quiet: print(f\"Epoch:{epoch+1}/{epochs}, Loss:{cPath_mbgd[-1]}\")\n    \n    # clean up path variables\n    wPath_mbgd = np.squeeze(np.asarray(wPath_mbgd))\n    cPath_mbgd = np.squeeze(np.asarray(cPath_mbgd))\n    \n    return w_curr, wPath_mbgd, cPath_mbgd"
  },
  {
    "objectID": "posts/opt/opt.html#mbgd-on-linear-regression",
    "href": "posts/opt/opt.html#mbgd-on-linear-regression",
    "title": "An Introduction to Optimization",
    "section": "MBGD on Linear Regression",
    "text": "MBGD on Linear Regression\nLets use the setup function we defined above:\n\nx,X,y,f = lin_reg()\nplt.scatter(x,y);\n\n\n\n\n\nw_gd, wPath_gd, cPath_gd = gradient_descent(X,y,quiet=True)\nw_sgd, wPath_sgd, cPath_sgd = sgd(X,y)\nw_mbgd, wPath_mbgd, cPath_mbgd = mini_batch_gd(X,y,epochs=100,quiet=True,batch_size=1)\n\nterminating SGD after 4 epochs with weight change of 0.01009."
  },
  {
    "objectID": "posts/opt/opt.html#plot-it-1",
    "href": "posts/opt/opt.html#plot-it-1",
    "title": "An Introduction to Optimization",
    "section": "Plot it",
    "text": "Plot it\nLets take a look!\n\nfig = plt.figure(figsize=(20,7))\n\nax1 = fig.add_subplot(131)\nax1.plot(cPath_gd);\nax1.set_title(\"Loss by iteration for GD\")\nax1.set_xlabel(\"iteration\");\n\n\nax2 = fig.add_subplot(132)\nax2.plot(cPath_sgd);\nax2.set_title('Loss by iteration for SGD');\nax2.set_xlabel(\"iteration\");\nax2.set_ylabel(\"loss\");\n\nax3 = fig.add_subplot(133)\nax3.plot(cPath_mbgd);\nax3.set_title('Loss by iteration for MBGD');\nax3.set_xlabel(\"iteration\");\nax3.set_ylabel(\"loss\");\n\n\n\n\n\nfig = plt.figure(figsize=(15,7))\n\nax1 = fig.add_subplot(121)\nax1.plot(cPath_gd);\nax1.set_title(\"Loss by iteration for GD\")\nax1.set_xlabel(\"iteration\");\n\n\nax2 = fig.add_subplot(122)\nax2.plot(cPath_mbgd[0::10]);\nax2.set_title('Loss by EPOCH for MBGD');\nax2.set_xlabel(\"EPOCH\");\nax2.set_ylabel(\"loss\");\n\n\n\n\nLets take a look at the paths!\n\nnum_points = 300\nw_0 = np.linspace(0.7,1.3,num_points)\nw_1 = np.linspace(2.75,3.25,num_points)\nW_0,W_1 = np.meshgrid(w_0,w_1)\nwgrid = np.array([W_0,W_1]).reshape(2, len(w_0)**2)\ncosts = cost(X,y,wgrid).reshape(W_0.shape)\n\nplt.figure(figsize=(20,7));\n\n# Gradient Descent\nax1 = plt.subplot(131)\nax1.contour(W_0, W_1, costs, cmap='inferno');\nax1.set_xlabel('w_0');\nax1.set_xlabel('w_1');\nax1.set_xlim([0.7,1.3])\nax1.set_ylim([2.75,3.25])\nax1.scatter(wPath_gd[:,0],wPath_gd[:,1], s=50,c=cPath_gd);\nax1.set_title(\"GD\");\n\n# SGD\nax2 = plt.subplot(132, sharex=ax1, sharey=ax1)\nax2.contour(W_0, W_1, costs, cmap='inferno');\nax2.scatter(wPath_sgd[:,0],wPath_sgd[:,1], c=range(len(wPath_sgd)));\nax2.set_title(\"SGD\");\n\n# MBGD\nax3 = plt.subplot(133, sharex=ax1, sharey=ax1)\nax3.contour(W_0, W_1, costs, cmap='inferno');\nax3.scatter(wPath_mbgd[:,0],wPath_mbgd[:,1], c=range(len(wPath_mbgd)));\nax3.set_title(\"Mini-batch GD\");\n\n\n\n\n\nPause-and-ponder: What do you notice about the graphs above? What can you say about these graphs?"
  },
  {
    "objectID": "posts/opt/opt.html#mbgd-on-polynomial-regression",
    "href": "posts/opt/opt.html#mbgd-on-polynomial-regression",
    "title": "An Introduction to Optimization",
    "section": "MBGD on Polynomial Regression",
    "text": "MBGD on Polynomial Regression\nLets run this on the polynomial problem\n\nx,X,y,f = poly_reg()\nplt.figure(figsize=(12,7))\nplt.scatter(x,y);\nplt.plot(x,f(x),'--r',alpha=0.5);\n\n\n\n\n\nw_gd, wPath_gd, cPath_gd = gradient_descent(X,y,quiet=True)\nw_sgd, wPath_sgd, cPath_sgd = sgd(X,y)\nw_mbgd, wPath_mbgd, cPath_mbgd = mini_batch_gd(X,y,epochs=100,quiet=True)\n\nterminating SGD after 4 epochs with weight change of 0.01274."
  },
  {
    "objectID": "posts/opt/opt.html#plot-it-2",
    "href": "posts/opt/opt.html#plot-it-2",
    "title": "An Introduction to Optimization",
    "section": "Plot it!",
    "text": "Plot it!\n\nfig = plt.figure(figsize=(20,7))\n\nax1 = fig.add_subplot(131)\nax1.plot(cPath_gd);\nax1.set_title(\"Loss by iteration for GD\")\nax1.set_xlabel(\"iteration\");\n\n\nax2 = fig.add_subplot(132)\nax2.plot(cPath_sgd);\nax2.set_title('Loss by iteration for SGD');\nax2.set_xlabel(\"iteration\");\nax2.set_ylabel(\"loss\");\n\nax3 = fig.add_subplot(133)\nax3.plot(cPath_mbgd[::10]);\nax3.set_title('Loss by iteration for MBGD');\nax3.set_xlabel(\"iteration\");\nax3.set_ylabel(\"loss\");\n\n\n\n\nNow lets look at by epoch for MBSGD\n\nfig = plt.figure(figsize=(20,7))\n\nax1 = fig.add_subplot(131)\nax1.plot(cPath_gd);\nax1.set_title(\"Loss by iteration for GD\")\nax1.set_xlabel(\"iteration\");\n\n\nax2 = fig.add_subplot(132)\nax2.plot(cPath_sgd);\nax2.set_title('Loss by iteration for SGD');\nax2.set_xlabel(\"iteration\");\nax2.set_ylabel(\"loss\");\n\nax3 = fig.add_subplot(133)\nax3.plot(cPath_mbgd);\nax3.set_title('Loss by EPOCH for MBGD');\nax3.set_xlabel(\"EPOCH\");\nax3.set_ylabel(\"loss\");\n\n\n\n\n\nax3 = fig.add_subplot(133)\nax3.plot(cPath_mbgd);\nax3.set_title('Loss by EPOCH for MBGD');\nax3.set_xlabel(\"EPOCH\");\nax3.set_ylabel(\"loss\");\n\nHow about those paths!\n\nif regenerate_anim:\n    fig = plt.figure(figsize=(30,7))\n\n    ax1 = fig.add_subplot(131)\n    ax1.scatter(x,y);\n\n    ax2 = fig.add_subplot(132)\n    ax2.scatter(x,y);\n\n    ax3 = fig.add_subplot(133)\n    ax3.scatter(x,y);\n\n\n\n    line1, = ax1.plot([],[],'--r')\n    line2, = ax2.plot([],[],'--r')\n    line3, = ax3.plot([],[],'--r')\n\n\n    num_frames = 50\n    gd_frames = np.linspace(0,len(wPath_gd)-1,num_frames).astype(int)\n    sgd_frames = np.linspace(0,len(wPath_sgd)-1,num_frames).astype(int)\n    mbgd_frames = np.linspace(0,len(wPath_mbgd)-1,num_frames).astype(int)\n\n\n    def animate(i):\n        gd_frame = gd_frames[i]\n        sgd_frame = sgd_frames[i]\n        mbgd_frame = sgd_frames[i]\n\n\n        coefs_gd = np.flip(np.squeeze(wPath_gd[gd_frame]))\n        coefs_sgd = np.flip(np.squeeze(wPath_sgd[sgd_frame]))\n        coefs_mbgd = np.flip(np.squeeze(wPath_mbgd[mbgd_frame]))\n\n        poly_gd = np.poly1d(coefs_gd)\n        poly_sgd = np.poly1d(coefs_sgd)\n        poly_mbgd = np.poly1d(coefs_mbgd)\n\n        line1.set_data(x,poly_gd(x));\n        ax1.set_title('GDIteration: %d - %.2f x^3 + %.2f x^2 + %.2f x + %.2f' % (i, coefs_gd[0], coefs_gd[1], coefs_gd[2], coefs_gd[3]))\n\n        line2.set_data(x,poly_sgd(x));\n        ax2.set_title('SGD Iteration: %d - %.2f x^3 + %.2f x^2 + %.2f x + %.2f' % (i, coefs_sgd[0], coefs_sgd[1], coefs_sgd[2], coefs_sgd[3]))\n\n        line3.set_data(x,poly_mbgd(x));\n        ax3.set_title('MBGD Iteration: %d - %.2f x^3 + %.2f x^2 + %.2f x + %.2f' % (i, coefs_mbgd[0], coefs_mbgd[1], coefs_mbgd[2], coefs_mbgd[3]))\n\n        return line1, line2, line3\n\n    plt.close()\n    anim = matplotlib.animation.FuncAnimation(fig, animate, frames=num_frames, interval=100)\n    # HTML(anim.to_html5_video())\n    anim.save('multi_lin_reg_mbgd.mp4')\n    \nVideo('multi_lin_reg_mbgd.mp4',width=800)\n\n\n      Your browser does not support the video element.\n    \n\n\n\nPause-and-ponder: What do you notice about these graphs? Reflect on it and answer here!"
  },
  {
    "objectID": "posts/opt/opt.html#implementation-3",
    "href": "posts/opt/opt.html#implementation-3",
    "title": "An Introduction to Optimization",
    "section": "Implementation",
    "text": "Implementation\n\ndef sgd_momentum(dataset, labels, w_init=None, eta=None, beta=None, epochs=5):\n    if w_init is None:\n        w_init = np.random.rand(dataset.shape[1],1)\n    if eta is None:\n        eta = 1e-2\n    if beta is None:\n        beta = 0.1\n        \n    # dataset\n    N = dataset.shape[0]\n    d = dataset.shape[1]\n    \n    # clean up initial point sizes:\n    w_init = np.asarray(w_init).reshape(d,1)\n    \n    # M-SGD params\n    delta = 1e-10\n    delta_w = 1\n    max_iter = 1000\n    iterations = 0\n\n    # save weights and costs along path\n    wPath = [w_init]\n\n    # calc cost at that initial value\n    idx = np.random.randint(N)\n    x_i = dataset[np.newaxis,idx,:].T\n    y_i = labels[idx,:]\n    cPath = [cost_sgd(x_i,y_i,w_init)]\n        \n    # take first step\n    w_old = w_init\n    z_old = 0\n    \n    # epochs of training\n    for epoch in range(epochs):\n        \n        # shuffle the dataset - note we do this by shuffling indices\n        indices = np.random.permutation(N)\n        shuffled_dataset = dataset[indices,:]\n        shuffled_labels = labels[indices,:]\n        \n        for i in range(N):\n            # np.newaxis is there to deal with the 1 for the row size\n            curr_sample = shuffled_dataset[i,:]\n            curr_sample = curr_sample[np.newaxis].T\n            curr_label = shuffled_labels[i,:]\n            \n            # update your weights with the grad at one sample\n            grad = cost_grad_sgd(curr_sample,curr_label,w_old)\n            z_new = beta*z_old + grad\n            w_new = w_old - eta * z_new\n            \n            # update paths\n            wPath.append(w_new)\n            cPath.append(cost_sgd(curr_sample,curr_label,w_new))\n            \n            # calculate weight change\n            delta_w = np.linalg.norm(w_new - w_old)\n            \n            # make your previous new weights the current weights\n            w_old = w_new\n            z_old = z_new\n        \n        # at end of epoch, check to see if you should terminate\n        if delta_w < delta:\n            break\n    \n    # clean up path variables\n    wPath = np.squeeze(np.asarray(wPath))\n    cPath = np.squeeze(np.asarray(cPath))\n    \n    print(f\"terminating M-SGD after {epoch} epochs with weight change of {delta_w:0.5f}.\")\n    \n    return w_new, wPath, cPath"
  },
  {
    "objectID": "posts/opt/opt.html#m-sgd-on-linear-regression",
    "href": "posts/opt/opt.html#m-sgd-on-linear-regression",
    "title": "An Introduction to Optimization",
    "section": "M-SGD on Linear Regression",
    "text": "M-SGD on Linear Regression\n\ndef lin_reg():\n    def f(x):\n        return 3*x + 1\n    N = 100\n    x = np.linspace(-2,2,N)\n    y = f(x) + np.random.randn(N) # our zero man gaussian noise\n    \n    X = np.ones((N,2))\n    X[:,1] = x\n    \n    y = y[:,np.newaxis]\n\n    return x,X,y,f\n\n\nx,X,y,f = lin_reg()\nplt.scatter(x,y);\n\n\n\n\n\nw_sgd, wPath_sgd, cPath_sgd = sgd(X,y,w_init=[0.75,2.5],eta=1e-2,epochs=100)\nw_sgd_m, wPath_sgd_m, cPath_sgd_m = sgd_momentum(X,y,w_init=[0.75,2.5],eta=1e-5,epochs=100,beta=0.9)\n\nterminating SGD after 99 epochs with weight change of 0.00242.\nterminating M-SGD after 99 epochs with weight change of 0.00006."
  },
  {
    "objectID": "posts/opt/opt.html#plot-it-3",
    "href": "posts/opt/opt.html#plot-it-3",
    "title": "An Introduction to Optimization",
    "section": "Plot it!",
    "text": "Plot it!\n\nplot_path(wPath_sgd_m,cPath_sgd_m,colorby='iter')"
  },
  {
    "objectID": "posts/opt/opt.html#comparison-1",
    "href": "posts/opt/opt.html#comparison-1",
    "title": "An Introduction to Optimization",
    "section": "Comparison",
    "text": "Comparison\n\nw_init=[0,2]\neta=1e-3\nbeta=0.9\n\nw_sgd, wPath_sgd, cPath_sgd = sgd(X,y,w_init=w_init,eta=eta,epochs=100)\nw_sgd_m, wPath_sgd_m, cPath_sgd_m = sgd_momentum(X,y,w_init=w_init,eta=eta,beta=beta,epochs=100)\n\nplot_path(wPath_sgd,cPath_sgd);\nplot_path(wPath_sgd_m,cPath_sgd_m);\n\nterminating SGD after 99 epochs with weight change of 0.00094.\nterminating M-SGD after 99 epochs with weight change of 0.00831."
  },
  {
    "objectID": "posts/opt/opt.html#plot-paths",
    "href": "posts/opt/opt.html#plot-paths",
    "title": "An Introduction to Optimization",
    "section": "Plot Paths!",
    "text": "Plot Paths!\n\nw_init=[0,2]\neta=1e-4\nbeta=0.99\n\nw_sgd, wPath_sgd, cPath_sgd = sgd(X,y,w_init=w_init,eta=eta,epochs=100)\nw_sgd_m, wPath_sgd_m, cPath_sgd_m = sgd_momentum(X,y,w_init=w_init,eta=eta,beta=beta,epochs=100)\n\nterminating SGD after 99 epochs with weight change of 0.00001.\nterminating M-SGD after 99 epochs with weight change of 0.00168.\n\n\n\n# plot\nnum_points = 300\n\nx_l = 0.5\nx_r = 1.5\ny_l = 2.5\ny_r = 3.5\n\nw_0 = np.linspace(x_l,x_r,num_points)\nw_1 = np.linspace(y_l,y_r,num_points)\nW_0,W_1 = np.meshgrid(w_0,w_1)\nwgrid = np.array([W_0,W_1]).reshape(2, len(w_0)**2)\ncosts = cost(X,y,wgrid).reshape(W_0.shape)\n\nplt.figure(figsize=(20,7));\n\n# SGD\nax1 = plt.subplot(121)\nax1.contour(W_0, W_1, costs, cmap='inferno');\nax1.set_xlabel('w_0');\nax1.set_xlabel('w_1');\nax1.set_xlim([x_l,x_r])\nax1.set_ylim([y_l,y_r])\nax1.scatter(wPath_sgd[:,0],wPath_sgd[:,1], s=50,c=cPath_sgd);\nax1.set_title(\"SGD\");\n\n# MSGD\nax2 = plt.subplot(122, sharex=ax1, sharey=ax1)\nax2.contour(W_0, W_1, costs, cmap='inferno');\nax2.scatter(wPath_sgd_m[:,0],wPath_sgd_m[:,1], c=range(len(wPath_sgd_m)));\nax2.set_title(\"M-SGD\");\n\n\n\n\n\nPause-and-ponder: What do you notice? Go back and change the \\eta and \\beta parameters above and jot your observations down here!"
  },
  {
    "objectID": "posts/opt/opt.html#gd-momentum",
    "href": "posts/opt/opt.html#gd-momentum",
    "title": "An Introduction to Optimization",
    "section": "GD + Momentum",
    "text": "GD + Momentum\n\nSee-and-do: Go back, and add some momentum to our stock GD implementation, and make your comparisons!\n\n\ndef gd_momentum():\n    # EDIT HERE!\n    return"
  },
  {
    "objectID": "posts/opt/opt.html#mbgd-momentum",
    "href": "posts/opt/opt.html#mbgd-momentum",
    "title": "An Introduction to Optimization",
    "section": "MBGD + Momentum",
    "text": "MBGD + Momentum\n\nSee-and-do: Go back, and add some momentum to our Mini-batch GD implementation, and make your comparisons!\n\n\ndef mbgd_momentum():\n    # EDIT HERE!\n    return"
  },
  {
    "objectID": "posts/opt/opt.html#implementation-4",
    "href": "posts/opt/opt.html#implementation-4",
    "title": "An Introduction to Optimization",
    "section": "Implementation",
    "text": "Implementation\n\ndef mini_batch_rmsprop(dataset, labels, w_init=None, eta=None, gamma=None, batch_size=10, epochs=5, quiet=False):\n    if w_init is None:\n        w_init = np.random.rand(dataset.shape[1],1)\n    if eta is None:\n        eta = 1e-2\n    if gamma is None:\n        gamma = 0.9\n        \n    # dataset\n    N = dataset.shape[0]\n    d = dataset.shape[1]\n\n    num_batches = int(N/batch_size)\n    \n    # clean up initial point sizes:\n    w_init = np.asarray(w_init).reshape(d,1)\n    \n    # mini-batch params \n    delta = 1e-5\n    delta_w = 1\n    iterations = 0\n\n    # save weights and costs along path\n    wPath_rms = []\n    cPath_rms = []\n    \n    # take the first step\n    w_old = w_init\n    v_old = 0\n    \n    for epoch in range(epochs):\n        \n        for batch_num in range(num_batches):\n\n            # get current batch\n            batch_start = batch_num*batch_size\n            batch_end = (batch_num+1)*batch_size\n            batch = dataset[batch_start:batch_end,:]\n            batch_labels = labels[batch_start:batch_end]\n\n            # update your weights\n            grad = cost_grad(batch,batch_labels,w_old)\n            \n            v_new = gamma*v_old + (1-gamma)*grad**2\n            w_new = w_old - (eta/np.sqrt(v_new + np.random.rand()))*grad \n \n\n            # update old weights\n            w_old = w_new\n            v_old = v_new\n            \n            # append trajectory\n            wPath_rms.append(w_new)\n            cPath_rms.append(cost(batch,batch_labels,w_new))\n            \n    \n        if not quiet: print(f\"Epoch:{epoch+1}/{epochs}, Loss:{cPath_rms[-1]}\")\n    \n    # clean up path variables\n    wPath_rms = np.squeeze(np.asarray(wPath_rms))\n    cPath_rms = np.squeeze(np.asarray(cPath_rms))\n    \n    return w_new, wPath_rms, cPath_rms"
  },
  {
    "objectID": "posts/opt/opt.html#plot-it-4",
    "href": "posts/opt/opt.html#plot-it-4",
    "title": "An Introduction to Optimization",
    "section": "Plot it!",
    "text": "Plot it!\n\nx,X,y,f = lin_reg()\nplt.scatter(x,y);\n\n\n\n\n\ngamma = 0.8\nw_sgd, wPath_sgd, cPath_sgd = sgd(X,y,w_init=[0.75,2.5],eta=1e-2,epochs=100)\nw_rms, wPath_rms, cPath_rms = mini_batch_rmsprop(X,y,w_init=[0.75,2.5],eta=1e-2,gamma=gamma,epochs=100,quiet=True)\n\nterminating SGD after 99 epochs with weight change of 0.01311.\n\n\n\nw_init=[0,2]\neta=1e-2\ngamma=0.8\n\nw_mbgd, wPath_mbgd, cPath_mbgd = mini_batch_gd(X,y,epochs=100,quiet=True)\nw_sgd, wPath_sgd, cPath_sgd = sgd(X,y,w_init=w_init,eta=eta,epochs=100)\nw_rms, wPath_rms, cPath_rms = mini_batch_rmsprop(X,y,w_init=w_init,eta=eta,gamma=gamma,epochs=100,quiet=True)\n\nplot_path(wPath_mbgd,cPath_mbgd);\nplot_path(wPath_sgd,cPath_sgd);\nplot_path(wPath_rms,cPath_rms);\n\nterminating SGD after 99 epochs with weight change of 0.04209."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Diego A. Mesa, PhD",
    "section": "",
    "text": "I‚Äôm currently a computer science professor at Vanderbilt University, with a passion for Deep Learning and its many applications.\nI believe in writing as an aid to thinking, and writing as you speak. I believe in slowing down, and that a thing worth doing, is worth doing badly.\nI believe in the pervasiveness of research debt, and in the invaluable role of research distillers.\nPlease check out a working draft of an interactive Machine Learning with Python+Numpy textbook I am working on at my posts page."
  },
  {
    "objectID": "index.html#technical-interests",
    "href": "index.html#technical-interests",
    "title": "Diego A. Mesa, PhD",
    "section": "Technical Interests",
    "text": "Technical Interests\nDeep Learning, Information Theory, Theoretical Machine Learning, High Dimensional Bayesian Inference, Optimization, Explainable/Interpretable AI"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Diego A. Mesa, PhD",
    "section": "Experience",
    "text": "Experience\nAssistant Professor of the Practice of Computer Science || Vanderbilt University  January 2021 ‚Äì Present\nPostdoctoral Fellow || Vanderbilt University  Computer Science and Biomedical Informatics  January 2018 ‚Äì December 2019\nDoctoral Researcher || University of California, San Diego  August 2011 ‚Äì June 2017"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Diego A. Mesa, PhD",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego || San Diego, CA  PhD in Bioengineering || Aug.¬†2011 - June 2017  Emphasis on high-dimensional Bayesian inference, optimal sequential communication and information theory\nUniversity of Florida || Gainesville, FL  BS || Aug.¬†2006 - Dec.¬†2010"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Diego A. Mesa",
    "section": "",
    "text": "All publications are available on my Google Scholar. Instead, this page gives a working draft of an interactive Machine Learning with Python+Numpy textbook I am working on.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nA Bottom-Up Introduction to Neural Networks\n\n\nFrom 1-D Forward propagation to vectorized Backprop and everything in between\n\n\n\n\ntrue\n\n\nneural networks\n\n\nlinear algebra\n\n\nnumpy\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to Optimization\n\n\nFrom Gradients, to Stochastic Gradient Descent with Momentum and ADAM and everything inbetween!\n\n\n\n\ntrue\n\n\noptimization\n\n\nnumpy\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to Markov Chains, MCMC and The Gibbs Sampler\n\n\nFrom Probability to Markov Chain Monte Carlo and the Gibbs Sampler!\n\n\n\n\ntrue\n\n\nprobability\n\n\nmcmc\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\nNo matching items"
  }
]