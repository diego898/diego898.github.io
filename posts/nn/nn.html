<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Diego Mesa">
<meta name="dcterms.date" content="2023-02-01">

<title>Diego A. Mesa - A Bottom-Up Introduction to Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Diego A. Mesa</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">about</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html">
 <span class="menu-text">posts</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../diego.mesa@vanderbilt.edu"><i class="bi bi-envelope-fill" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/diego898"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/diego-a-mesa/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/diego898"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://scholar.google.com/citations?user=8SnAElkAAAAJ&amp;hl=en"><i class="bi bi-stack" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">A Bottom-Up Introduction to Neural Networks</h1>
            <p class="subtitle lead">From 1-D Forward propagation to vectorized Backprop and everything inbetween</p>
                                <div class="quarto-categories">
                <div class="quarto-category">neural networks</div>
                <div class="quarto-category">linear algebra</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Diego Mesa </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 1, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#nn-walkthrough" id="toc-nn-walkthrough" class="nav-link active" data-scroll-target="#nn-walkthrough">NN Walkthrough</a></li>
  <li><a href="#forward-direction" id="toc-forward-direction" class="nav-link" data-scroll-target="#forward-direction">Forward Direction</a>
  <ul class="collapse">
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a></li>
  <li><a href="#d-network" id="toc-d-network" class="nav-link" data-scroll-target="#d-network">1-D Network</a>
  <ul class="collapse">
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a></li>
  </ul></li>
  <li><a href="#multidimensional-network" id="toc-multidimensional-network" class="nav-link" data-scroll-target="#multidimensional-network">Multidimensional Network</a>
  <ul class="collapse">
  <li><a href="#notation" id="toc-notation" class="nav-link" data-scroll-target="#notation">Notation</a></li>
  <li><a href="#implementation-1" id="toc-implementation-1" class="nav-link" data-scroll-target="#implementation-1">Implementation</a></li>
  </ul></li>
  <li><a href="#mini-batch-notation" id="toc-mini-batch-notation" class="nav-link" data-scroll-target="#mini-batch-notation">Mini-Batch Notation</a></li>
  <li><a href="#dealing-with-biases" id="toc-dealing-with-biases" class="nav-link" data-scroll-target="#dealing-with-biases">Dealing with Biases</a></li>
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions">Activation Functions</a>
  <ul class="collapse">
  <li><a href="#linear" id="toc-linear" class="nav-link" data-scroll-target="#linear">Linear</a></li>
  <li><a href="#a-final-sigmoid" id="toc-a-final-sigmoid" class="nav-link" data-scroll-target="#a-final-sigmoid">A Final Sigmoid</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#backprop" id="toc-backprop" class="nav-link" data-scroll-target="#backprop">Backprop</a>
  <ul class="collapse">
  <li><a href="#latex-definitions" id="toc-latex-definitions" class="nav-link" data-scroll-target="#latex-definitions">Latex definitions</a></li>
  <li><a href="#d-case" id="toc-d-case" class="nav-link" data-scroll-target="#d-case">1-D Case</a>
  <ul class="collapse">
  <li><a href="#simulation" id="toc-simulation" class="nav-link" data-scroll-target="#simulation">Simulation</a></li>
  <li><a href="#nudge-colorbluewl" id="toc-nudge-colorbluewl" class="nav-link" data-scroll-target="#nudge-colorbluewl">Nudge <span class="math inline">\color{blue}{w^L}</span></a></li>
  <li><a href="#nudge-colorbluew_1" id="toc-nudge-colorbluew_1" class="nav-link" data-scroll-target="#nudge-colorbluew_1">Nudge <span class="math inline">\color{blue}{w_1}</span></a></li>
  <li><a href="#we-could-do-better" id="toc-we-could-do-better" class="nav-link" data-scroll-target="#we-could-do-better">We could do better</a></li>
  <li><a href="#lets-keep-going" id="toc-lets-keep-going" class="nav-link" data-scroll-target="#lets-keep-going">Lets keep going!</a></li>
  <li><a href="#speaking-of-error" id="toc-speaking-of-error" class="nav-link" data-scroll-target="#speaking-of-error">Speaking of Error‚Ä¶</a></li>
  <li><a href="#what-about-the-rest" id="toc-what-about-the-rest" class="nav-link" data-scroll-target="#what-about-the-rest">What about the rest?</a></li>
  <li><a href="#fully-recursive-definition" id="toc-fully-recursive-definition" class="nav-link" data-scroll-target="#fully-recursive-definition">Fully recursive definition</a></li>
  <li><a href="#but-wait" id="toc-but-wait" class="nav-link" data-scroll-target="#but-wait">But wait‚Ä¶</a></li>
  <li><a href="#the-four-fundamental-backpropagation-equations" id="toc-the-four-fundamental-backpropagation-equations" class="nav-link" data-scroll-target="#the-four-fundamental-backpropagation-equations">The Four Fundamental Backpropagation Equations‚Ñ¢</a></li>
  <li><a href="#implementation-2" id="toc-implementation-2" class="nav-link" data-scroll-target="#implementation-2">Implementation</a></li>
  </ul></li>
  <li><a href="#multidimensional-case" id="toc-multidimensional-case" class="nav-link" data-scroll-target="#multidimensional-case">Multidimensional case</a>
  <ul class="collapse">
  <li><a href="#latex-definitions-1" id="toc-latex-definitions-1" class="nav-link" data-scroll-target="#latex-definitions-1">Latex definitions</a></li>
  <li><a href="#notation-1" id="toc-notation-1" class="nav-link" data-scroll-target="#notation-1">Notation</a></li>
  <li><a href="#implementation-3" id="toc-implementation-3" class="nav-link" data-scroll-target="#implementation-3">Implementation</a></li>
  </ul></li>
  <li><a href="#n-d-revisited---vector-notation" id="toc-n-d-revisited---vector-notation" class="nav-link" data-scroll-target="#n-d-revisited---vector-notation">N-D Revisited - Vector Notation</a>
  <ul class="collapse">
  <li><a href="#f.f.b.e---vectorized" id="toc-f.f.b.e---vectorized" class="nav-link" data-scroll-target="#f.f.b.e---vectorized">F.F.B.E - Vectorized‚Ñ¢</a></li>
  <li><a href="#implementation-4" id="toc-implementation-4" class="nav-link" data-scroll-target="#implementation-4">Implementation</a></li>
  </ul></li>
  <li><a href="#more-interesting-example" id="toc-more-interesting-example" class="nav-link" data-scroll-target="#more-interesting-example">More interesting example</a>
  <ul class="collapse">
  <li><a href="#train-our-network" id="toc-train-our-network" class="nav-link" data-scroll-target="#train-our-network">Train our Network!</a></li>
  </ul></li>
  <li><a href="#gradient-checking" id="toc-gradient-checking" class="nav-link" data-scroll-target="#gradient-checking">Gradient Checking</a>
  <ul class="collapse">
  <li><a href="#numerical-gradients" id="toc-numerical-gradients" class="nav-link" data-scroll-target="#numerical-gradients">Numerical Gradients!</a></li>
  <li><a href="#implementation-5" id="toc-implementation-5" class="nav-link" data-scroll-target="#implementation-5">Implementation</a></li>
  </ul></li>
  <li><a href="#compare-to-nielsen-implementation" id="toc-compare-to-nielsen-implementation" class="nav-link" data-scroll-target="#compare-to-nielsen-implementation">Compare to Nielsen implementation:</a></li>
  <li><a href="#fully-vectorized-implementation" id="toc-fully-vectorized-implementation" class="nav-link" data-scroll-target="#fully-vectorized-implementation">Fully vectorized implementation</a>
  <ul class="collapse">
  <li><a href="#implementation-6" id="toc-implementation-6" class="nav-link" data-scroll-target="#implementation-6">Implementation</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources">Additional Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="nn-walkthrough" class="level1">
<h1>NN Walkthrough</h1>
<p>In this notebook we will walk through the forward and backward direction of neural networks.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.datasets</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> cm</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># for creating animations</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.animation</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># styling additions</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># style = "&lt;style&gt;div.warn{background-color: #fcf2f2;border-color: #dFb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;}&lt;/style&gt;"</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>style <span class="op">=</span> <span class="st">"&lt;style&gt;div.info{padding: 15px; border: 1px solid transparent; border-left: 5px solid #dfb5b4; border-color: transparent; margin-bottom: 10px; border-radius: 4px; background-color: #fcf8e3; border-color: #faebcc;}&lt;/style&gt;"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>HTML(style)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<style>div.info{padding: 15px; border: 1px solid transparent; border-left: 5px solid #dfb5b4; border-color: transparent; margin-bottom: 10px; border-radius: 4px; background-color: #fcf8e3; border-color: #faebcc;}</style>
</div>
</div>
</section>
<section id="forward-direction" class="level1">
<h1>Forward Direction</h1>
<p>Lets start with the simplest network we can imagine, and first understand how a neural network calculates its output for a particular input. This is known as the ‚Äúforward‚Äù direction (we will see why later).</p>
<section id="dataset" class="level2">
<h2 class="anchored" data-anchor-id="dataset">Dataset</h2>
<p>Lets generate a simple 1-D toy dataset:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> toy():</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,N).reshape(N,<span class="dv">1</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> ((inputs<span class="op">&gt;</span><span class="dv">0</span>).astype(<span class="bu">int</span>)<span class="op">*</span><span class="dv">2</span><span class="op">-</span><span class="dv">1</span>).reshape(N,<span class="dv">1</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> inputs,labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>inputs,labels <span class="op">=</span> toy()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>inputs[<span class="dv">0</span>:<span class="dv">5</span>],labels[<span class="dv">0</span>:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(array([[-5.        ],
        [-4.98998999],
        [-4.97997998],
        [-4.96996997],
        [-4.95995996]]),
 array([[-1],
        [-1],
        [-1],
        [-1],
        [-1]]))</code></pre>
</div>
</div>
</section>
<section id="d-network" class="level2">
<h2 class="anchored" data-anchor-id="d-network">1-D Network</h2>
<p>Lets give the simplest network we can imagine. One that consists of a few 1-D ‚Äúlayers‚Äù of size 1! We have an: * input node/layer, * hidden node/layer:<span class="math inline">h_1</span>, * another hidden node/layer <span class="math inline">h_2</span> * output node/layer.</p>
<p>Networks are typically drawn with the <strong>weights</strong> on the wires. Our simple network can be given as: <img src="figures/simple_network.png" class="img-fluid"></p>
<div class="info">
<p><strong>Note:</strong> This is sometimes confusing, as technically its the <strong>activations</strong> of the previous layer that ‚Äúflow‚Äù into the next layer. However, the weights are what we are tying to <strong>learn</strong>, and their relative <strong>strengths</strong> tells us something about the structure of the network.</p>
</div>
<p>Each node has an associated: * weight * bias * activation function</p>
<p>In our example, we have <span class="math inline">w_1,b_1,w_2,b_2,w_3,b_3</span> as our <strong>parameters</strong> and we are using the <strong>sigmoid</strong> as our activation function.</p>
<p>The function of each node, is to apply its own weight and bias to a previous layers activation value, and then pass it through its activation function to produce its own activation. For example, <span class="math inline">h_1</span> is doing:</p>
<p><span class="math display">
a_1 = \sigma(w\cdot x + b)
</span></p>
<p>The <em>input</em> to a nodes activation function is useful to think about separately, so we can introduce an additional variable to denote it as: <span class="math display">
z_i = w\cdot x + b, \quad\quad a_1 = \sigma(z_i)
</span></p>
<p>So, we can describe the behavior of each of our nodes as:</p>
<p><img src="figures/nodes.png" class="img-fluid"></p>
<section id="implementation" class="level3">
<h3 class="anchored" data-anchor-id="implementation">Implementation</h3>
<p>Now we can actually implement this simple network. Lets start by recalling the sigmoid function:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>plt.plot(np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>),sigmoid(np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>)))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="nn_files/figure-html/cell-6-output-1.png" width="571" height="411"></p>
</div>
</div>
<p>Lets define the function of a <strong>node</strong>, being careful to match our notation:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> node(w,b,a):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> w<span class="op">*</span>a <span class="op">+</span> b</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sigmoid(z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we are ready to define our <strong>actual network</strong>! Lets initialize our random weights and biases:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>w1,b1,w2,b2,w3,b3 <span class="op">=</span> np.random.rand(<span class="dv">6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, lets pick a training example and run it through our network:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span>,label <span class="op">=</span> inputs[<span class="dv">0</span>],labels[<span class="dv">0</span>]</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># h1 node - operates on our input</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>a1 <span class="op">=</span> node(w1,b1,<span class="bu">input</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># h2 node - operates on a1</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>a2 <span class="op">=</span> node(w2,b2,a1)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># output node - operates on a2 - produces our output</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> node(w3,b3,a2)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets see how we did!</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"for input: </span><span class="sc">{</span><span class="bu">input</span><span class="sc">}</span><span class="ss"> with label: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">, this network calculated: </span><span class="sc">{</span>y_hat<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>for input: [-5.] with label: [-1], this network calculated: [0.74906357]</code></pre>
</div>
</div>
<p>As we know, to actually see how we did, we need to <strong>define a cost</strong>! Lets proceed with the usual average-MSE:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse(y_hat,y,average<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> average:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.mean((y_hat<span class="op">-</span>y)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ((y_hat<span class="op">-</span>y)<span class="op">**</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cost_mse(y_hat,y):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span><span class="op">*</span>np.linalg.norm(y<span class="op">-</span>y_hat)<span class="op">**</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"for input: </span><span class="sc">{</span><span class="bu">input</span><span class="sc">}</span><span class="ss"> with label: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">, this network calculated: </span><span class="sc">{</span>output<span class="sc">}</span><span class="ss">, giving us a cost of: </span><span class="sc">{</span>mse(output,label)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>for input: [-5.] with label: [-1], this network calculated: [0.74906357], giving us a cost of: 3.0592233770969504</code></pre>
</div>
</div>
<p>Ok - so we‚Äôve demonstrated the entire forward direction, for one sample. Lets define a function for this simple network, so we can run our entire training set through!</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_network(inputs):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> []</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> inputs.shape[<span class="dv">0</span>]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialized weights</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    w1,b1,w2,b2,w3,b3 <span class="op">=</span> np.random.rand(<span class="dv">6</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> inputs[i,:]</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># h1 node</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        a1 <span class="op">=</span> node(w1,b1,<span class="bu">input</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># h2 node</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        a2 <span class="op">=</span> node(w2,b2,a1)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># output node</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> node(w3,b3,a2)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># append to form output</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        outputs.append(output)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.asarray(outputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now calculate our <strong>average loss</strong> over the entire training set:</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>mse(simple_network(inputs),labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>1.5262278676793288</code></pre>
</div>
</div>
<hr>
</section>
</section>
<section id="multidimensional-network" class="level2">
<h2 class="anchored" data-anchor-id="multidimensional-network">Multidimensional Network</h2>
<p>We‚Äôve looked at a very simple example above which captures the essence of what we are doing. However, our networks are really never actually composed of layers with one element each. Instead, <strong>each</strong> layer has <strong>multiple</strong> nodes.</p>
<p>Technically, we could continue in the same way as above and individually number our weights and biases but this quickly gets out of hand! As an exercise, try repeating the above analysis and implementation where each hidden layer is now of size 2!</p>
<p>Ironically, to avoid this notational complexity, it seems like we must introduce additional notation, and rewrite our problem in the language of <strong>linear algebra</strong>.</p>
<p>To introduce this notation, lets imagine a network the following structure: <img src="figures/md-1.png" class="img-fluid"></p>
<p>To make our analysis simpler, lets zoom in on node node, and all of its input weights:</p>
<p><img src="figures/md-2.png" class="img-fluid"></p>
<p>We‚Äôve highlighted the 0th node in our last hidden layer and all of its inputs from the previous layer in <span class="math inline">\color{orange}{\text{orange}}</span>. We‚Äôve also numbered each of it input nodes with their layer-specific numbering and shown them in <span class="math inline">\color{blue}{\text{blue}}</span>.</p>
<p>We‚Äôve also named each weight according to the following format:</p>
<p><span class="math display">
w_{\;\color{orange}{\text{current node #}} \;,\; \color{blue}{\text{incoming node #}}}
</span></p>
<p>This may seem a bit counter intuitive at first, as the tendency when reading from left to write is to want to write our weights as:</p>
<p><span class="math display">
w_{\;\color{blue}{\text{incoming node #}} \;,\; \color{orange}{\text{current node #}} }
</span></p>
<p>You absolutely can, but that will result in a bunch of transposes in later equations. To get rid of them now, we will number our weights as we did above. As is typically the case, people make seemingly weird/arbitrary decisions at the front to result in simplifications down the line.</p>
<p>Recall, the function of a node is to apply its weight to the activation of its input/previous layer. In this case, we have three previous nodes/input nodes, so we will also write them in <span class="math inline">\color{blue}{\text{blue}}</span> to make it clear that they are coming from the previous layer.</p>
<p>So our orange node is performing:</p>
<p><span class="math display">
\color{orange}{w}_{\color{orange}{0},\color{blue}{0}} \cdot \color{blue}{a_0} +
\color{orange}{w}_{\color{orange}{0},\color{blue}{1}} \cdot \color{blue}{a_1} +
\color{orange}{w}_{\color{orange}{0},\color{blue}{2}} \cdot \color{blue}{a_2} +
\color{orange}{b_0}
</span></p>
<p>Already, our eyes should be screaming <strong>dot-product</strong>!</p>
<section id="notation" class="level3">
<h3 class="anchored" data-anchor-id="notation">Notation</h3>
<p>Indeed, we can form a <strong>vector</strong> of the <font color="blue"><strong>previous layer‚Äôs</strong></font> activations as:</p>
<p><span class="math display">
\color{blue}{\mathbf{a}_{prev}} =
\begin{bmatrix}
\color{blue}{a_0 \\
a_1 \\
a_2}
\end{bmatrix}
</span></p>
<p>and a <strong>vector</strong> of the 0-th neurons <strong>weights</strong> in the <font color="orange"><strong>current layer</strong></font> as:</p>
<p><span class="math display">
\color{orange}{\mathbf{w}_0} =
\begin{bmatrix}
\color{orange}{w}_{\color{orange}{0},\color{blue}{0}} \\
\color{orange}{w}_{\color{orange}{0},\color{blue}{1}} \\
\color{orange}{w}_{\color{orange}{0},\color{blue}{2}}
\end{bmatrix}
</span></p>
<p>where we have used color again to make it clear what layer we are talking about: the <font color="orange">current</font> or <font color="blue">previous</font> layer.</p>
<p>Then we can rewrite what our orange node is calculating as: <span class="math display">
\begin{align}
\color{orange}{z_0} &amp;= \color{orange}{\mathbf{w}_0} \cdot \color{blue}{\mathbf{a}_{prev}} + \color{orange}{b_0} \\
\color{orange}{a_0} &amp;= \sigma(\color{orange}{z_0})
\end{align}
</span></p>
<p>Well, we‚Äôve managed to rewrite the activation of <strong>one-node</strong> in slightly better notation. But we can we do better! Lets now reason about the entire layer!</p>
<p>Recall, we already have a vector of the <font color="blue"><strong>previous layer‚Äôs</strong> </font>activations in <span class="math inline">\color{blue}{\mathbf{a}_{prev}}</span>, although we never actually gave a formula for it. Based on the formula for the <font color="orange">0th-node</font> in the <font color="orange">current layer</font> we just gave, lets try to give a <strong>vector</strong> of activations for the entire <font color="orange"><strong>current layer</strong></font>.</p>
<p>(Note: to prevent a color explosion, since we‚Äôre talking about the current layer, I will drop orange to refer to it most places. I will keep blue to refer to the previous layer).</p>
<p>To simplify our analysis, lets first note that: <span class="math display">
\mathbf{a}_{curr} =
\begin{bmatrix}
\sigma (z_0) \\
\sigma (z_1) \\
\sigma (z_2) \\
\end{bmatrix} =
\sigma\left(\;
\begin{bmatrix}
z_0 \\
z_1 \\
z_2 \\
\end{bmatrix}\;
\right) =
\sigma (\mathbf{z}_{curr})
</span></p>
<p>So, lets focus on writing a formula for the vector <span class="math inline">\mathbf{z}_{curr}</span>:</p>
<p><span class="math display">
\mathbf{z}_{curr} =
\begin{bmatrix}
\mathbf{w}_0 \cdot \color{blue}{\mathbf{a}_{prev}} + b_0 \\
\mathbf{w}_1 \cdot \color{blue}{\mathbf{a}_{prev}} + b_1 \\
\mathbf{w}_2 \cdot \color{blue}{\mathbf{a}_{prev}} + b_2 \\
\end{bmatrix}
</span></p>
<p>Lets make it a bit clearer by writing our biases for the entire layer as a separate <strong>vector</strong> <span class="math inline">\mathbf{b}</span>:</p>
<p><span class="math display">
\mathbf{z}_{curr} =
\begin{bmatrix}
\mathbf{w}_0 \cdot \color{blue}{\mathbf{a}_{prev}} \\
\mathbf{w}_1 \cdot \color{blue}{\mathbf{a}_{prev}} \\
\mathbf{w}_2 \cdot \color{blue}{\mathbf{a}_{prev}}  \\
\end{bmatrix}
+
\mathbf{b}
</span></p>
<p>Just like we saw when we discussed linear regression, this vector of dot products is exactly the <strong>matrix-vector</strong> product of the <strong>weight matrix</strong> and the previous layers <strong>activation vector</strong>!</p>
<div class="info">
<p><strong>Definition</strong>: The current layers <strong>weight matrix</strong>: <span class="math inline">\mathbf{W}</span> is a matrix of <span class="math inline">k</span>-many rows, and <span class="math inline">j</span>-many columns, where <span class="math inline">k</span> is the number of nodes in the current layer, and <span class="math inline">j</span> is the number of nodes in the previous layer:</p>
<p><span class="math display">
\mathbf{W} \in \mathbb{R}^{k,j}=
\begin{bmatrix}
\mathbf{w}_0^T \\
\mathbf{w}_1^T \\
\ldots \\
\mathbf{w}_k^T
\end{bmatrix} =
\begin{bmatrix}
w_{0,0} &amp; w_{0,1} &amp; \ldots &amp; w_{0,j} \\
\ldots \\
w_{k,0} &amp; w_{0,1} &amp; \ldots &amp; w_{k,j} \\
\end{bmatrix}
</span></p>
<p><strong>Note</strong>: each row of the weight matrix represents all inputs to a specific node in the current layer.</p>
</div>
<p>Now, we can finally write a complete linear algebraic equation for the function of a <font color="orange">current layer</font> on a <font color="blue">previous layer</font>:</p>
<p><span class="math display">
\begin{align}
\mathbf{z}_{curr} &amp;= \mathbf{W}\color{blue}{\mathbf{a}_{prev}}+\mathbf{b} \\
\mathbf{a}_{curr} &amp;= \sigma(\mathbf{z}_{curr})
\end{align}
</span></p>
<p>Now, neural networks do this <strong>sequentially</strong>, so the last piece of the puzzle is to be able to refer to a <em>specific layer</em> by number. We now introduce the final piece of notation to let us do this: a <strong>superscript</strong> to designate the layer number:</p>
<div class="info">
<p>The activation of layer <span class="math inline">L</span> is given by:</p>
<p><span class="math display">
\begin{align}
\mathbf{z}^L &amp;= \mathbf{W}^L \mathbf{a}^{L-1}+\mathbf{b}^L \\
\mathbf{a}^L &amp;= \sigma(\mathbf{z}^L)
\end{align}
</span></p>
<p>This is often written succinctly as:</p>
<p><span class="math display">
\boxed{\mathbf{a}^L = \sigma(\mathbf{W}\mathbf{a}^{L-1} + \mathbf{b})}
</span></p>
<p>where the specific <span class="math inline">\mathbf{W},\mathbf{b}</span> we are talking about is implied.</p>
</div>
<p><br> Wow we‚Äôve come a long way! We‚Äôve given a very clear and succinct linear algebraic equation for the entire forward direction for a network of any number of layers and size of each layer!</p>
<p>Lets perform a <strong>size sanity check</strong>: * <span class="math inline">\mathbf{W}</span> is of size <span class="math inline">k \times j</span>, where <span class="math inline">j</span> is the number of neurons in the previous layer. * <span class="math inline">\mathbf{a}^{L-1}</span> is a vector of size <span class="math inline">j \times 1</span>, the activations of the previous layer. * Their multiplication results in a vector of size <span class="math inline">k \times 1</span>, where <span class="math inline">k</span> is the number of neurons in the current layer. * Our bias vector is also <span class="math inline">k \times 1</span> (as we expect!).</p>
<p>So everything works as expected!</p>
<p>This is why we decided to write our weight matrix to be of size <span class="math inline">k \times j</span> or <span class="math inline">\text{# neurons in prev layer} \times \text{# neurons in current layer}</span> instead of the other way around. If we had, we‚Äôd need a transpose in the equation above.</p>
<hr>
</section>
<section id="implementation-1" class="level3">
<h3 class="anchored" data-anchor-id="implementation-1">Implementation</h3>
<p>Armed with our new notation, lets write an implementation of the network we gave above:</p>
<p><img src="figures/md-1.png" class="img-fluid"></p>
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: What should the sizes of each <span class="math inline">W</span> be for this network? Lets go through it together!</p>
</div>
<p>Ok! Now lets implement it!</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>inputs,labels <span class="op">=</span> toy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_network2(inputs):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> []</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> inputs.shape[<span class="dv">0</span>]</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize weight matrices - notice the dimensions</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    W_1 <span class="op">=</span> np.random.randn(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    W_2 <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">3</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    W_3 <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and our biases</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    b_1 <span class="op">=</span> np.random.randn(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    b_2 <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    b_3 <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loop through training data</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># correct size for current input</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> inputs[i,:]</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># layer 1</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        a_1 <span class="op">=</span> sigmoid(W_1<span class="op">*</span><span class="bu">input</span> <span class="op">+</span> b_1)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># layer 2</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        a_2 <span class="op">=</span> sigmoid(W_2.dot(a_1)<span class="op">+</span>b_2)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># output layer</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> sigmoid(W_3.dot(a_2)<span class="op">+</span>b_3)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># append to form output</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>        outputs.append(output)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.squeeze(np.asarray(outputs)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> simple_network2(inputs)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(inputs,outputs,c<span class="op">=</span>labels)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="nn_files/figure-html/cell-18-output-1.png" width="588" height="411"></p>
</div>
</div>
</section>
</section>
<section id="mini-batch-notation" class="level2">
<h2 class="anchored" data-anchor-id="mini-batch-notation">Mini-Batch Notation</h2>
<p>Note, this implementation runs an example through the network <strong>one-by-one</strong>! Instead, we can imagine feeding our entire dataset through <strong>at once</strong>! This formalism will pave the way for us later to feed in a mini-batch at a time.</p>
<p>Lets just reason our way through this one from <strong>first-principles</strong> (fancy way to say: <strong>lets match our matrix sizes!</strong>), seeing how we get our entire dataset through the first hidden layer.</p>
<p>The first weight matrix is of size:</p>
<p><span class="math display">
\mathbf{W}_1: (\text{size of hidden layer} \times \text{dimension of input})
</span></p>
<p>which in this case is: <span class="math inline">3 \times 1</span>. If our input was 2-D, it would be <span class="math inline">3 \times 2</span>. So what we <em>dot</em> it with, needs to be of size: <span class="math inline">\text{dimension of input} \times \text{??}</span>.</p>
<p><strong>Its in that dimension that we can place our entire dataset!</strong></p>
<p>So, now we‚Äôre going to be shuffling <strong>activation matrices</strong> around! In these activation matrices, each <strong>column</strong> is an activation for the last layer on a different training example!</p>
<p><img src="figures/activation-matrix.png" class="img-fluid"></p>
<p>So we expect the <strong>first activation matrix</strong> to be of size: <span class="math inline">\text{dimension of input} \times \text{number of samples}</span>. This means this must also be the size of the initial input matrix for the first hidden layer.</p>
<p>So, we can rewrite our layer propagation equation above for our entire dataset:</p>
<p><span class="math display">
\boxed{\mathbf{A}^L = \sigma(\mathbf{W}\mathbf{A}^{L-1} + \mathbf{b})}
</span></p>
<p>where we use <strong>broadcasting rules</strong> to let us add a vector <span class="math inline">\mathbf{b}</span> to a matrix.</p>
<hr>
<p>Lets make a special note about the <strong>first hidden layer</strong>, and how it processes our <strong>input matrix</strong>.</p>
<p>Typically, we imagine our data matrix such that the first dimension is the <code>batch_size</code>:</p>
<p><img src="figures/X.png" class="img-fluid"></p>
<p>This means <strong>each row</strong> of this matrix, corresponds to <strong>one sample.</strong></p>
<p>So if our data matrix is of size <span class="math inline">\text{batch_size} \times \text{dimension}</span>, in order for our first layer to calculate correctly, <strong>we have to make the sizes work</strong>! Meaning, our first layer should be:</p>
<p><span class="math display">
\mathbf{A}^1 = \sigma(\mathbf{W}^1\mathbf{X}^T + \mathbf{b})
</span></p>
<p>where <span class="math inline">X^T</span> is:</p>
<p><img src="figures/X-T.png" class="img-fluid"></p>
<div class="info">
<p><strong>Note:</strong> Here we <strong>define</strong> the input matrix <span class="math inline">X</span> to be of size: <span class="math inline">N \times d</span>. That is why we transpose it in the first layer. This is by no means universal, and different numerical libraries do it differently. You might come across libraries or papers to talks, where the input matrix is defined to be of size <span class="math inline">d \times N</span>. If that is the case, the first layer does <strong>not</strong> need an <span class="math inline">X^T</span>!</p>
</div>
<p>Now we can implement this simple network, using this notation!</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_network2_batch(inputs):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># assume inputs is of shape Nxd</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize weight matrices - notice the dimensions</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    W_1 <span class="op">=</span> np.random.randn(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    W_2 <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">3</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    W_3 <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and our biases</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    b_1 <span class="op">=</span> np.random.randn(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    b_2 <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    b_3 <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">NOTE</span><span class="co"> - there is no for loop here! </span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># layer 1</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    a_1 <span class="op">=</span> sigmoid(W_1.dot(X.T) <span class="op">+</span> b_1)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># layer 2</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    a_2 <span class="op">=</span> sigmoid(W_2.dot(a_1)<span class="op">+</span>b_2)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output layer</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> sigmoid(W_3.dot(a_2)<span class="op">+</span>b_3)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.squeeze(np.asarray(outputs)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now lets actually run our <strong>entire dataset</strong> through (since we aren‚Äôt yet dealing with batches), to generate outputs:</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>inputs,labels <span class="op">=</span> toy()</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> outputs <span class="op">=</span> simple_network2(inputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Just for fun, lets plot it!</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>fig,(ax1,ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>,figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">7</span>))</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Original poits and labels'</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>ax1.scatter(inputs,labels)<span class="op">;</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Original poits and Y_Hat'</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>ax2.scatter(inputs,outputs,color<span class="op">=</span><span class="st">'orange'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="nn_files/figure-html/cell-21-output-1.png" width="1185" height="579"></p>
</div>
</div>
<p>Note the huge difference in our y-axis! This is <strong>without</strong> and training so its bound to be bad!</p>
<div class="info">
<p><strong>Pause-and-ponder</strong>: Go back and re run the network above and re-plot the final representation. Notice how random it is! This is because we initialize with random weights, but never actually do any training!</p>
</div>
</section>
<section id="dealing-with-biases" class="level2">
<h2 class="anchored" data-anchor-id="dealing-with-biases">Dealing with Biases</h2>
<p>Another very common notational trick people do, as we saw in <strong>linear regression</strong>, was to <strong>add ‚Äúanother dimension‚Äù</strong> to ‚Äúautomatically‚Äù deal with our <strong>bias</strong>.</p>
<p>Here, that means adding <strong>another ‚Äúnode‚Äù to each layer</strong>. This node has <strong>no input, only outputs</strong>, and its <strong>activation is always the value 1</strong>.</p>
<p><img src="figures/biases.png" class="img-fluid"></p>
<div class="info">
<p><strong>Question</strong>: How does this act as a bias?</p>
</div>
<p>Well, lets look at what this means for a particular node. Lets once again highlight the orange node:</p>
<p><img src="figures/bias-weights.png" class="img-fluid"></p>
<p>What <strong>effect</strong> does this have for this orange nodes update? Well, lets write out what it is:</p>
<p><img src="figures/bias-weights-eqn.png" class="img-fluid"></p>
<p>So, <span class="math inline">\color{orange}{w}_{\color{orange}{0},\color{blue}{3}}</span> is <strong>always</strong> being multiplied by the value <span class="math inline">1</span>. This is exactly the role of the bias!</p>
<p><img src="figures/bias-weights-eqn-simp.png" class="img-fluid"></p>
<p>As we can see, the addition of a constant node in a layer <strong>gives an extra weight to each node in the next layer</strong>. This weight, multiplied by 1, <strong>acts as the bias for each node</strong>.</p>
<p>So all we have to do, is <strong>add an extra <font color="red">column</font> to each weight matrix</strong> in our network. (Note: often, this bias is omitted from the final layer).</p>
<p>Now, for a layer <span class="math inline">L</span>, the <strong>weight matrix</strong> is of size: <span class="math inline">k \times j+1</span>, where <span class="math inline">k</span> is the number of <em>actual</em>/<em>real</em> hidden nodes in layer <span class="math inline">L</span>, and <span class="math inline">j</span> is the number of <em>actual/real</em> hidden nodes in layer <span class="math inline">L-1</span>.</p>
<p><strong>Note</strong>: often this is drawn with the bias nodes on top of the others, not below.</p>
<p><img src="figures/bias-on-top.png" class="img-fluid"></p>
<p>So we would think of the 0-th weight acting as the bias, so we would add an extra column to the left/right of each weight matrix. Its ultimately the same thing</p>
<p>As a sanity check that this works, lets compare the output of the previous implementation with this new notation. To do so, we need to make sure they use the same initialization. Lets take it out of the function and ‚Äúmake it flat‚Äù so we can easily compare:</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># W and b network</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weight matrices - notice the dimensions</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>W_1 <span class="op">=</span> np.random.randn(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>W_2 <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">3</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>W_3 <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># and our biases</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>b_1 <span class="op">=</span> np.random.randn(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>b_2 <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>b_3 <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># W network - adding biases to the right:</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>W_1_b <span class="op">=</span> np.hstack((W_1,b_1))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>W_2_b <span class="op">=</span> np.hstack((W_2,b_2))</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>W_3_b <span class="op">=</span> np.hstack((W_3,b_3))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets run the ‚Äúold network‚Äù notation:</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># layer 1</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>a_1_old <span class="op">=</span> sigmoid(W_1.dot(inputs.T) <span class="op">+</span> b_1)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># layer 2</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>a_2_old <span class="op">=</span> sigmoid(W_2.dot(a_1_old)<span class="op">+</span>b_2)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># output layer</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>output_old <span class="op">=</span> sigmoid(W_3.dot(a_2_old)<span class="op">+</span>b_3)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>output_old <span class="op">=</span> np.squeeze(np.asarray(output_old)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And now the ‚Äúnew‚Äù network notation. Note: in order to run the inputs through, we need to add the ‚Äúextra dimension of 1‚Äôs‚Äù, as we‚Äôve done many times before!</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>inputs_new <span class="op">=</span> np.c_[inputs,np.ones(inputs.shape[<span class="dv">0</span>])]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can go ahead and ‚Äúfeed it in‚Äù</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># layer 1</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>a_1_new <span class="op">=</span> sigmoid(W_1_b.dot(inputs_new.T))</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="co"># append the 1 to the end of the previous layers activations:</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>a_1_new <span class="op">=</span> np.r_[a_1_new, np.ones((<span class="dv">1</span>,a_1_new.shape[<span class="dv">1</span>]))]</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="co"># layer 2</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>a_2_new <span class="op">=</span> sigmoid(W_2_b.dot(a_1_new))</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co"># append the 1 to the end of the previous layers activations:</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>a_2_new <span class="op">=</span> np.r_[a_2_new, np.ones((<span class="dv">1</span>,a_2_new.shape[<span class="dv">1</span>]))]</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="co"># output layer</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>output_new <span class="op">=</span> sigmoid(W_3_b.dot(a_2_new))</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>output_new <span class="op">=</span> np.squeeze(np.asarray(output_new)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets verify they are both equal:</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>np.equal(output_new,output_old).<span class="bu">all</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>False</code></pre>
</div>
</div>
<div class="info">
<p><strong>Note</strong>: This might seem like <strong>a strange hack</strong> - We have to reshape each layers weight matrix, and each layers activation matrix to account for this extra ‚Äú1‚Äù flying around everywhere.</p>
<p>I will not try to convince you one way or the other which makes the most sense. I‚Äôm just explaining it here in case it is useful to you to think of the bias as ‚Äúbeing wrapped up‚Äù in our weight matrix, as it was when we discussed linear regression.</p>
<p>Moving forward, we will <strong>not</strong> be using this notation in the rest of the notebook.</p>
</div>
</section>
<section id="activation-functions" class="level2">
<h2 class="anchored" data-anchor-id="activation-functions">Activation Functions</h2>
<p>Lets imagine revisiting the previous network structure under different activation functions:</p>
<section id="linear" class="level3">
<h3 class="anchored" data-anchor-id="linear">Linear</h3>
<p>Lets start with a <strong>linear activation</strong> function:</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear(z):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This is almost always called a linear activation, but is better thought of as the identity activation - that is, it just returns what it was given, unaltered.</p>
<p>What does this mean for the computation our network can perform?</p>
<p>Once again, lets flatten our implementation to take a look:</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>inputs,labels <span class="op">=</span> toy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weight matrices - notice the dimensions</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>W_1 <span class="op">=</span> np.random.randn(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>W_2 <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">3</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>W_3 <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co"># and our biases</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>b_1 <span class="op">=</span> np.random.randn(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>b_2 <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>b_3 <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># layer 1</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>a_1 <span class="op">=</span> linear(W_1.dot(inputs.T) <span class="op">+</span> b_1)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># layer 2</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>a_2 <span class="op">=</span> linear(W_2.dot(a_1)<span class="op">+</span>b_2)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co"># output layer</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> linear(W_3.dot(a_2)<span class="op">+</span>b_3)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> np.squeeze(np.asarray(outputs)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets plot it:</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>fig,(ax1,ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>,figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">7</span>))</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Original poits and labels'</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>ax1.scatter(inputs,labels)<span class="op">;</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Original poits and Y_Hat'</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>ax2.scatter(inputs,outputs,color<span class="op">=</span><span class="st">'orange'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="nn_files/figure-html/cell-32-output-1.png" width="1185" height="579"></p>
</div>
</div>
<div class="info">
<p>üßê <strong>Pause-and-ponder</strong>: Go back and re-run the above a few times. What does this mean?</p>
</div>
<p>Remember, what we are visualizing is the inputs and the corresponding output value - what our network has been able to map to.</p>
<p>You might be noticing that this is strangely linear! Indeed! With linear activation functions, all we are able to do is calculate some linear combination of our inputs!</p>
<p>Lets dig in a bit more, and show what we are actually calculating at Layer <span class="math inline">L</span>:</p>
<p><span class="math display">
A^L = W_3((W_2(W_1X^T + b_1) + b_2) + b_3
</span></p>
<p>Well, lets distribute this out: <span class="math display">
A^L = W_3W_2W_1X^T + W_3W_2b_1 + W_3b_2 + b_3
</span></p>
<p>Ok, wait a second‚Ä¶ this is getting confusing - lets check to make sure the sizes work out!</p>
<p><img src="figures/linear-sizes.png" class="img-fluid"></p>
<div class="info">
<p>Lets go through this part together in class!</p>
</div>
<p>Ok! We can see that the sizes do in fact work out!</p>
<div class="info">
<p>üßê<strong>Pause-and-ponder:</strong> In your resulting equation, perform some clarifying substitutions. What do we discover?</p>
</div>
<p>After some substitutions, we can write an equation like the following:</p>
<p><span class="math display">
\hat{\mathbf{Y}} = \mathbf{W}^*X^T + \mathbf{b}^*
</span></p>
<p>Which tells us that a NN with only linear activations, is ultimately just another linear function of its inputs! It doesn‚Äôt matter how deep or wide it is!</p>
</section>
<section id="a-final-sigmoid" class="level3">
<h3 class="anchored" data-anchor-id="a-final-sigmoid">A Final Sigmoid</h3>
<p>Given our understanding above, what would happen if we only add a sigmoid at the end? Something like:</p>
<p><img src="figures/final-sig.png" class="img-fluid"></p>
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: What do you think it represents? What is its <strong>representational capacity</strong>?</p>
</div>
</section>
</section>
</section>
<section id="backprop" class="level1">
<h1>Backprop</h1>
<p>Now that we have a good idea about <strong>forward propagation</strong> we‚Äôre ready to learn by <strong>backpropagating an error</strong> signal through the network, allowing us to <strong>attribute</strong> ‚Äúblame‚Äù/‚Äúerror‚Äù to each specific weight/bias in our network.</p>
<p>In other words, one way to think of what the result of backprop is as a large vector which tells us which parameters are the most responsible for our error - or phrased another way - which ones we should change, and by how much, and in what direction, in order to get the biggest reduction of our error.</p>
<section id="latex-definitions" class="level2">
<h2 class="anchored" data-anchor-id="latex-definitions">Latex definitions</h2>
<p>This cell just creates some latex definitions we will need later.</p>
<p><span class="math display">
\newcommand{\bx}[1]{\color{purple}{b^{#1}}}
\newcommand{\bl}{\color{purple}{b^L}}
\newcommand{\wx}[1]{\color{blue}{w^{#1}}}
\newcommand{\wl}{\color{blue}{w^L}}
\newcommand{\wone}{\color{blue}{w_1}}
\newcommand{\ax}[1]{\color{green}{a^{#1}}}
\newcommand{\al}{\color{green}{a^L}}
\newcommand{\zx}[1]{\color{orange}{z^{#1}}}
\newcommand{\zl}{\color{orange}{z^L}}
\newcommand{\ap}{\color{green}{a^{L-1}}}
\newcommand{\czero}{\color{red}{C_0}}
\newcommand{\dc}{\color{red}{\partial C_0}}
\newcommand{\dw}[1]{\color{blue}{\partial \wx{#1}}}
\newcommand{\dz}[1]{\color{orange}{\partial \zx{#1}}}
\newcommand{\da}[1]{\color{green}{\partial \ax{#1}}}
\newcommand{\db}[1]{\color{purple}{\partial \bx{#1}}}
\newcommand{\dap}{\color{green}{\partial \ax{L-1}}}
\newcommand{\dcdw}[1]{\frac{\dc}{\dw{#1}}}
\newcommand{\dcdz}[1]{\frac{\dc}{\dz{#1}}}
\newcommand{\dzdb}[1]{\frac{\dz{#1}}{\db{#1}}}
\newcommand{\dzdw}[1]{\frac{\dz{#1}}{\dw{#1}}}
\newcommand{\dadz}[1]{\frac{\da{#1}}{\dz{#1}}}
\newcommand{\dcda}[1]{\frac{\dc}{\da{#1}}}
\newcommand{\dcdb}[1]{\frac{\dc}{\db{#1}}}
\newcommand{\dcdap}{\frac{\dc}{\dap}}
\newcommand{\deltal}{\delta^L}
\newcommand{\deltax}[1]{\delta^{#1}}
</span></p>
<p><strong>Note: Make sure you run this cell!</strong></p>
</section>
<section id="d-case" class="level2">
<h2 class="anchored" data-anchor-id="d-case">1-D Case</h2>
<p><strong>Note</strong>: This presentation follows <strong>very closely</strong> these two <strong>fantastic</strong> resources * <a href="https://www.youtube.com/watch?v=tIeHLnjs5U8">3blue1brown - Backpropagation calculus</a> * <a href="http://neuralnetworksanddeeplearning.com/chap2.html">Michael Nielsen - Neural Networks and Deep Learning - Chapter 2</a></p>
<p>Lets revisit the simplest network we started with at the beginning of this notebook:</p>
<p><img src="figures/1d-bp-net.png" class="img-fluid"></p>
<p><strong>Notation Note</strong>: we‚Äôre introducing a new notation <span class="math inline">\color{blue}{w_3 = w^L, w_2 = w^{L-1}}, \ldots</span> and we will be using them interchangeably.</p>
<p>To focus our discussion, lets just focus on the last two levels, and label their activation values:</p>
<p><img src="figures/1d-bp-end.png" class="img-fluid"></p>
<p>The output layer defines the ‚Äúrepresentation‚Äù our network has created for a specific input:</p>
<p><span class="math display">
\begin{align}
\zl &amp;= \wl\ap+\bl \\
\al &amp;= \sigma(\zl)
\end{align}
</span></p>
<p>Where now we are using color to denote the <em>kind</em> of variable we are talking about. For example, activations for any layer are <font color="green">green</font>.</p>
<p>So, its with this layers activation that we want to measure our <strong>cost</strong>, on a <strong>specific example</strong> <span class="math inline">x_0</span> run through our network:</p>
<p><span class="math display">
\czero = (\al - y)^2
</span></p>
<p>Another way to think about this process, is as this <strong>computational graph</strong>:</p>
<p><img src="figures/1d-bp-graph.png" class="img-fluid"></p>
<p>This tells a ‚Äúcausal‚Äù story, about what variables are needed to compute other variables. Note: this could be carried even further back through the network, all the way to the inputs!</p>
<div class="info">
<p><strong>Note:</strong> This is a ‚Äúlight weight‚Äù/conceptual <strong>computational graph</strong>. Its a way to introduce the concept of backpropagating partial derivatives through a graph using the chain rule.</p>
</div>
<p>Lets try to understand exactly what a ‚Äúpartial derivative‚Äù like <span class="math inline">\dcdw{L}</span> is telling us, by associating <strong>a little number line</strong> with each of these variables:</p>
<p><img src="figures/1d-bp-graph-num-line.png" class="img-fluid"></p>
<p>Pictorially, this is telling us that a little nudge to a weight, results in a nudge to the ‚Äúactivity‚Äù/<span class="math inline">\zl</span> of the neuron (Q: how big/how small of a nudge?), which then results in a nudge to the activation/<span class="math inline">\al</span> of the neuron (Q: how big/how small of a nudge?) which then results in a nudge to the total cost of the network (how big/how small of a nudge?).</p>
<p>So conceptually, the partial <span class="math inline">\dcdw{L}</span> is capturing:</p>
<p><img src="figures/1d-bp-dcdw.png" class="img-fluid"></p>
<section id="simulation" class="level3">
<h3 class="anchored" data-anchor-id="simulation">Simulation</h3>
<p>We actually go ahead and simulate this to better understand what its telling us! Lets start by running an example through our network:</p>
<div class="cell" data-tags="[]" data-execution_count="32">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>inputs,outputs <span class="op">=</span> toy()</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span>,output <span class="op">=</span> inputs[<span class="dv">0</span>],outputs[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weights</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>w_1 <span class="op">=</span> np.random.randn()</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>w_2 <span class="op">=</span> np.random.randn()</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>w_3 <span class="op">=</span> <span class="dv">1</span> <span class="co">#exagerating for plotting purposes</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="co"># and our biases</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>b_1 <span class="op">=</span> np.random.randn()</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>b_2 <span class="op">=</span> np.random.randn()</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>b_3 <span class="op">=</span> np.random.randn()</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="co"># layer 1</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>z_1 <span class="op">=</span> w_1<span class="op">*</span><span class="bu">input</span><span class="op">+</span>b_1</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>a_1 <span class="op">=</span> sigmoid(z_1)</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="co"># layer 2</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>z_2 <span class="op">=</span> w_2<span class="op">*</span>a_1<span class="op">+</span>b_2</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>a_2 <span class="op">=</span> sigmoid(z_2)</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a><span class="co"># output layer</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>z_3 <span class="op">=</span> w_3<span class="op">*</span>a_2<span class="op">+</span>b_3</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>a_3 <span class="op">=</span> sigmoid(z_3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>cost_0 <span class="op">=</span> (output <span class="op">-</span> a_3)<span class="op">**</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="nudge-colorbluewl" class="level3">
<h3 class="anchored" data-anchor-id="nudge-colorbluewl">Nudge <span class="math inline">\color{blue}{w^L}</span></h3>
<p>Now lets give <span class="math inline">\wl</span> a ‚Äúlittle nudge‚Äù, and see how it propagates through the network!</p>
<p><strong>Note</strong>: This is not really a little nudge. Ive greatly exaggerated it to provide a good looking plot.</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>nudge <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">51</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>z_3_nudged <span class="op">=</span> (w_3 <span class="op">+</span> nudge)<span class="op">*</span>a_2 <span class="op">+</span> b_3</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>a_3_nudged <span class="op">=</span> sigmoid(z_3_nudged)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>cost_0_nudged <span class="op">=</span> (a_3_nudged <span class="op">-</span> output)<span class="op">**</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets plot it!</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>fig, (ax1,ax2,ax3) <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">3</span>,figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">7</span>),sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>ax1.plot(w_3<span class="op">+</span>nudge,z_3_nudged,c<span class="op">=</span><span class="st">"orange"</span>)<span class="op">;</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>ax1.scatter(w_3,w_3<span class="op">*</span>a_2<span class="op">+</span>b_3,s<span class="op">=</span><span class="dv">100</span>,c<span class="op">=</span><span class="st">'orange'</span>)<span class="op">;</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'w_3 + nudge'</span>)<span class="op">;</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'z_3'</span>)<span class="op">;</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">"How a nudge in w_3 affects z_3"</span>)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>ax2.plot(w_3<span class="op">+</span>nudge, a_3_nudged,c<span class="op">=</span><span class="st">'green'</span>)<span class="op">;</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>ax2.scatter(w_3,sigmoid(w_3<span class="op">*</span>a_2<span class="op">+</span>b_3),s<span class="op">=</span><span class="dv">100</span>,c<span class="op">=</span><span class="st">'green'</span>)<span class="op">;</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'w_3 + nudge'</span>)<span class="op">;</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'a_3'</span>)<span class="op">;</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">"How a nudge in a_3 affects z_3"</span>)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>ax3.plot(w_3<span class="op">+</span>nudge, cost_0_nudged,c<span class="op">=</span><span class="st">'red'</span>)<span class="op">;</span></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>ax3.scatter(w_3,(sigmoid(w_3<span class="op">*</span>a_2<span class="op">+</span>b_3)<span class="op">-</span>output)<span class="op">**</span><span class="dv">2</span>,s<span class="op">=</span><span class="dv">100</span>,c<span class="op">=</span><span class="st">'red'</span>)<span class="op">;</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>ax3.set_xlabel(<span class="st">'w_3 + nudge'</span>)<span class="op">;</span></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>ax3.set_ylabel(<span class="st">'C_0'</span>)<span class="op">;</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>ax3.set_title(<span class="st">"How a nudge in w_3 affects C_0"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="nn_files/figure-html/cell-37-output-1.png" width="1567" height="597"></p>
</div>
</div>
<p>What is this telling us? That a little nudge in <span class="math inline">w_3</span> results in very different changes to each the variables down our computational graph!</p>
</section>
<section id="nudge-colorbluew_1" class="level3">
<h3 class="anchored" data-anchor-id="nudge-colorbluew_1">Nudge <span class="math inline">\color{blue}{w_1}</span></h3>
<p>Lets repeat this simulation, except go <strong>more back/backer</strong> to see how a nudge at <span class="math inline">w_1</span> affects our output layer and our cost!</p>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>nudge <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">51</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>z_1_nudged <span class="op">=</span> (w_1 <span class="op">+</span> nudge)<span class="op">*</span><span class="bu">input</span> <span class="op">+</span> b_1</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>a_1_nudged <span class="op">=</span> sigmoid(z_1_nudged)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>z_2_nudged <span class="op">=</span> w_2<span class="op">*</span>a_1_nudged <span class="op">+</span> b_2</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>a_2_nudged <span class="op">=</span> sigmoid(z_2_nudged)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>z_3_nudged <span class="op">=</span> w_3<span class="op">*</span>a_2_nudged <span class="op">+</span> b_3</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>a_3_nudged <span class="op">=</span> sigmoid(z_3_nudged)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>cost_0_nudged <span class="op">=</span> (a_3_nudged <span class="op">-</span> output)<span class="op">**</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Plot it!</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>fig, (ax1,ax2,ax3) <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">3</span>,figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">7</span>),sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>ax1.plot(w_1<span class="op">+</span>nudge,z_1_nudged,c<span class="op">=</span><span class="st">"orange"</span>)<span class="op">;</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>ax1.scatter(w_1,w_1<span class="op">*</span><span class="bu">input</span><span class="op">+</span>b_1,s<span class="op">=</span><span class="dv">100</span>,c<span class="op">=</span><span class="st">'orange'</span>)<span class="op">;</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'W+nudge'</span>)<span class="op">;</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'z_1'</span>)<span class="op">;</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">"How a nudge in w_1 affects z_1"</span>)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>ax2.plot(w_1<span class="op">+</span>nudge, a_3_nudged,c<span class="op">=</span><span class="st">'green'</span>)<span class="op">;</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>ax2.scatter(w_1,sigmoid(w_3<span class="op">*</span>(sigmoid(w_2<span class="op">*</span>sigmoid(w_1<span class="op">*</span><span class="bu">input</span><span class="op">+</span>b_1)<span class="op">+</span>b_2))<span class="op">+</span>b_3),s<span class="op">=</span><span class="dv">100</span>,c<span class="op">=</span><span class="st">'green'</span>)<span class="op">;</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'W+nudge'</span>)<span class="op">;</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'a_3'</span>)<span class="op">;</span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">"How a nudge in w_1 affects a_3"</span>)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>ax3.plot(w_1<span class="op">+</span>nudge, cost_0_nudged,c<span class="op">=</span><span class="st">'red'</span>)<span class="op">;</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>ax3.scatter(w_1,(sigmoid(w_3<span class="op">*</span>(sigmoid(w_2<span class="op">*</span>sigmoid(w_1<span class="op">*</span><span class="bu">input</span><span class="op">+</span>b_1)<span class="op">+</span>b_2))<span class="op">+</span>b_3)<span class="op">-</span>output)<span class="op">**</span><span class="dv">2</span>,s<span class="op">=</span><span class="dv">100</span>,c<span class="op">=</span><span class="st">'red'</span>)<span class="op">;</span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>ax3.set_xlabel(<span class="st">'W+nudge'</span>)<span class="op">;</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>ax3.set_ylabel(<span class="st">'C_0'</span>)<span class="op">;</span></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>ax3.set_title(<span class="st">"How a nudge in w_1 affects our cost"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="nn_files/figure-html/cell-39-output-1.png" width="1563" height="597"></p>
</div>
</div>
<p>Ah, these graphs have are a bit more interesting, so we can point out the following:</p>
<p><strong>Implicit</strong> in all of these graphs are the following idea:</p>
<p><span class="math display">
\frac{\text{the amount of change in a dependent variable}}{\text{the amount of change in a variable that it depends on}}
</span></p>
<p>Well, this is just the <strong>rate of change</strong> of the graph! Which we might remember! Lets restate this idea for each subplot:</p>
<p>The specific rate of change for the first subplot is:</p>
<p><span class="math display">
\frac{\text{resulting change in }\color{orange}{z_1}}{\text{some amount of change in } \color{blue}{w_1}} = \frac{\color{orange}{\Delta z_1}}{\color{blue}{\Delta w_1}}
</span></p>
<p>The specific rate of change for the second subplot is:</p>
<p><span class="math display">
\frac{\text{resulting change in }\color{green}{a_3}}{\text{some amount of change in } \color{blue}{w_1}} = \frac{\color{green}{\Delta a_3}}{\color{blue}{\Delta w_1}}
</span></p>
<p>The specific rate of change for the third subplot is:</p>
<p><span class="math display">
\frac{\text{resulting change in } \color{red}{C_0}}{\text{some amount of change in } \color{blue}{w_1}} = \frac{\color{red}{\Delta C_0}}{\color{blue}{\Delta w_1}}
</span></p>
<p>Aha! That last subplot is telling us something about how <strong>sensitive</strong> the cost <span class="math inline">\czero</span> is to changes in <span class="math inline">\wone</span>. This is what we want!</p>
<p>We can see that a little change to <span class="math inline">\wone</span> to the left/right (depends on random vals), results in a big change to our final cost on this example!</p>
<p>This <strong>rate of change</strong>/<strong>derivative</strong> tells us the direction we need to change <span class="math inline">\wone</span> in order to reduce our costs!</p>
<hr>
</section>
<section id="we-could-do-better" class="level3">
<h3 class="anchored" data-anchor-id="we-could-do-better">We could do better</h3>
<p>Now, from the plotting code for the last subplot up there you might notice that to generate that last subplot, we basically had to run the entire nudged <span class="math inline">\wone + \Delta</span> all the way through our entire network!</p>
<p>We are also reasoning about the derivative by looking at a graph, instead of actually calculating it so that we can use it.</p>
<p>Lets see if we can think of a clever scheme of <strong>actually calculating</strong> our partials with respect to our weights: <span class="math inline">\ldots,\dcdw{L-1},\dcdw{L}</span>, by starting with</p>
<p><span class="math display">
\dcdw{L} = ?
</span></p>
<p>Lets <strong>bring back our computational graph</strong>:</p>
<p><img src="figures/1d-bp-graph.png" class="img-fluid"></p>
<p>Lets use the following idea to <strong>decompose our graph</strong> starting from the end/bottom:</p>
<p><span class="math display">
\frac{\text{the amount of change in a dependent variable}}{\text{the amount of change in a variable that it depends on}}
</span></p>
<hr>
<p>Starting at <span class="math inline">\czero</span>, we have:</p>
<p><span class="math display">
\dcda{L}
</span></p>
<p>This object tells us how the cost changes with respect to the final layers activation. Lets calculate it:</p>
<p><span class="math display">
\begin{align}
\czero &amp;= (\al - y)^2 \\
\dcda{L}  &amp;= 2(\al -y)
\end{align}
</span></p>
<p>What happens if <span class="math inline">\al</span> is small? Well then it contributes less to our error!</p>
<p>So we should focus on making sure that when <span class="math inline">\al</span> is large, its correct!</p>
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: What about <span class="math inline">\frac{\czero}{\partial y}</span> ? What can we say about this?</p>
</div>
<hr>
<p>According to our computational graph, thats all of the <strong>immediate</strong> ‚Äúindependent‚Äù variables <span class="math inline">\czero</span> has. So lets now switch our focus to <span class="math inline">\al</span>:</p>
<p><span class="math display">
\begin{align}
\al &amp;= \sigma (\zl) \\
\dadz{L} &amp;= \sigma ^{\color{red}{\prime}}(\zl)
\end{align}
</span></p>
<p>where <span class="math inline">\sigma^{\color{red}{\prime}}(\cdot)</span> is the <em>derivative</em> of our sigmoid activation function:</p>
<p><span class="math display">
\sigma^{\color{red}{\prime}}(\cdot) = \sigma(\cdot)(1-\sigma(\cdot))
</span></p>
<p>This lets us rewrite the above as:</p>
<p><span class="math display">
\begin{align}
\dadz{L} &amp;= \sigma(\zl)(1-\sigma(\zl))
\end{align}
</span></p>
<hr>
<p>We now have the following partials:</p>
<p><span class="math display">
\begin{align}
\dcda{L} &amp;: \text{how changing } \al \text{ changes } \czero \\ \\
\dadz{L} &amp;: \text{how changing } \zl \text{ changes } \al
\end{align}
</span></p>
<p>It shouldn‚Äôt require too much convincing that we can <strong>multiply</strong> these two objects together to create a new partial:</p>
<p><span class="math display">
\begin{align}
\dcda{L} \cdot \dadz{L} &amp;= \dcdz{L} \\
                        &amp;= \text{how changing } \zl \text{ changes } \czero
\end{align}
</span></p>
<p>Indeed, the notation itself suggests this by allowing us to ‚Äúcancel out‚Äù partials that appear on top and on bottom:</p>
<div style="text-align:center">
<p><img src="figures/dcdz.png"></p>
</div>
<p>The last step is to actually write this out to get an expression for <span class="math inline">\dcdz{L}</span>:</p>
<p><span class="math display">
\begin{align}
\dcdz{L} &amp;= \dadz{L} \cdot \dcda{L} \\
         &amp;= \underbrace{\sigma(\zl)(1-\sigma(\zl))}_{\text{how changing }\zl\text{ affects }\al} \cdot  \underbrace{2(\al -y)}_{\text{how changing }\al\text{ affects }\czero}
\end{align}
</span></p>
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: We have just discovered and applied the <strong>chain rule</strong>! We have used it to <strong>backpropagate</strong> a change at the output of our last layer: <span class="math inline">\dcda{L}</span>, to a change at the input of the activation function of our last layer: <span class="math inline">\dadz{L}</span>.</p>
</div>
<div class="info">
<p>üìñ<strong>Semi-Definition</strong>: <strong>Backpropagation</strong> can be thought of as applying the chain rule <strong>back</strong> across our computational graph!</p>
</div>
<hr>
</section>
<section id="lets-keep-going" class="level3">
<h3 class="anchored" data-anchor-id="lets-keep-going">Lets keep going!</h3>
<p>So far we‚Äôve backpropagated once. Lets look at our map so far:</p>
<p><img src="figures/1d-bp-graph-current.png" class="img-fluid"></p>
<p>Lets keep <strong>backpropagating</strong> (chain-ruling)! We can now look at the inputs <span class="math inline">\zl</span> has. Lets focus on one of the most interesting for right now, <span class="math inline">\wl</span>:</p>
<p><span class="math display">
\begin{align}
\zl &amp;= \wl\ax{L-1}+\bl \\
\dzdw{L} &amp;= \ax{L-1}
\end{align}
</span></p>
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: This derivative has a particularly interesting interpretation. Its saying that the amount a nudge to our weight in the last layer influences the ‚Äúactivity‚Äù of the last layer, depends on how strong the previous neuron was firing! In other words, if the previous neuron was very active, even a small nudge could cause big changes! But if the previous neuron was always low activation/low firing, then this weight doesn‚Äôt really matter!</p>
<p><strong>Note:</strong> This observation is often stated as:</p>
<ul>
<li>‚ÄúNeurons that fire together, wire together‚Äù,</li>
<li>‚ÄúWeights between low activation neurons learn very slowly‚Äù</li>
</ul>
</div>
<p>With this, we can <strong>chain it</strong> together with our other partials to write an expression for <span class="math inline">\dcdw{L}</span>:</p>
<p><span class="math display">
\begin{align}
\dcdw{L} &amp;= \dzdw{L}\dcdz{L}\\ \\
         &amp;= \dzdw{L}\dadz{L}\dcda{L} \\ \\
         &amp;= \underbrace{\ax{L-1}}_{\text{how changing }\wl\text{ affects } \zl} \quad \underbrace{\sigma(\zl)(1-\sigma(\zl))}_{\text{how changing }\zl\text{ affects }\al} \quad \underbrace{2(\al -y)}_{\text{how changing }\al\text{ affects }\czero}
\end{align}
</span></p>
<p>Or simply:</p>
<p><span class="math display">
\begin{align}
\dcdw{L} &amp;= \dzdw{L}\dadz{L}\dcda{L} \\
         &amp;= \left(\ax{L-1}\right) \left(\sigma(\zl)(1-\sigma(\zl))\right) \left(2(\al -y)\right)
\end{align}
</span></p>
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: From here, can you guess the appropriate equation for <span class="math inline">\dcdb{L}</span>?</p>
<p><strong>Hint</strong>:</p>
<p><span class="math display">
\frac{\dc}{?} = \frac{\dz{L}}{?}\dadz{L}\dcda{L}
</span></p>
</div>
<hr>
<p>Wow! We‚Äôve come a long way! We finally have an actual expression for how a change to <strong>one of our weights</strong> (<span class="math inline">\wl</span>) causes a change to our cost <strong>for a single example</strong>.</p>
<p><img src="figures/1d-bp-graph-wl.png" class="img-fluid"></p>
<p>Lets finish up by using the <strong>hint</strong> I gave above</p>
<p><span class="math display">
\frac{\dc}{?} = \frac{\dz{L}}{?}\dadz{L}\dcda{L}
</span></p>
<p>to find an expression for how the cost changes with respect to the <strong>previous layers activity</strong> <span class="math inline">\dcda{L-1}</span>:</p>
<p><span class="math display">
\dcda{L-1} = \underbrace{\frac{\dz{L}}{\da{L-1}}}_{\text{need to calculate}}\underbrace{\dadz{L}\dcda{L}}_{\text{already have this}}
</span></p>
<p>So all we need is an expression for <span class="math inline">\frac{\dz{L}}{\da{L-1}}</span>:</p>
<p><span class="math display">
\begin{align}
\zl &amp;= \wl\ax{L-1}+\bl \\
\frac{\dz{L}}{\da{L-1}} &amp;= \wl
\end{align}
</span></p>
<p>Letting us write:</p>
<p><span class="math display">
\dcda{L-1} = \left(\wl\right) \left(\sigma(\zl)(1-\sigma(\zl))\right) \left(2(\al -y)\right)
</span></p>
<p>For completion, and as an answer to a previous pause-and-ponder, lets write out the expression for <span class="math inline">\dcdb{L}</span>:</p>
<p><span class="math display">
\dcdb{L} = \dzdb{L}\dadz{L}\dcda{L}
</span></p>
<p>We can get an expression for <span class="math inline">\dzdb{L}</span>:</p>
<p><span class="math display">
\begin{align}
\zl      &amp;= \wl\ax{L-1}+\bl \\
\dzdb{L} &amp;= 1
\end{align}
</span></p>
<p>And so <span class="math display">
\begin{align}
\dcdb{L} &amp;= 1\cdot\dadz{L}\dcda{L}\\
         &amp;= \left(\sigma(\zl)(1-\sigma(\zl))\right) \left(2(\al -y)\right)
\end{align}
</span></p>
<p>Lets group this level all together to write all partials for this level:</p>
<p><span class="math display">
\begin{align}
\dcdw{L}   &amp;= \left(\ax{L-1}\right) &amp;&amp;\cdot&amp;\left(\sigma(\zl)(1-\sigma(\zl))\right) \left(2(\al -y)\right) \\
\dcdb{L}   &amp;= 1 &amp;&amp;\cdot&amp;\left(\sigma(\zl)(1-\sigma(\zl))\right) \left(2(\al -y)\right) \\
\dcda{L-1} &amp;=\left(\wl\right) &amp;&amp;\cdot&amp;\left(\sigma(\zl)(1-\sigma(\zl))\right) \left(2(\al -y)\right)
\end{align}
</span></p>
<hr>
</section>
<section id="speaking-of-error" class="level3">
<h3 class="anchored" data-anchor-id="speaking-of-error">Speaking of Error‚Ä¶</h3>
<p>Aha! We can notice they all have something in common:</p>
<p><span class="math display">
\dcdz{L}=\dadz{L}\dcda{L}
</span></p>
<div class="info">
<p><strong>Definition</strong>: The <strong>error</strong> associated with layer <span class="math inline">L</span> is given by:</p>
<p><span class="math display">
\deltal = \dcdz{L}=\dadz{L}\dcda{L}
</span></p>
<p>often stated simply as:</p>
<p><span class="math display">
\deltal = \dcdz{L}
</span></p>
<p>and completely as:</p>
<p><span class="math display">
\begin{align}
\deltal &amp;= \sigma(\zl)(1-\sigma(\zl)) (\al -y) \\
        &amp;= \sigma^{\color{red}{\prime}}(\zl) (\al-y)
\end{align}
</span></p>
<p><strong>Note:</strong> We‚Äôve discarded that <span class="math inline">2</span> from the cost as is often done.</p>
</div>
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: Why is this called the error?</p>
</div>
<p>This lets rewrite our partials as:</p>
<p><span class="math display">
\begin{align}
\dcdw{L}   &amp;= \ax{L-1} \deltal \\
\dcdb{L}   &amp;= \deltal \\
\dcda{L-1} &amp;= \wl \deltal
\end{align}
</span></p>
<hr>
</section>
<section id="what-about-the-rest" class="level3">
<h3 class="anchored" data-anchor-id="what-about-the-rest">What about the rest?</h3>
<p>And thats it! We‚Äôve <strong>finally</strong> finished out how our cost function changes with respect to one layer: <span class="math inline">L</span>. But we‚Äôve only done one layer! Not to worry! We‚Äôve discovered backpropagation and the chain rule! So the rest is easy!</p>
<p>Lets look at our map</p>
<p><img src="figures/1d-bp-map-lminus1.png" class="img-fluid"></p>
<p>We‚Äôve boxed in what we know how to do so far. This also tells us its very straight forward to now give the equations for layer <span class="math inline">L-1</span>:</p>
<p><span class="math display">
\begin{align}
\dcdw{L-1} &amp;= \dzdw{L-1} \dadz{L-1} \dcda{L-1} \\
\dcdb{L-1} &amp;= \dzdb{L-1} \dadz{L-1} \dcda{L-1} \\
\dcda{L-2} &amp;= \frac{\dz{L-1}}{\da{L-2}} \dadz{L-1} \dcda{L-1}
\end{align}
</span></p>
<hr>
</section>
<section id="fully-recursive-definition" class="level3">
<h3 class="anchored" data-anchor-id="fully-recursive-definition">Fully recursive definition</h3>
<p>Wow! We again see they have something in common!</p>
<div class="info">
<p><strong>Definition</strong>: The error for layer <span class="math inline">L-1</span> is:</p>
<p><span class="math display">
\deltax{L-1} = \dcdz{L-1}=\dadz{L-1}\dcda{L-1}
</span></p>
<p>often stated simply as:</p>
<p><span class="math display">
\deltax{L-1} = \dcdz{L-1}
</span></p>
</div>
<p>We can now restate the above as:</p>
<p><span class="math display">
\begin{align}
\dcdw{L-1} &amp;= \ax{L-2} \deltax{L-1} \\
\dcdb{L-1} &amp;=  \deltax{L-1} \\
\dcda{L-2} &amp;= \wx{L-1} \deltax{L-1}
\end{align}
</span></p>
<p>That is a <strong>recursive definition</strong> that serves for the rest of the graph!</p>
<hr>
</section>
<section id="but-wait" class="level3">
<h3 class="anchored" data-anchor-id="but-wait">But wait‚Ä¶</h3>
<p>Theres more! Lets bring back the definitions of error for layer <span class="math inline">L-1</span>, and the partials for layer <span class="math inline">L</span>:</p>
<p>Partials at layer <span class="math inline">L</span>:</p>
<p><span class="math display">
\begin{align}
\dcdw{L}   &amp;= \ax{L-1} \deltal \\
\dcdb{L}   &amp;= \deltal \\
\dcda{L-1} &amp;= \wl \deltal
\end{align}
</span></p>
<p>Error at layer <span class="math inline">L-1</span>:</p>
<p><span class="math display">
\deltax{L-1} = \dcdz{L-1}=\dadz{L-1}\dcda{L-1}
</span></p>
<p>Aha! We see something in common again!</p>
<p><span class="math display">
\boxed{\dcda{L-1} = \wl \deltal}
</span></p>
<p><span class="math display">
\deltax{L-1} = \dcdz{L-1}=\dadz{L-1}\boxed{\dcda{L-1}}
</span></p>
<p>Lets sub that in and write:</p>
<p><span class="math display">
\deltax{L-1} = \dcdz{L-1}=\dadz{L-1}\wl \deltal
</span></p>
<p>We‚Äôve discovered another <strong>major equation!</strong>:</p>
<div class="info">
<p>üìñ <strong>Definition</strong>: The error at <strong>any</strong> layer <span class="math inline">L-1</span> can be written as a function of the <strong>next layer</strong> <span class="math inline">L</span>:</p>
<p><span class="math display">
\deltax{L-1} = \wl \deltal \cdot \sigma^{\color{red}{\prime}}(\zx{L-1})
</span></p>
</div>
<p>For example, for layer <span class="math inline">L-2</span>:</p>
<p><span class="math display">
\begin{align}
\deltax{L-2} &amp;= \wx{L-1}\deltax{L-1}\sigma^{\color{red}{\prime}}(\zx{L-2}) \\
             &amp;= \wx{L-1}\left[  \wl \deltal \cdot \sigma^{\color{red}{\prime}}(\zx{L-1}) \right]\sigma^{\color{red}{\prime}}(\zx{L-2})
\end{align}
</span></p>
<hr>
</section>
<section id="the-four-fundamental-backpropagation-equations" class="level3">
<h3 class="anchored" data-anchor-id="the-four-fundamental-backpropagation-equations">The Four Fundamental Backpropagation Equations‚Ñ¢</h3>
<p>We did it! We‚Äôve completed backprop on this simple 1D network!</p>
<p><img src="figures/1d-bp-net.png" class="img-fluid"></p>
<p>Lets cap this off by rewriting <strong>scalar versions</strong> of the four fundamental backpropagation equations:</p>
<div class="info">
<p><strong>üìñDefinition</strong>: <strong>Scalar</strong> versions of the <strong>Four Fundamental Backpropagation Equations‚Ñ¢</strong> are given by:</p>
<p><span class="math display">
\begin{align}
\delta^L &amp;= (\al-y) \cdot \sigma^{\color{red}{\prime}}(\zl)  \tag{BP1} \\
\deltax{\ell} &amp;= \wx{l+1} \delta^{\ell+1} \cdot \sigma^{\color{red}{\prime}}(\zx{\ell}) \tag{BP2} \\
\dcdb{\ell} &amp;= \delta^{\ell} \tag{BP3} \\
\dcdw{\ell} &amp;= \ax{\ell-1}\delta^{\ell} \tag{BP4}
\end{align}
</span></p>
<p>where <span class="math inline">\ell</span> is any layer.</p>
</div>
<p>Some explanation of each:</p>
<ul>
<li>BP1 defines the error for the <strong>last layer</strong>. This is the first thing we calculate to perform backprop.</li>
<li>BP2 defines the error for <strong>any layer</strong> <span class="math inline">\ell</span> in terms of the error at the next level <span class="math inline">\ell+1</span>.</li>
<li>BP3 defines the bias update at <strong>any layer</strong> <span class="math inline">\ell</span></li>
<li>BP4 defines the weight update at <strong>any layer</strong> <span class="math inline">\ell</span></li>
</ul>
<p>BP2 and BP4 have <strong>interesting interpretations</strong> which will become more salient when we get to matrices/vectors, but that we can first describe here:</p>
<hr>
<p>Lets start with BP2:</p>
<p><span class="math display">
\deltax{\ell} = \wx{l+1} \delta^{\ell+1} \cdot \sigma^{\color{red}{\prime}}(\zx{\ell})
</span></p>
<p>We can think of this as <strong>the</strong> backprop equation, as it clearly captures the backward flow of errors from outputs back through the network!</p>
<p>You can also think of BP2 as saying the following: Just as <span class="math inline">\wx{\ell+1}</span> acted on the <strong>activation</strong> of the <strong>previous</strong> layer <span class="math inline">\ax{l}</span> to <strong>bring it forward</strong> to the current layer, it acts on the <strong>error</strong> of the <strong>current layer</strong> to <strong>bring it back</strong> to the previous layer!</p>
<p><strong>Note:</strong> This will become a bit more intutive when we get to matrices, as we will see this equation will have a weight matrix in the forward direction, and a <strong>transpose</strong> in the reverse direction</p>
<hr>
<p>Lets rewrite BP4 as:</p>
<p><span class="math display">
\dcdw{} = \color{green}{a_{in}}\delta_{out}
</span></p>
<p>where it‚Äôs understood that <span class="math inline">\color{green}{a_{in}}</span> is the activation of the neuron input to the weight <span class="math inline">w</span>, and <span class="math inline">\delta_{out}</span> is the <strong>error</strong> of the neuron output from the weight <span class="math inline">w</span>.</p>
<p>Clearly, when <span class="math inline">\color{green}{a_{in}}</span> is small, then <span class="math inline">\dcdw{}</span> is small. This is another way to say that this weight <strong>learns slowly</strong>, meaning that it‚Äôs not changing much during gradient descent. In other words, one consequence of BP4 is that weights output from low-activation neurons learn slowly.</p>
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: What else can we say? Think about what these equations are telling us!</p>
</div>
</section>
<section id="implementation-2" class="level3">
<h3 class="anchored" data-anchor-id="implementation-2">Implementation</h3>
<p>We‚Äôre now ready to go ahead and implement backprop ourselves!</p>
<div class="info">
<p>üí™üèΩ<strong>Exercise</strong>: Implement this procedure on our toy example!</p>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backprop_1D():</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># EDIT HERE</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 0: fix all FP values: a1, a2, a3 and all parameters</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: calculate delta_L</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: Once we‚Äôve implemented these equations and calculated our partials all the way back through our network, what do we do?!</p>
</div>
</section>
</section>
<section id="multidimensional-case" class="level2">
<h2 class="anchored" data-anchor-id="multidimensional-case">Multidimensional case</h2>
<p>Now things are going to get <strong>interesting</strong>! In our simple toy example, we only had one neuron per layer. This means in our resulting equations everything was just scalar values.</p>
<section id="latex-definitions-1" class="level3">
<h3 class="anchored" data-anchor-id="latex-definitions-1">Latex definitions</h3>
This cell contains more latex definitions. $
<p>$<strong>Note</strong>: remember to run it!</p>
</section>
<section id="notation-1" class="level3">
<h3 class="anchored" data-anchor-id="notation-1">Notation</h3>
<p>Now, we have to be a bit more careful! Lets remind ourselves of the notation we used above:</p>
<p><img src="figures/nd-net.png" class="img-fluid"></p>
<p>Here, we‚Äôve labeled each neuron by its activation, layer number, and number in layer. For example, <span class="math inline">\color{green}{a_j^L}</span> is neuron <span class="math inline">{j}</span>, in layer <span class="math inline">L</span>, and <span class="math inline">\color{blue}{w_{{j},{k}}^L}</span> is the weight that connects neuron <span class="math inline">{k}</span> in layer <span class="math inline">L-1</span> to neuron <span class="math inline">{j}</span> in layer <span class="math inline">L</span>.</p>
<p>Now, the <strong>cost</strong> of a single example <span class="math inline">\czero</span> is a sum over the activations of all neurons in the last layer:</p>
<p><span class="math display">
\czero = \frac{1}{2}\sum_{{j}} (\color{green}{a_j^L}- y_{{j}})^2
</span></p>
<p>which in vector notation is:</p>
<p><span class="math display">
\czero = \frac{1}{2}\|\color{green}{\mathbf{a}}^L - \mathbf{y}\|^2
</span></p>
<p>Before going into vector/matrix notation, lets still deal with an <strong>individual weight</strong> in this network: <span class="math inline">\color{blue}{w_{{j},{k}}^L}</span>, and write out our backprop equation for this single weight:</p>
<p><span class="math display">
\frac{\dc}{\color{blue}{\partial w_{{j},{k}}^L}}= \frac{\color{orange}{\partial z_j^L}}{\color{blue}{\partial w_{{j},{k}}^L}}\frac{\color{green}{\partial \color{green}{a_j^L}}}{\color{orange}{\partial z_j^L}}\frac{\dc}{\color{green}{\partial \color{green}{a_j^L}}}
</span></p>
<p>For completion, lets write expressions for each of its pieces.</p>
<p>For <span class="math inline">\frac{\dc}{\color{green}{\partial \color{green}{a_j^L}}}</span>:</p>
<p><span class="math display">
\begin{align}
\czero &amp;= \frac{1}{2}\sum_{{j}} (\color{green}{a_j^L}- y_{{j}})^2 \\
\frac{\dc}{\color{green}{\partial \color{green}{a_j^L}}}&amp;= (\color{green}{a_j^L}- y_j)
\end{align}
</span></p>
<p>For <span class="math inline">\frac{\color{green}{\partial \color{green}{a_j^L}}}{\color{orange}{\partial z_j^L}}</span>: <span class="math display">
\begin{align}
\frac{\color{green}{\partial \color{green}{a_j^L}}}{\color{orange}{\partial z_j^L}}= \sigma(\color{orange}{z_j^L})(1-\color{orange}{z_j^L})
\end{align}
</span></p>
<p>For <span class="math inline">\frac{\color{orange}{\partial z_j^L}}{\color{blue}{\partial w_{{j},{k}}^L}}</span>: <span class="math display">
\begin{align}
\color{orange}{z_j^L}&amp;= \ldots + \color{blue}{w_{{j},{k}}^L}\color{green}{a_k^{L-1}} + \ldots \\
\frac{\color{orange}{\partial z_j^L}}{\color{blue}{\partial w_{{j},{k}}^L}}&amp; =  \color{green}{a_k^{L-1}}
\end{align}
</span></p>
<p><strong>Note:</strong> Pay particular attention to the <span class="math inline">k</span> index in the equation above! Remember, our weights are applied to the activations of the previous layer, which we are using <span class="math inline">k</span> to index!</p>
<p>Just as we did above, we can define a <strong>neuron specific error measure</strong> for neuron <span class="math inline">j</span> in layer <span class="math inline">L</span> as:</p>
<p><span class="math display">
\begin{align}
\delta_{j}^L &amp;= \frac{\dc}{\color{orange}{\partial}\color{orange}{z_j^L}} \\
             &amp;= \frac{\color{green}{\partial \color{green}{a_j^L}}}{\color{orange}{\partial z_j^L}}\frac{\dc}{\color{green}{\partial \color{green}{a_j^L}}}\\
             &amp;= \sigma(\color{orange}{z_j^L})\sigma(1-\color{orange}{z_j^L}) (\color{green}{a_j^L}- y_j)
\end{align}
</span></p>
<p><strong>Note</strong>: pay particular attention to the <span class="math inline">j</span> index above! This is with respect to the current layer, which we are using <span class="math inline">j</span> to index!</p>
<p>This lets us rewrite the above as:</p>
<p><span class="math display">
\frac{\dc}{\color{blue}{\partial w_{{j},{k}}^L}}= \color{green}{a_k^{L-1}} \delta_{j}^L
</span></p>
<p>Above we‚Äôve given the equation for how the cost changes with a specific weight in our multi dimensional network. Notice how close it is to the 1D case!</p>
<div class="info">
<p>üí™üèΩ<strong>Exercise</strong>: Give the equation for <span class="math inline">\frac{\dc}{\color{purple}{\partial b^L_j}}</span></p>
</div>
<p>Now, lets look at <span class="math inline">\frac{\dc}{\color{green}{\partial \color{green}{a_k^{L-1}}}}</span>. This is asking how the cost varies when we change the activation of a neuron <span class="math inline">k</span> in layer <span class="math inline">L-1</span>.</p>
<p>Just using pattern matching, we might <strong>want</strong> to write:</p>
<p><span class="math display">
\frac{\dc}{\color{green}{\partial \color{green}{a_k^{L-1}}}} \stackrel{\color{red}{?}}{=} \color{blue}{w_{{j},{k}}^L}\delta_j^L
</span></p>
<p>But <strong>this is incorrect!</strong> To see why, lets draw a picture showing how nueron <span class="math inline">k</span> in layer <span class="math inline">L-1</span> affects the current layer (and therefore the cost):</p>
<p><img src="figures/2d-ak.png" class="img-fluid"></p>
<p>Aha! The activation of neuron <span class="math inline">k</span> in layer <span class="math inline">L-1: \color{green}{a_k^{L-1}}</span> does <strong>not</strong> just flow through neuron <span class="math inline">j</span> in layer <span class="math inline">L</span>, it flows through <strong>every single neuron in layer <span class="math inline">L</span>!</strong></p>
<p>So if we want to account for the effect a small nudge to this neuron‚Äôs activation value has on our cost, we need to account for <strong>each neuron in layer <span class="math inline">L</span></strong>, and each associated weight!</p>
<p>Our correct equation is then:</p>
<p><span class="math display">
\frac{\dc}{\color{green}{\partial \color{green}{a_k^{L-1}}}} = \sum_j \left[ \color{blue}{w_{{j},{k}}^L}\delta_j^L \right]
</span></p>
</section>
<section id="implementation-3" class="level3">
<h3 class="anchored" data-anchor-id="implementation-3">Implementation</h3>
<p>Ok! We‚Äôve gone to the multidimensional case, but still given equations for each individual parameter. So really, its almost exactly as it was in the 1D case!</p>
<div class="info">
<p>üí™üèΩ<strong>Exercise</strong>: Implement the backprop equations above!</p>
</div>
<p>As a hint, some <strong>pseudo-code</strong> for the implementation would be:</p>
<pre><code># skipping setup
# skipping forward pass

for sample in inputs:
    # last layer
    for neuron_j in layer[-1].neurons:
        delta_j = # calculate according to formula above
        dc_dwj = # calculate according to formula above
        dc_dbj = # calculate according to formula above
        dc_dak^{L-1} = # calculate according to formula above
    
    # other layers
    for layer in layers-1:
        for neuron_j in layer[l].neurons:
            delta_j = # calculate according to formula above
            dc_dwj = # calculate according to formula above
            dc_dbj = # calculate according to formula above
            dc_dak^{L-1} = # calculate according to formula above
    </code></pre>
</section>
</section>
<section id="n-d-revisited---vector-notation" class="level2">
<h2 class="anchored" data-anchor-id="n-d-revisited---vector-notation">N-D Revisited - Vector Notation</h2>
<p>Above, we gave specific equations for each parameter in a multi dimensional network. This will work, as your implementation should prove!</p>
<p>However, it leaves much to be desired in terms of efficiency, and conciseness, and it doesn‚Äôt allow us to make full use of the magic of our numerical linear algebra libraries!</p>
<p>Lets begin our analysis with our error vector <span class="math inline">\delta_{j}^L</span>:</p>
<p><span class="math display">
\begin{align}
\delta_{j}^L &amp;= \frac{\dc}{\color{orange}{\partial}\color{orange}{z_j^L}} \\
             &amp;= \frac{\color{green}{\partial \color{green}{a_j^L}}}{\color{orange}{\partial z_j^L}}\frac{\dc}{\color{green}{\partial \color{green}{a_j^L}}}\\
             &amp;= \sigma(\color{orange}{z_j^L})\sigma(1-\color{orange}{z_j^L}) (\color{green}{a_j^L}- y_j) \\
             &amp;= \sigma^{\color{red}{\prime}}(\color{orange}{z_j^L}) (\color{green}{a_j^L}- y_j)
\end{align}
</span> where we brought back the <span class="math inline">\sigma^{\color{red}{\prime}}(\cdot)</span> notation.</p>
<p>We would like to write a <strong>vector</strong> for our error <span class="math inline">\delta^L</span>, where each component is:</p>
<p><span class="math display">
\delta^L = \begin{bmatrix}
\sigma^{\color{red}{\prime}}(\color{orange}{z_1^L}) (\color{green}{a_1^L} - y_1) \\
\cdots \\
\sigma^{\color{red}{\prime}}(\color{orange}{z_j^L}) (\color{green}{a_j^L} - y_j)\\
\cdots
\end{bmatrix}
</span></p>
<p>This is exactly the definition of element-wise product of the following vectors: <span class="math inline">\color{orange}{\mathbf{z}^L},\color{green}{\mathbf{a}^L}</span> and <span class="math inline">\mathbf{y}</span>:</p>
<p><span class="math display">
\delta^L = \sigma^{\color{red}{\prime}}(\color{orange}{\mathbf{z}^L}) \odot (\color{green}{\mathbf{a}^L} - \mathbf{y})
</span></p>
<p>where we introduce another piece of notation:</p>
<div class="info">
<p>üìñ<strong>Definition</strong>: The <strong>Hadamard product</strong> between two vectors is the element-wise product:</p>
<p><span class="math display">
\mathbf{a} \odot \mathbf{b} =
\begin{bmatrix}
a_1 \cdot b_1 \\
\cdots \\
a_n \cdot b_n
\end{bmatrix}
</span></p>
</div>
<p>Lets define a <strong>function</strong> to implement this element-wise product:</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> hadamard(a,b):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> np.zeros_like(a)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(a.shape[<span class="dv">0</span>]):</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>        result[i] <span class="op">=</span> a[i] <span class="op">*</span> b[i]</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets test it!</p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>a,b <span class="op">=</span> np.random.randint(<span class="dv">1</span>,<span class="dv">10</span>,(<span class="dv">3</span>,<span class="dv">1</span>)),np.random.randint(<span class="dv">1</span>,<span class="dv">10</span>,(<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>a,b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>(array([[9],
        [9],
        [8]]),
 array([[8],
        [5],
        [1]]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>hadamard(a,b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>array([[72],
       [45],
       [ 8]])</code></pre>
</div>
</div>
<p>It works!</p>
<p><strong>However</strong>, Python actually already does this element-wise product for us! Using the <code>*</code> operator!</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>a<span class="op">*</span>b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>array([[72],
       [45],
       [ 8]])</code></pre>
</div>
</div>
<p>So we didn‚Äôt have to implement our own. But thats ok, because we know how to. Lets get back to business!</p>
<hr>
<p>We‚Äôve rewritten BP1 in vector notation. Lets now focus on BP2: <strong>moving the error ‚Äúbackward‚Äù</strong>.</p>
<p>Well, we can probably already guess its going to involve the hadamard product with <span class="math inline">\sigma^{\color{red}{\prime}}(\cdot)</span> as it did previously. We just don‚Äôt know with what yet!</p>
<p><span class="math display">
\begin{align}
\delta^\ell = ?? \odot \sigma^{\color{red}{\prime}}(\mathbf{\zx{\ell}})
\end{align}
</span></p>
<p>Well, lets try to reason through it from ‚Äúfirst principles‚Äù (<strong>recall</strong>: this means <strong>make the sizes work!</strong>).</p>
<p><span class="math inline">\delta^\ell</span> is a vector, which should be of size <span class="math inline">k</span>, the number of elements in layer <span class="math inline">\ell</span>. We know its going to be formed with the <strong>weight matrix</strong> and <span class="math inline">\delta^{\ell+1}</span>, so lets write their sizes:</p>
<p><span class="math display">
\color{blue}{W^{\ell+1}}: (j \times k), \quad\quad \delta^{\ell+1}: (j \times 1)
</span></p>
<p>How do we multiply these out to get a vector of size <span class="math inline">k \times 1</span> out?</p>
<p>üßê</p>
<p>Indeed! We need to <strong>transpose</strong> our weight matrix, and then take the usual matrix-vector product with <span class="math inline">\delta^{\ell+1}</span>!</p>
<p>Now we can write the vectorized equation for the error at any layer:</p>
<div class="info">
<p>üìñ<strong>Vectorized BP2</strong>: The error at <strong>any</strong> layer <span class="math inline">\ell</span> can be written as as function of the next layer as:</p>
<p><span class="math display">
\delta^{\ell} = (\color{blue}{W^{\ell+1}})^T \delta^{\ell+1} \odot \sigma^{\color{red}{\prime}}(\mathbf{\zx{\ell}})
</span></p>
</div>
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: What can we say about our previous interpretation in light of this new equation?</p>
</div>
<p>Lets quickly tackle BP3:</p>
<div class="info">
<p>üìñ<strong>Vectorized-BP3</strong>:</p>
<p><span class="math display">
\frac{\dc}{\color{purple}{\partial \mathbf{b}^\ell}} = \delta^\ell
</span></p>
</div>
<p>Well, that was easy! Its just as it was above, but as a vector.</p>
<p>Hm. Now we get to the tough one: <strong>BP4</strong>. This is now a partial derivative of our cost <strong>with respect to weight matrix</strong>!</p>
<p>Lets start at the last layer. <span class="math inline">\color{blue}{W^L}</span> is of size <span class="math inline">j \times k</span>, where <span class="math inline">j</span> is the size of the last layer.</p>
<p>So, we should expect its <strong>gradient</strong> to be of the same size: <strong>a matrix</strong>. If its not clear why, just imagine how were going to use this partial. We‚Äôre going to add it the current value of <span class="math inline">\color{blue}{W}</span> in gradient descent, so it better be of the same size!</p>
<p>To form this matrix, we have <span class="math inline">\color{green}{\mathbf{a}^{L-1}}</span>, which is of size <span class="math inline">k \times 1</span>, the size of the previous layer, and <span class="math inline">\delta^L</span>, which is of size <span class="math inline">j \times 1</span>.</p>
<p>So how do we form this matrix using these two vectors? We take the <strong>outer product</strong>:</p>
<p><span class="math display">
\delta^L(\color{green}{\mathbf{a}^{L-1}})^T
</span></p>
<p>Lets make sure the sizes work!</p>
<p><span class="math display">
(j \times 1)(1 \times k) = (j \times k)
</span></p>
<p>Perfect!</p>
<div class="info">
<p>üìñ<strong>Vectorized BP4</strong>:</p>
<p><span class="math display">
\frac{\dc}{ \color{blue}{ \partial \mathbf{W^\ell} } } = \delta^L(\color{green}{\mathbf{a}^{L-1}})^T
</span></p>
</div>
<p>Finally! Lets give fully vectorized forms of the batchprop equations:</p>
<hr>
<section id="f.f.b.e---vectorized" class="level3">
<h3 class="anchored" data-anchor-id="f.f.b.e---vectorized">F.F.B.E - Vectorized‚Ñ¢</h3>
<p>Lets cap this off by writing <strong>vectorized versions</strong> of the four fundamental backpropagation equations:</p>
<div class="info">
<p><strong>üìñDefinition</strong>: <strong>Vectorized</strong> versions of the <strong>Four Fundamental Backpropagation Equations‚Ñ¢</strong> are given by:</p>
<p><span class="math display">
\begin{align}
\delta^L &amp;= (\color{green}{\mathbf{a}^L} - \mathbf{y})  \odot  \sigma^{\color{red}{\prime}}(\color{orange}{\mathbf{z}^L}) \tag{BP1}\\
\delta^{\ell} &amp;= (\color{blue}{W^{\ell+1}})^T \delta^{\ell+1} \odot \sigma^{\color{red}{\prime}}(\mathbf{\zx{\ell}})\tag{BP2} \\
\frac{\dc}{\color{purple}{\partial \mathbf{b}^\ell}} &amp;= \delta^\ell \tag{BP3} \\
\frac{\dc}{ \color{blue}{ \partial \mathbf{W^\ell} } } &amp;= \delta^\ell(\color{green}{\mathbf{a}^{\ell-1}})^T \tag{BP4}
\end{align}
</span></p>
</div>
<hr>
</section>
<section id="implementation-4" class="level3">
<h3 class="anchored" data-anchor-id="implementation-4">Implementation</h3>
<p>Wow! Now we‚Äôve really come a long way!</p>
<div class="info">
<p>üí™üèΩ<strong>Exercise</strong>: Implement the backprop equations above!</p>
</div>
As you‚Äôre implementing this, think about the following:
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: How do you deal with multiple samples?</p>
</div>
<p>Initialize our toy problem as always:</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>X,y <span class="op">=</span> toy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since we‚Äôre still dealing with a <strong>flat implementation</strong>, lets go ahead and initialize our weights and biases here:</p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weight matrices - notice the dimensions</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>W_1_init <span class="op">=</span> np.random.randn(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>W_2_init <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">3</span>)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>W_3_init <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> [W_1_init,W_2_init,W_3_init]</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a><span class="co"># and our biases</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>b_1_init <span class="op">=</span> np.random.randn(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>b_2_init <span class="op">=</span> np.random.randn(<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>b_3_init <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>biases <span class="op">=</span> [b_1_init,b_2_init,b_3_init]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can use the feedforward batch implementation we gave above:</p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feedforward_batch(X, weights, biases):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># assume inputs is of shape Nxd</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    W_1,W_2,W_3 <span class="op">=</span> weights[<span class="dv">0</span>],weights[<span class="dv">1</span>],weights[<span class="dv">2</span>]</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    b_1,b_2,b_3 <span class="op">=</span> biases[<span class="dv">0</span>],biases[<span class="dv">1</span>],biases[<span class="dv">2</span>]</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    activities <span class="op">=</span> []</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    activations <span class="op">=</span> []</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">NOTE</span><span class="co"> - there is no for loop here! </span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># treat activations like layer_0 activations:</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>    activations.append(X.T)</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># layer 1</span></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>    z_1 <span class="op">=</span> W_1.dot(X.T) <span class="op">+</span> b_1</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>    a_1 <span class="op">=</span> sigmoid(z_1)</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>    activities.append(z_1)</span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>    activations.append(np.squeeze(np.asarray(a_1)))</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># layer 2</span></span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>    z_2 <span class="op">=</span> W_2.dot(a_1)<span class="op">+</span>b_2</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>    a_2 <span class="op">=</span> sigmoid(z_2)</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>    activities.append(z_2)</span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>    activations.append(np.squeeze(np.asarray(a_2)))</span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output layer</span></span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a>    z_3 <span class="op">=</span> W_3.dot(a_2)<span class="op">+</span>b_3</span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> sigmoid(z_3)</span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a>    activities.append(z_3)</span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a>    activations.append(np.asarray(y_hat))</span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activities, activations</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets actually feed-forward our entire dataset:</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>activities, activations <span class="op">=</span> feedforward_batch(X,weights,biases)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets check the sizes of these:</p>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(activities[<span class="dv">0</span>].shape,activities[<span class="dv">1</span>].shape,activities[<span class="dv">2</span>].shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(3, 1000) (2, 1000) (1, 1000)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(activations[<span class="dv">0</span>].shape,activations[<span class="dv">1</span>].shape,</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>      activations[<span class="dv">2</span>].shape, activations[<span class="dv">3</span>].shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(1, 1000) (3, 1000) (2, 1000) (1, 1000)</code></pre>
</div>
</div>
<p><strong>Note</strong>: We‚Äôre calling our input our first activity, which is why there is one extra.</p>
<p>Now we‚Äôre ready to run backprop <strong>on each sample</strong> in our dataset, to generate bias and weight updates for each sample.</p>
<p>Lets start with a sigmoid prime function we will need:</p>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid_prime(z):</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sigmoid(z)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>sigmoid(z))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>plt.plot(sigmoid(np.linspace(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>)))<span class="op">;</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>plt.plot(sigmoid_prime(np.linspace(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>)))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="nn_files/figure-html/cell-53-output-1.png" width="571" height="411"></p>
</div>
</div>
<p>Lets continue our work:</p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>N,d <span class="op">=</span> inputs.shape</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>weight_updates <span class="op">=</span> []</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>bias_updates <span class="op">=</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>layer_num <span class="op">=</span> <span class="bu">len</span>(activations)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>weight_updates <span class="op">=</span> []</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>bias_updates <span class="op">=</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now our equations are above are only for one sample! So lets pick one and proceed:</p>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>sample_idx <span class="op">=</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ok, so lets run backprop on that one sample, by starting with our last layer:</p>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co">########################################</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="co">#           Last Level</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="co">########################################</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="co"># get last layer specific variables</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>z_L <span class="op">=</span> activities[<span class="op">-</span><span class="dv">1</span>][:,sample_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>a_L <span class="op">=</span> activations[<span class="op">-</span><span class="dv">1</span>][:,sample_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>a_L_minus_1 <span class="op">=</span> activations[<span class="op">-</span><span class="dv">2</span>][:,sample_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>(z_L.shape,a_L.shape,a_L_minus_1.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>((1, 1), (1, 1), (2, 1))</code></pre>
</div>
</div>
<p>Are these what we expect?</p>
<p>Lets use them in the formulas we gave above!</p>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate delta_L for the last level</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>delta_L <span class="op">=</span> (a_L<span class="op">-</span>y[sample_idx])<span class="op">*</span>sigmoid_prime(z_L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>delta_L.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>(1, 1)</code></pre>
</div>
</div>
<p>Lets make a list to store all these delta values!</p>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>deltas <span class="op">=</span> []</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>deltas.append(delta_L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now lets calculate the bias update:</p>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate bias update</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>dc_db_l <span class="op">=</span> delta_L</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>bias_updates <span class="op">=</span> [dc_db_l] <span class="op">+</span> bias_updates</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>dc_db_l.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>(1, 1)</code></pre>
</div>
</div>
<p>And the weight update:</p>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calcualte weight updates</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>dc_dw_l <span class="op">=</span> delta_L.dot((a_L_minus_1).T)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>weight_updates <span class="op">=</span> [dc_dw_l] <span class="op">+</span> weight_updates</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>dc_dw_l.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>(1, 2)</code></pre>
</div>
</div>
<p>Wow! We‚Äôve run backprop across our last layer! Now all we have to do, is apply this to the rest of our layers!</p>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co">########################################</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="co"># loop through each layer, from 2 to L-1</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="co">######################################## </span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,layer_num):</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># using level as a **negative index**</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> <span class="op">-</span>layer</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># uncomment this print statement </span></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to help you understand how negative indexing works</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Loop variable: </span><span class="sc">{</span>layer<span class="sc">}</span><span class="ss">, l=</span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">, corresponding to layer: l</span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate delta_l for each layer</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>    z_l <span class="op">=</span> activities[l][:,sample_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>    a_l <span class="op">=</span> activations[l][:,sample_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>    delta_l <span class="op">=</span> weights[l<span class="op">+</span><span class="dv">1</span>].T.dot(deltas[l<span class="op">+</span><span class="dv">1</span>]) <span class="op">*</span> sigmoid_prime(z_l)</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>    deltas <span class="op">=</span> [delta_l] <span class="op">+</span> deltas</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate bias update</span></span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a>    dc_db_l <span class="op">=</span> delta_l</span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>    bias_updates <span class="op">=</span> [dc_db_l] <span class="op">+</span> bias_updates</span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calcualte weight updates</span></span>
<span id="cb78-23"><a href="#cb78-23" aria-hidden="true" tabindex="-1"></a>    a_l_minus_1 <span class="op">=</span> activations[l<span class="op">-</span><span class="dv">1</span>][:,sample_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb78-24"><a href="#cb78-24" aria-hidden="true" tabindex="-1"></a>    dc_dw_l <span class="op">=</span> delta_l.dot((a_l_minus_1).T)</span>
<span id="cb78-25"><a href="#cb78-25" aria-hidden="true" tabindex="-1"></a>    weight_updates <span class="op">=</span> [dc_dw_l] <span class="op">+</span> weight_updates</span>
<span id="cb78-26"><a href="#cb78-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb78-27"><a href="#cb78-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'=========Layer:</span><span class="sc">{</span>layer_num<span class="op">+</span>l<span class="sc">}</span><span class="ss">========='</span>)</span>
<span id="cb78-28"><a href="#cb78-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Using negative index: </span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb78-29"><a href="#cb78-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'z_l:</span><span class="sc">{</span>z_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l:</span><span class="sc">{</span>a_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l_minus_1:</span><span class="sc">{</span>a_l_minus_1<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb78-30"><a href="#cb78-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'delta_l:</span><span class="sc">{</span>delta_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, dc_db_l:</span><span class="sc">{</span>dc_db_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">,dc_dw_l:</span><span class="sc">{</span>dc_dw_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb78-31"><a href="#cb78-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loop variable: 2, l=-2, corresponding to layer: l-2
=========Layer:2=========
Using negative index: -2
z_l:(2, 1), a_l:(2, 1), a_l_minus_1:(3, 1)
delta_l:(2, 1), dc_db_l:(2, 1),dc_dw_l:(2, 3)

Loop variable: 3, l=-3, corresponding to layer: l-3
=========Layer:1=========
Using negative index: -3
z_l:(3, 1), a_l:(3, 1), a_l_minus_1:(1, 1)
delta_l:(3, 1), dc_db_l:(3, 1),dc_dw_l:(3, 1)
</code></pre>
</div>
</div>
<p>All thats left to do, is wrap this up in a reusable function!</p>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backprop_1_example(activities,activations,y,sample_idx,quiet<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>    layer_num <span class="op">=</span> <span class="bu">len</span>(activations)</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>    weight_updates <span class="op">=</span> []</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>    bias_updates <span class="op">=</span> []</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">#           Last Level</span></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get last layer specific variables</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>    a_L <span class="op">=</span> activations[<span class="op">-</span><span class="dv">1</span>][:,sample_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>    a_L_minus_1 <span class="op">=</span> activations[<span class="op">-</span><span class="dv">2</span>][:,sample_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>    z_L <span class="op">=</span> activities[<span class="op">-</span><span class="dv">1</span>][:,sample_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate delta_L for the last level</span></span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>    delta_L <span class="op">=</span> (a_L<span class="op">-</span>y)<span class="op">*</span>sigmoid_prime(z_L)</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a>    deltas <span class="op">=</span> []</span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>    deltas.append(delta_L)</span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate bias update</span></span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>    dc_db_L <span class="op">=</span> delta_L</span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a>    bias_updates <span class="op">=</span> [dc_db_L] <span class="op">+</span> bias_updates</span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calcualte weight updates</span></span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a>    dc_dw_L <span class="op">=</span> delta_L.dot((a_L_minus_1).T)</span>
<span id="cb80-25"><a href="#cb80-25" aria-hidden="true" tabindex="-1"></a>    weight_updates <span class="op">=</span> [dc_dw_L] <span class="op">+</span> weight_updates</span>
<span id="cb80-26"><a href="#cb80-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb80-27"><a href="#cb80-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> quiet:</span>
<span id="cb80-28"><a href="#cb80-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'=========Layer:</span><span class="sc">{</span>layer_num<span class="op">+-</span><span class="dv">1</span><span class="sc">}</span><span class="ss">========='</span>)</span>
<span id="cb80-29"><a href="#cb80-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Using negative index: -1'</span>)</span>
<span id="cb80-30"><a href="#cb80-30" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'z_l:</span><span class="sc">{</span>z_L<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l:</span><span class="sc">{</span>a_L<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l_minus_1:</span><span class="sc">{</span>a_L_minus_1<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb80-31"><a href="#cb80-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'delta_l:</span><span class="sc">{</span>delta_L<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, dc_db_l:</span><span class="sc">{</span>dc_db_L<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">,dc_dw_l:</span><span class="sc">{</span>dc_dw_L<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb80-32"><a href="#cb80-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb80-33"><a href="#cb80-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb80-34"><a href="#cb80-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb80-35"><a href="#cb80-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loop through each layer, from 2 to L-1</span></span>
<span id="cb80-36"><a href="#cb80-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">######################################## </span></span>
<span id="cb80-37"><a href="#cb80-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,layer_num):</span>
<span id="cb80-38"><a href="#cb80-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># using level as a **negative index**</span></span>
<span id="cb80-39"><a href="#cb80-39" aria-hidden="true" tabindex="-1"></a>        l <span class="op">=</span> <span class="op">-</span>layer</span>
<span id="cb80-40"><a href="#cb80-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb80-41"><a href="#cb80-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># uncomment this print statement </span></span>
<span id="cb80-42"><a href="#cb80-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to help you understand how negative indexing works</span></span>
<span id="cb80-43"><a href="#cb80-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(f"Loop variable: {layer}, l={l}, corresponding to layer: l{l}")</span></span>
<span id="cb80-44"><a href="#cb80-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-45"><a href="#cb80-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate delta_l for each layer</span></span>
<span id="cb80-46"><a href="#cb80-46" aria-hidden="true" tabindex="-1"></a>        a_l <span class="op">=</span> activations[l][:,sample_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb80-47"><a href="#cb80-47" aria-hidden="true" tabindex="-1"></a>        z_l <span class="op">=</span> activities[l][:,sample_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb80-48"><a href="#cb80-48" aria-hidden="true" tabindex="-1"></a>        delta_l <span class="op">=</span> weights[l<span class="op">+</span><span class="dv">1</span>].T.dot(deltas[l<span class="op">+</span><span class="dv">1</span>]) <span class="op">*</span> sigmoid_prime(z_l)</span>
<span id="cb80-49"><a href="#cb80-49" aria-hidden="true" tabindex="-1"></a>        deltas <span class="op">=</span> [delta_l] <span class="op">+</span> deltas</span>
<span id="cb80-50"><a href="#cb80-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-51"><a href="#cb80-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate bias update</span></span>
<span id="cb80-52"><a href="#cb80-52" aria-hidden="true" tabindex="-1"></a>        dc_db_l <span class="op">=</span> delta_l</span>
<span id="cb80-53"><a href="#cb80-53" aria-hidden="true" tabindex="-1"></a>        bias_updates <span class="op">=</span> [dc_db_l] <span class="op">+</span> bias_updates</span>
<span id="cb80-54"><a href="#cb80-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-55"><a href="#cb80-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calcualte weight updates</span></span>
<span id="cb80-56"><a href="#cb80-56" aria-hidden="true" tabindex="-1"></a>        a_l_minus_1 <span class="op">=</span> activations[l<span class="op">-</span><span class="dv">1</span>][:,sample_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb80-57"><a href="#cb80-57" aria-hidden="true" tabindex="-1"></a>        dc_dw_l <span class="op">=</span> delta_l.dot((a_l_minus_1).T)</span>
<span id="cb80-58"><a href="#cb80-58" aria-hidden="true" tabindex="-1"></a>        weight_updates <span class="op">=</span> [dc_dw_l] <span class="op">+</span> weight_updates</span>
<span id="cb80-59"><a href="#cb80-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb80-60"><a href="#cb80-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> quiet:</span>
<span id="cb80-61"><a href="#cb80-61" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'=========Layer:</span><span class="sc">{</span>layer_num<span class="op">+</span>l<span class="sc">}</span><span class="ss">========='</span>)</span>
<span id="cb80-62"><a href="#cb80-62" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'Using negative index: </span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb80-63"><a href="#cb80-63" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'z_l:</span><span class="sc">{</span>z_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l:</span><span class="sc">{</span>a_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l_minus_1:</span><span class="sc">{</span>a_l_minus_1<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb80-64"><a href="#cb80-64" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'delta_l:</span><span class="sc">{</span>delta_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, dc_db_l:</span><span class="sc">{</span>dc_db_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">,dc_dw_l:</span><span class="sc">{</span>dc_dw_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb80-65"><a href="#cb80-65" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>()</span>
<span id="cb80-66"><a href="#cb80-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb80-67"><a href="#cb80-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weight_updates, bias_updates</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now have the following: * <code>feedforward_batch</code>: a function which feeds entire <strong>batches</strong> through our network * <code>backprop_1_example</code>: a function which backpropagates <strong>the erorr associated with a single example</strong> through the network</p>
<p>Lets now revisit:</p>
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: How do you deal with multiple samples?</p>
</div>
<p>And try to think of how we can implement this algorithm:</p>
</section>
</section>
<section id="more-interesting-example" class="level2">
<h2 class="anchored" data-anchor-id="more-interesting-example">More interesting example</h2>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the dataset</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> sklearn.datasets.make_circles(</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>, shuffle<span class="op">=</span><span class="va">False</span>, factor<span class="op">=</span><span class="fl">0.3</span>, noise<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate the red and blue samples for plotting</span></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>x_red <span class="op">=</span> X[y<span class="op">==</span><span class="dv">0</span>]</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>x_blue <span class="op">=</span> X[y<span class="op">==</span><span class="dv">1</span>]</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a><span class="co"># correct size of y</span></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'shape of X: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(X.shape))</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'shape of y: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(y.shape))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>shape of X: (1000, 2)
shape of y: (1000, 1)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot both classes on the x1, x2 plane</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x_red[:,<span class="dv">0</span>], x_red[:,<span class="dv">1</span>], <span class="st">'r*'</span>, </span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="st">'class: red star'</span>, alpha<span class="op">=</span><span class="fl">0.75</span>)</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x_blue[:,<span class="dv">0</span>], x_blue[:,<span class="dv">1</span>], <span class="st">'bo'</span>, </span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="st">'class: blue circle'</span>, alpha<span class="op">=</span><span class="fl">0.75</span>)</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$x_1$'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$x_2$'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>plt.axis([<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'red star vs blue circle classes in the input space'</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="nn_files/figure-html/cell-67-output-1.png" width="847" height="619"></p>
</div>
</div>
<section id="train-our-network" class="level3">
<h3 class="anchored" data-anchor-id="train-our-network">Train our Network!</h3>
<p>Now we can actually train our network using gradient descent as we have before!</p>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>eta<span class="op">=</span><span class="fl">1e-2</span></span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>dataset<span class="op">=</span>X</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>quiet<span class="op">=</span><span class="va">False</span></span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weight matrices - notice the dimensions</span></span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>W_1_init <span class="op">=</span> np.random.rand(<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>W_2_init <span class="op">=</span> np.random.rand(<span class="dv">3</span>,<span class="dv">3</span>)</span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>W_3_init <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> [W_1_init,W_2_init,W_3_init]</span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a><span class="co"># and our biases</span></span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a>b_1_init <span class="op">=</span> np.random.rand(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb84-18"><a href="#cb84-18" aria-hidden="true" tabindex="-1"></a>b_2_init <span class="op">=</span> np.random.rand(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb84-19"><a href="#cb84-19" aria-hidden="true" tabindex="-1"></a>b_3_init <span class="op">=</span> np.random.rand(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb84-20"><a href="#cb84-20" aria-hidden="true" tabindex="-1"></a>biases <span class="op">=</span> [b_1_init,b_2_init,b_3_init]</span>
<span id="cb84-21"><a href="#cb84-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-22"><a href="#cb84-22" aria-hidden="true" tabindex="-1"></a><span class="co"># network</span></span>
<span id="cb84-23"><a href="#cb84-23" aria-hidden="true" tabindex="-1"></a>W_1,W_2,W_3 <span class="op">=</span> weights[<span class="dv">0</span>],weights[<span class="dv">1</span>],weights[<span class="dv">2</span>]</span>
<span id="cb84-24"><a href="#cb84-24" aria-hidden="true" tabindex="-1"></a>b_1,b_2,b_3 <span class="op">=</span> biases[<span class="dv">0</span>],biases[<span class="dv">1</span>],biases[<span class="dv">2</span>]</span>
<span id="cb84-25"><a href="#cb84-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-26"><a href="#cb84-26" aria-hidden="true" tabindex="-1"></a><span class="co"># mini-batch params </span></span>
<span id="cb84-27"><a href="#cb84-27" aria-hidden="true" tabindex="-1"></a>num_batches <span class="op">=</span> <span class="bu">int</span>(N<span class="op">/</span>batch_size)</span>
<span id="cb84-28"><a href="#cb84-28" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb84-29"><a href="#cb84-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-30"><a href="#cb84-30" aria-hidden="true" tabindex="-1"></a><span class="co"># debugging lists</span></span>
<span id="cb84-31"><a href="#cb84-31" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> []</span>
<span id="cb84-32"><a href="#cb84-32" aria-hidden="true" tabindex="-1"></a>grad_norm_1 <span class="op">=</span> []</span>
<span id="cb84-33"><a href="#cb84-33" aria-hidden="true" tabindex="-1"></a>grad_norm_2 <span class="op">=</span> []</span>
<span id="cb84-34"><a href="#cb84-34" aria-hidden="true" tabindex="-1"></a>grad_norm_3 <span class="op">=</span> []</span>
<span id="cb84-35"><a href="#cb84-35" aria-hidden="true" tabindex="-1"></a>norm_1 <span class="op">=</span> []</span>
<span id="cb84-36"><a href="#cb84-36" aria-hidden="true" tabindex="-1"></a>norm_2 <span class="op">=</span> []</span>
<span id="cb84-37"><a href="#cb84-37" aria-hidden="true" tabindex="-1"></a>norm_3 <span class="op">=</span> []</span>
<span id="cb84-38"><a href="#cb84-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-39"><a href="#cb84-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-40"><a href="#cb84-40" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb84-41"><a href="#cb84-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-42"><a href="#cb84-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># work on current batch </span></span>
<span id="cb84-43"><a href="#cb84-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_num <span class="kw">in</span> <span class="bu">range</span>(num_batches):</span>
<span id="cb84-44"><a href="#cb84-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-45"><a href="#cb84-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get current batch</span></span>
<span id="cb84-46"><a href="#cb84-46" aria-hidden="true" tabindex="-1"></a>        batch_start <span class="op">=</span> batch_num<span class="op">*</span>batch_size</span>
<span id="cb84-47"><a href="#cb84-47" aria-hidden="true" tabindex="-1"></a>        batch_end <span class="op">=</span> (batch_num<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>batch_size</span>
<span id="cb84-48"><a href="#cb84-48" aria-hidden="true" tabindex="-1"></a>        batch <span class="op">=</span> dataset[batch_start:batch_end,:]</span>
<span id="cb84-49"><a href="#cb84-49" aria-hidden="true" tabindex="-1"></a>        batch_labels <span class="op">=</span> labels[batch_start:batch_end,:].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb84-50"><a href="#cb84-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-51"><a href="#cb84-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># feedforward on batch</span></span>
<span id="cb84-52"><a href="#cb84-52" aria-hidden="true" tabindex="-1"></a>        activities, activations <span class="op">=</span> feedforward_batch(batch, weights,biases)</span>
<span id="cb84-53"><a href="#cb84-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb84-54"><a href="#cb84-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-55"><a href="#cb84-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># setup matrices to hold gradients</span></span>
<span id="cb84-56"><a href="#cb84-56" aria-hidden="true" tabindex="-1"></a>        grad_W_1 <span class="op">=</span> np.zeros_like(W_1)</span>
<span id="cb84-57"><a href="#cb84-57" aria-hidden="true" tabindex="-1"></a>        grad_b_1 <span class="op">=</span> np.zeros_like(b_1)</span>
<span id="cb84-58"><a href="#cb84-58" aria-hidden="true" tabindex="-1"></a>        grad_W_2 <span class="op">=</span> np.zeros_like(W_2)</span>
<span id="cb84-59"><a href="#cb84-59" aria-hidden="true" tabindex="-1"></a>        grad_b_2 <span class="op">=</span> np.zeros_like(b_2)</span>
<span id="cb84-60"><a href="#cb84-60" aria-hidden="true" tabindex="-1"></a>        grad_W_3 <span class="op">=</span> np.zeros_like(W_3)</span>
<span id="cb84-61"><a href="#cb84-61" aria-hidden="true" tabindex="-1"></a>        grad_b_3 <span class="op">=</span> np.zeros_like(b_3)</span>
<span id="cb84-62"><a href="#cb84-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb84-63"><a href="#cb84-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-64"><a href="#cb84-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loop through each example in the batch</span></span>
<span id="cb84-65"><a href="#cb84-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(batch.shape[<span class="dv">0</span>]):</span>
<span id="cb84-66"><a href="#cb84-66" aria-hidden="true" tabindex="-1"></a>            current_sample <span class="op">=</span> batch[idx,:]</span>
<span id="cb84-67"><a href="#cb84-67" aria-hidden="true" tabindex="-1"></a>            current_label <span class="op">=</span> batch_labels[idx]</span>
<span id="cb84-68"><a href="#cb84-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-69"><a href="#cb84-69" aria-hidden="true" tabindex="-1"></a>            <span class="co"># get current weight and bias updates</span></span>
<span id="cb84-70"><a href="#cb84-70" aria-hidden="true" tabindex="-1"></a>            weight_updates, bias_updates <span class="op">=</span> backprop_1_example(activities,activations,current_label,idx)</span>
<span id="cb84-71"><a href="#cb84-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-72"><a href="#cb84-72" aria-hidden="true" tabindex="-1"></a>            <span class="co"># aggregate them </span></span>
<span id="cb84-73"><a href="#cb84-73" aria-hidden="true" tabindex="-1"></a>            grad_W_1 <span class="op">=</span> grad_W_1 <span class="op">+</span> weight_updates[<span class="dv">0</span>]</span>
<span id="cb84-74"><a href="#cb84-74" aria-hidden="true" tabindex="-1"></a>            grad_b_1 <span class="op">=</span> grad_b_1 <span class="op">+</span> bias_updates[<span class="dv">0</span>]</span>
<span id="cb84-75"><a href="#cb84-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-76"><a href="#cb84-76" aria-hidden="true" tabindex="-1"></a>            grad_W_2 <span class="op">=</span> grad_W_2 <span class="op">+</span> weight_updates[<span class="dv">1</span>]</span>
<span id="cb84-77"><a href="#cb84-77" aria-hidden="true" tabindex="-1"></a>            grad_b_2 <span class="op">=</span> grad_b_2 <span class="op">+</span> bias_updates[<span class="dv">1</span>]</span>
<span id="cb84-78"><a href="#cb84-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-79"><a href="#cb84-79" aria-hidden="true" tabindex="-1"></a>            grad_W_3 <span class="op">=</span> grad_W_3 <span class="op">+</span> weight_updates[<span class="dv">2</span>]</span>
<span id="cb84-80"><a href="#cb84-80" aria-hidden="true" tabindex="-1"></a>            grad_b_3 <span class="op">=</span> grad_b_3 <span class="op">+</span> bias_updates[<span class="dv">2</span>]</span>
<span id="cb84-81"><a href="#cb84-81" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb84-82"><a href="#cb84-82" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb84-83"><a href="#cb84-83" aria-hidden="true" tabindex="-1"></a>        grad_norm_1.append(np.linalg.norm(grad_W_1))</span>
<span id="cb84-84"><a href="#cb84-84" aria-hidden="true" tabindex="-1"></a>        grad_norm_2.append(np.linalg.norm(grad_W_2))</span>
<span id="cb84-85"><a href="#cb84-85" aria-hidden="true" tabindex="-1"></a>        grad_norm_3.append(np.linalg.norm(grad_W_3))</span>
<span id="cb84-86"><a href="#cb84-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-87"><a href="#cb84-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-88"><a href="#cb84-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># take your steps:</span></span>
<span id="cb84-89"><a href="#cb84-89" aria-hidden="true" tabindex="-1"></a>        W_1 <span class="op">=</span> W_1 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(grad_W_1)</span>
<span id="cb84-90"><a href="#cb84-90" aria-hidden="true" tabindex="-1"></a>        b_1 <span class="op">=</span> b_1 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(grad_b_1)</span>
<span id="cb84-91"><a href="#cb84-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-92"><a href="#cb84-92" aria-hidden="true" tabindex="-1"></a>        W_2 <span class="op">=</span> W_2 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(grad_W_2)</span>
<span id="cb84-93"><a href="#cb84-93" aria-hidden="true" tabindex="-1"></a>        b_2 <span class="op">=</span> b_2 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(grad_b_2)</span>
<span id="cb84-94"><a href="#cb84-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-95"><a href="#cb84-95" aria-hidden="true" tabindex="-1"></a>        W_3 <span class="op">=</span> W_3 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(grad_W_3)</span>
<span id="cb84-96"><a href="#cb84-96" aria-hidden="true" tabindex="-1"></a>        b_3 <span class="op">=</span> b_3 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(grad_b_3)</span>
<span id="cb84-97"><a href="#cb84-97" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb84-98"><a href="#cb84-98" aria-hidden="true" tabindex="-1"></a>        norm_1.append(np.linalg.norm(W_1))</span>
<span id="cb84-99"><a href="#cb84-99" aria-hidden="true" tabindex="-1"></a>        norm_2.append(np.linalg.norm(W_2))</span>
<span id="cb84-100"><a href="#cb84-100" aria-hidden="true" tabindex="-1"></a>        norm_3.append(np.linalg.norm(W_3))</span>
<span id="cb84-101"><a href="#cb84-101" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb84-102"><a href="#cb84-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># save new weights and biases to be used in the next feedforward step</span></span>
<span id="cb84-103"><a href="#cb84-103" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> [W_1,W_2,W_3]</span>
<span id="cb84-104"><a href="#cb84-104" aria-hidden="true" tabindex="-1"></a>        biases <span class="op">=</span> [b_1,b_2,b_3]</span>
<span id="cb84-105"><a href="#cb84-105" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb84-106"><a href="#cb84-106" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate current loss </span></span>
<span id="cb84-107"><a href="#cb84-107" aria-hidden="true" tabindex="-1"></a>        loss.append(mse(activations[<span class="op">-</span><span class="dv">1</span>],batch_labels))</span>
<span id="cb84-108"><a href="#cb84-108" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb84-109"><a href="#cb84-109" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch:</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch:1/10
Epoch:2/10
Epoch:3/10
Epoch:4/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch:5/10
Epoch:6/10
Epoch:7/10
Epoch:8/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch:9/10
Epoch:10/10</code></pre>
</div>
</div>
<p>üßê Hm. Wait a minute - <strong>how do we know we‚Äôre doing the right thing</strong>? We should check it!</p>
</section>
</section>
<section id="gradient-checking" class="level2">
<h2 class="anchored" data-anchor-id="gradient-checking">Gradient Checking</h2>
<p>Backprop implementations are notoriously difficult to get right! That means we should implement some kind of <strong>numerical check for correctness</strong>.</p>
<div class="info">
<p>üßê<strong>Pause-and-ponder</strong>: Can you think of a way we could <strong>numerically check</strong> that our <em>analytical</em> gradients are correct?</p>
</div>
<section id="numerical-gradients" class="level3">
<h3 class="anchored" data-anchor-id="numerical-gradients">Numerical Gradients!</h3>
<p>Having thought about it a bit, you might have thought to check our numerical approximations for the derivative!</p>
<p>Recall, we could write the cost function as a function of <strong>one long vector</strong> of <strong>all of our parameters</strong>:</p>
<p><span class="math display">
\czero(\color{blue}{\mathbf{W}_1,\mathbf{W}_2,\mathbf{W}_3},\color{purple}{\mathbf{b}_1,\mathbf{b}_2,\mathbf{b}_3}) = \czero(\color{blue}{w_{0,0},w_{0,1},\ldots,w_{1,0},w_{1,1}},\ldots,\color{purple}{b_{0,0},b_{0,1}},\ldots)
</span></p>
<p>To abstract this away, we can write it out as:</p>
<p><span class="math display">
\czero (\theta_1,\theta_2,\ldots,\theta_i,\ldots)
</span></p>
<p>Well, then one way we could check for correctness, is to calculate <strong>the two sided difference</strong> <span class="math inline">\Delta_i</span>:</p>
<p><span class="math display">
\begin{align}
\Delta_i &amp;= \frac{\czero (\theta_1,\theta_2,\ldots,\color{red}{\theta_i+\epsilon},\ldots) - \czero (\theta_1,\theta_2,\ldots,\color{red}{\theta_i-\epsilon},\ldots)}{2\epsilon}
\end{align}
</span></p>
<p>We know this is an <strong>approximation</strong> for our desired partial when <span class="math inline">\epsilon</span> is <strong>very small</strong>:</p>
<p><span class="math display">
\Delta_i \approx \frac{\dc}{\partial \theta_i}
</span></p>
<p>So we can use this approximation to see how close it is to our analytical gradient!</p>
<p>Specifically, we want to calculate:</p>
<p><span class="math display">
\frac{\|\Delta_i - \frac{\dc}{\partial \theta_i}  \|_2}{\|\Delta_i\|_2 + \|\frac{\dc}{\partial \theta_i}\|_2  }
</span></p>
<p>And verify that this is <strong>very small</strong> for all of our parameters! Specifically, we can choose <span class="math inline">\epsilon = 10^{-7}</span>, and expect this difference to be <span class="math inline">&lt;10^{-7}</span>. If so, we can call our backprop code:</p>
<center>
<strong>numerically verified‚Ñ¢ üòé</strong>
</center>
<p>A note before we proceed, its very common to use the <code>ravel</code> function to do this <em>flattening out</em>, and then use <code>concatenate</code> to join our newly flattened arrays‚Äù</p>
<div class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>a,b <span class="op">=</span> np.random.randint(<span class="dv">1</span>,<span class="dv">10</span>,(<span class="dv">3</span>,<span class="dv">2</span>)),np.random.randint(<span class="dv">1</span>,<span class="dv">10</span>,(<span class="dv">4</span>,<span class="dv">3</span>))</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>a,b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="68">
<pre><code>(array([[2, 5],
        [6, 1],
        [3, 8]]),
 array([[3, 5, 8],
        [8, 1, 1],
        [3, 9, 6],
        [7, 5, 4]]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>a.ravel()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="69">
<pre><code>array([2, 5, 6, 1, 3, 8])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>np.concatenate((a.ravel(),b.ravel()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>array([2, 5, 6, 1, 3, 8, 3, 5, 8, 8, 1, 1, 3, 9, 6, 7, 5, 4])</code></pre>
</div>
</div>
<p>So now, we can use these to flatten all of our parameters, and join them into one big vector which we can then <strong>perturb</strong>.</p>
<p>Lets define a couple helper functions which will help us <strong>unravel</strong> and then <strong>reform</strong> them.</p>
<p>The first is <code>get_params</code>, which will return a single vector of all our parameters, using python‚Äôs <strong>list comprehension</strong>:</p>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_params(weights,biases):</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>    weight_params <span class="op">=</span> [weight.ravel() <span class="cf">for</span> weight <span class="kw">in</span> weights]</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>    bias_params <span class="op">=</span> [bias.ravel() <span class="cf">for</span> bias <span class="kw">in</span> biases]</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>    all_params <span class="op">=</span> weight_params<span class="op">+</span>bias_params</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.concatenate(all_params)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>get_params(weights,biases)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>array([ 0.14188865,  0.63484706,  0.12494954,  0.12396009,  0.84370276,
        0.35345227,  0.92243417,  0.46469144,  0.24677724,  0.86751385,
        0.10747816,  0.66656228,  0.15417883,  0.02532289,  0.60376072,
        0.04574494, -1.0101568 , -0.64885254,  0.78366491,  0.34014546,
        0.54846053,  0.11253149,  0.44440565,  0.58982193,  0.43253046])</code></pre>
</div>
</div>
<p>Verify that this is of the correct length, based on our network structure!</p>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>get_params(weights,biases).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>(25,)</code></pre>
</div>
</div>
<p>We can also define a <code>set_params</code> function, which is responsible for <strong>re-forming</strong> our parameters into the correct shapes, given one long vector:</p>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_params(param_vector,orig_weights,orig_biases):</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set weights</span></span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>    new_weights<span class="op">=</span>[]</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>    pointer <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> weight_mat <span class="kw">in</span> orig_weights:</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>        curr_len <span class="op">=</span> (weight_mat.shape[<span class="dv">0</span>]<span class="op">*</span>weight_mat.shape[<span class="dv">1</span>])</span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>        new_mat <span class="op">=</span> param_vector[pointer:pointer<span class="op">+</span>curr_len].reshape(weight_mat.shape)</span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a>        new_weights.append(new_mat)</span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a>        pointer <span class="op">+=</span> curr_len</span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set biases</span></span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a>    new_biases<span class="op">=</span>[]</span>
<span id="cb99-13"><a href="#cb99-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> weight_vec <span class="kw">in</span> orig_biases:</span>
<span id="cb99-14"><a href="#cb99-14" aria-hidden="true" tabindex="-1"></a>        curr_len <span class="op">=</span> (weight_vec.shape[<span class="dv">0</span>]<span class="op">*</span>weight_vec.shape[<span class="dv">1</span>])</span>
<span id="cb99-15"><a href="#cb99-15" aria-hidden="true" tabindex="-1"></a>        new_vec <span class="op">=</span> param_vector[pointer:pointer<span class="op">+</span>curr_len].reshape(weight_vec.shape)</span>
<span id="cb99-16"><a href="#cb99-16" aria-hidden="true" tabindex="-1"></a>        new_biases.append(new_vec)</span>
<span id="cb99-17"><a href="#cb99-17" aria-hidden="true" tabindex="-1"></a>        pointer <span class="op">+=</span> curr_len</span>
<span id="cb99-18"><a href="#cb99-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb99-19"><a href="#cb99-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_weights,new_biases</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note: This function takes an original set of weights and biases, but only uses it for the sizes of each element!</p>
<p>Lets run both of our new functions and <strong>verify</strong> they work as expected:</p>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>new_weights,new_biases <span class="op">=</span> set_params(get_params(weights,biases),weights,biases)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can once again use list comprehension to quickly check if <code>new_weights</code> is equal to <code>weights</code>, and <code>new_biases</code> is equal to <code>biases</code> at each element:</p>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>[np.array_equal(i,j) <span class="cf">for</span> i,j <span class="kw">in</span> <span class="bu">zip</span>(weights,new_weights)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="76">
<pre><code>[True, True, True]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>[np.array_equal(i,j) <span class="cf">for</span> i,j <span class="kw">in</span> <span class="bu">zip</span>(biases,new_biases)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="77">
<pre><code>[True, True, True]</code></pre>
</div>
</div>
<p>We should see all <code>True</code> there! Perfect, our unraveling and raveling works as we expect. Now we can use this to perform gradient checking.</p>
</section>
<section id="implementation-5" class="level3">
<h3 class="anchored" data-anchor-id="implementation-5">Implementation</h3>
<p>We can now begin to implement our <strong>numerical gradient checking</strong>, by defining a function that lets us do this:</p>
<div class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_checking(weights,biases,batch,idx,current_label,weight_updates,bias_updates,epsilon<span class="op">=</span><span class="fl">1e-7</span>):</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># unravel the current params</span></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> get_params(weights,biases)</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>    diff_vec <span class="op">=</span> np.zeros_like(params)</span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loop through, perturbing one parameter at a time:</span></span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p_index <span class="kw">in</span> <span class="bu">range</span>(params.shape[<span class="dv">0</span>]):</span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>        perturb <span class="op">=</span> np.zeros_like(params)</span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>        perturb[p_index] <span class="op">=</span> epsilon</span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># feedforward at each side</span></span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a>        _, activations_p <span class="op">=</span> feedforward_batch(batch,<span class="op">*</span>set_params(params <span class="op">+</span> perturb,weights,biases))</span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a>        _, activations_m <span class="op">=</span> feedforward_batch(batch,<span class="op">*</span>set_params(params <span class="op">-</span> perturb,weights,biases))</span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-16"><a href="#cb105-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate cost of each side</span></span>
<span id="cb105-17"><a href="#cb105-17" aria-hidden="true" tabindex="-1"></a>        cost_plus <span class="op">=</span> cost_mse(activations_p[<span class="op">-</span><span class="dv">1</span>][:,idx],current_label)</span>
<span id="cb105-18"><a href="#cb105-18" aria-hidden="true" tabindex="-1"></a>        cost_minus <span class="op">=</span> cost_mse(activations_m[<span class="op">-</span><span class="dv">1</span>][:,idx],current_label)</span>
<span id="cb105-19"><a href="#cb105-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-20"><a href="#cb105-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calcualte \Delta_i</span></span>
<span id="cb105-21"><a href="#cb105-21" aria-hidden="true" tabindex="-1"></a>        diff_vec[p_index] <span class="op">=</span> (cost_plus <span class="op">-</span> cost_minus)<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>epsilon)</span>
<span id="cb105-22"><a href="#cb105-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-23"><a href="#cb105-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate the difference for this round of backprop</span></span>
<span id="cb105-24"><a href="#cb105-24" aria-hidden="true" tabindex="-1"></a>    grad_vec <span class="op">=</span> get_params(weight_updates, bias_updates)</span>
<span id="cb105-25"><a href="#cb105-25" aria-hidden="true" tabindex="-1"></a>    grad_error <span class="op">=</span> np.linalg.norm(diff_vec <span class="op">-</span> grad_vec)<span class="op">/</span>(np.linalg.norm(diff_vec) <span class="op">+</span> np.linalg.norm(grad_vec))</span>
<span id="cb105-26"><a href="#cb105-26" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb105-27"><a href="#cb105-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> grad_error <span class="op">&gt;</span> epsilon:</span>
<span id="cb105-28"><a href="#cb105-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Error in gradient calculation!"</span>)</span>
<span id="cb105-29"><a href="#cb105-29" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb105-30"><a href="#cb105-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> grad_error</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This function takes the following:</p>
<ul>
<li><code>weights</code>: the current weights of our network</li>
<li><code>biases</code>: the current biases of our network</li>
<li><code>batch</code>: the current batch of data</li>
<li><code>idx</code>: the index of our current sample,</li>
<li><code>current_label</code>: the current label, of the <strong>current sample</strong></li>
<li><code>weight_updates</code>: the list of gradients of the weights we calculated</li>
<li><code>bias_updates</code>: the list of gradients of the biases we calculated</li>
<li><code>epsilon=1e-7</code>: a default epsilon value</li>
</ul>
<p>Its a bit verbose, but thats because we‚Äôre going for a flat implementation. Once we‚Äôve understood all the pieces, we can wrap everything up nice and neat.</p>
<p>With this, lets re-run our training but this time perform gradient checking!</p>
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backprop_1_example1(activities,activations,y,quiet<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>    my_bp[<span class="st">'act'</span>].append(activations)</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>    layer_num <span class="op">=</span> <span class="bu">len</span>(activations)</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>    weight_updates <span class="op">=</span> []</span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a>    bias_updates <span class="op">=</span> []</span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb106-9"><a href="#cb106-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">#           Last Level</span></span>
<span id="cb106-10"><a href="#cb106-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb106-11"><a href="#cb106-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get last layer specific variables</span></span>
<span id="cb106-12"><a href="#cb106-12" aria-hidden="true" tabindex="-1"></a>    a_L <span class="op">=</span> activations[<span class="op">-</span><span class="dv">1</span>].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb106-13"><a href="#cb106-13" aria-hidden="true" tabindex="-1"></a>    a_L_minus_1 <span class="op">=</span> activations[<span class="op">-</span><span class="dv">2</span>].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb106-14"><a href="#cb106-14" aria-hidden="true" tabindex="-1"></a>    z_L <span class="op">=</span> activities[<span class="op">-</span><span class="dv">1</span>].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb106-15"><a href="#cb106-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb106-16"><a href="#cb106-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate delta_L for the last level</span></span>
<span id="cb106-17"><a href="#cb106-17" aria-hidden="true" tabindex="-1"></a>    delta_L <span class="op">=</span> (a_L<span class="op">-</span>y)<span class="op">*</span>sigmoid_prime(z_L)</span>
<span id="cb106-18"><a href="#cb106-18" aria-hidden="true" tabindex="-1"></a>    my_bp[<span class="st">'delta'</span>].append(delta_L)</span>
<span id="cb106-19"><a href="#cb106-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb106-20"><a href="#cb106-20" aria-hidden="true" tabindex="-1"></a>    deltas <span class="op">=</span> []</span>
<span id="cb106-21"><a href="#cb106-21" aria-hidden="true" tabindex="-1"></a>    deltas.append(delta_L)</span>
<span id="cb106-22"><a href="#cb106-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-23"><a href="#cb106-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate bias update</span></span>
<span id="cb106-24"><a href="#cb106-24" aria-hidden="true" tabindex="-1"></a>    dc_db_L <span class="op">=</span> delta_L</span>
<span id="cb106-25"><a href="#cb106-25" aria-hidden="true" tabindex="-1"></a>    bias_updates <span class="op">=</span> [dc_db_L] <span class="op">+</span> bias_updates</span>
<span id="cb106-26"><a href="#cb106-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-27"><a href="#cb106-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calcualte weight updates</span></span>
<span id="cb106-28"><a href="#cb106-28" aria-hidden="true" tabindex="-1"></a>    dc_dw_L <span class="op">=</span> delta_L.dot((a_L_minus_1).T)</span>
<span id="cb106-29"><a href="#cb106-29" aria-hidden="true" tabindex="-1"></a>    my_bp[<span class="st">'w'</span>].append(dc_dw_L)</span>
<span id="cb106-30"><a href="#cb106-30" aria-hidden="true" tabindex="-1"></a>    weight_updates <span class="op">=</span> [dc_dw_L] <span class="op">+</span> weight_updates</span>
<span id="cb106-31"><a href="#cb106-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb106-32"><a href="#cb106-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> quiet:</span>
<span id="cb106-33"><a href="#cb106-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'=========Layer:</span><span class="sc">{</span>layer_num<span class="op">+-</span><span class="dv">1</span><span class="sc">}</span><span class="ss">========='</span>)</span>
<span id="cb106-34"><a href="#cb106-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Using negative index: -1'</span>)</span>
<span id="cb106-35"><a href="#cb106-35" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'z_l:</span><span class="sc">{</span>z_L<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l:</span><span class="sc">{</span>a_L<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l_minus_1:</span><span class="sc">{</span>a_L_minus_1<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb106-36"><a href="#cb106-36" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'delta_l:</span><span class="sc">{</span>delta_L<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, dc_db_l:</span><span class="sc">{</span>dc_db_L<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">,dc_dw_l:</span><span class="sc">{</span>dc_dw_L<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb106-37"><a href="#cb106-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb106-38"><a href="#cb106-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb106-39"><a href="#cb106-39" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb106-40"><a href="#cb106-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loop through each layer, from 2 to L-1</span></span>
<span id="cb106-41"><a href="#cb106-41" aria-hidden="true" tabindex="-1"></a>    <span class="co">######################################## </span></span>
<span id="cb106-42"><a href="#cb106-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,layer_num):</span>
<span id="cb106-43"><a href="#cb106-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># using level as a **negative index**</span></span>
<span id="cb106-44"><a href="#cb106-44" aria-hidden="true" tabindex="-1"></a>        l <span class="op">=</span> <span class="op">-</span>layer</span>
<span id="cb106-45"><a href="#cb106-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb106-46"><a href="#cb106-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># uncomment this print statement </span></span>
<span id="cb106-47"><a href="#cb106-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to help you understand how negative indexing works</span></span>
<span id="cb106-48"><a href="#cb106-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(f"Loop variable: {layer}, l={l}, corresponding to layer: l{l}")</span></span>
<span id="cb106-49"><a href="#cb106-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-50"><a href="#cb106-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate delta_l for each layer</span></span>
<span id="cb106-51"><a href="#cb106-51" aria-hidden="true" tabindex="-1"></a>        a_l <span class="op">=</span> activations[l].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb106-52"><a href="#cb106-52" aria-hidden="true" tabindex="-1"></a>        z_l <span class="op">=</span> activities[l].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb106-53"><a href="#cb106-53" aria-hidden="true" tabindex="-1"></a>        delta_l <span class="op">=</span> weights[l<span class="op">+</span><span class="dv">1</span>].T.dot(deltas[l<span class="op">+</span><span class="dv">1</span>]) <span class="op">*</span> sigmoid_prime(z_l)</span>
<span id="cb106-54"><a href="#cb106-54" aria-hidden="true" tabindex="-1"></a>        my_bp[<span class="st">'delta'</span>].append(delta_l)</span>
<span id="cb106-55"><a href="#cb106-55" aria-hidden="true" tabindex="-1"></a>        deltas <span class="op">=</span> [delta_l] <span class="op">+</span> deltas</span>
<span id="cb106-56"><a href="#cb106-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-57"><a href="#cb106-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate bias update</span></span>
<span id="cb106-58"><a href="#cb106-58" aria-hidden="true" tabindex="-1"></a>        dc_db_l <span class="op">=</span> delta_l</span>
<span id="cb106-59"><a href="#cb106-59" aria-hidden="true" tabindex="-1"></a>        bias_updates <span class="op">=</span> [dc_db_l] <span class="op">+</span> bias_updates</span>
<span id="cb106-60"><a href="#cb106-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-61"><a href="#cb106-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calcualte weight updates</span></span>
<span id="cb106-62"><a href="#cb106-62" aria-hidden="true" tabindex="-1"></a>        a_l_minus_1 <span class="op">=</span> activations[l<span class="op">-</span><span class="dv">1</span>].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb106-63"><a href="#cb106-63" aria-hidden="true" tabindex="-1"></a>        dc_dw_l <span class="op">=</span> delta_l.dot((a_l_minus_1).T)</span>
<span id="cb106-64"><a href="#cb106-64" aria-hidden="true" tabindex="-1"></a>        my_bp[<span class="st">'w'</span>].append(dc_dw_l)</span>
<span id="cb106-65"><a href="#cb106-65" aria-hidden="true" tabindex="-1"></a>        weight_updates <span class="op">=</span> [dc_dw_l] <span class="op">+</span> weight_updates</span>
<span id="cb106-66"><a href="#cb106-66" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb106-67"><a href="#cb106-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> quiet:</span>
<span id="cb106-68"><a href="#cb106-68" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'=========Layer:</span><span class="sc">{</span>layer_num<span class="op">+</span>l<span class="sc">}</span><span class="ss">========='</span>)</span>
<span id="cb106-69"><a href="#cb106-69" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'Using negative index: </span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb106-70"><a href="#cb106-70" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'z_l:</span><span class="sc">{</span>z_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l:</span><span class="sc">{</span>a_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l_minus_1:</span><span class="sc">{</span>a_l_minus_1<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb106-71"><a href="#cb106-71" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'delta_l:</span><span class="sc">{</span>delta_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, dc_db_l:</span><span class="sc">{</span>dc_db_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">,dc_dw_l:</span><span class="sc">{</span>dc_dw_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb106-72"><a href="#cb106-72" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>()</span>
<span id="cb106-73"><a href="#cb106-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb106-74"><a href="#cb106-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weight_updates, bias_updates</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>eta<span class="op">=</span><span class="fl">1e-2</span></span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>dataset<span class="op">=</span>X</span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a>labels<span class="op">=</span>y</span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a>quiet<span class="op">=</span><span class="va">False</span></span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient checking</span></span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weight matrices - notice the dimensions</span></span>
<span id="cb107-15"><a href="#cb107-15" aria-hidden="true" tabindex="-1"></a>W_1_init <span class="op">=</span> np.random.rand(<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb107-16"><a href="#cb107-16" aria-hidden="true" tabindex="-1"></a>W_2_init <span class="op">=</span> np.random.rand(<span class="dv">3</span>,<span class="dv">3</span>)</span>
<span id="cb107-17"><a href="#cb107-17" aria-hidden="true" tabindex="-1"></a>W_3_init <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb107-18"><a href="#cb107-18" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> [W_1_init,W_2_init,W_3_init]</span>
<span id="cb107-19"><a href="#cb107-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-20"><a href="#cb107-20" aria-hidden="true" tabindex="-1"></a><span class="co"># and our biases</span></span>
<span id="cb107-21"><a href="#cb107-21" aria-hidden="true" tabindex="-1"></a>b_1_init <span class="op">=</span> np.random.rand(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb107-22"><a href="#cb107-22" aria-hidden="true" tabindex="-1"></a>b_2_init <span class="op">=</span> np.random.rand(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb107-23"><a href="#cb107-23" aria-hidden="true" tabindex="-1"></a>b_3_init <span class="op">=</span> np.random.rand(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb107-24"><a href="#cb107-24" aria-hidden="true" tabindex="-1"></a>biases <span class="op">=</span> [b_1_init,b_2_init,b_3_init]</span>
<span id="cb107-25"><a href="#cb107-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-26"><a href="#cb107-26" aria-hidden="true" tabindex="-1"></a><span class="co"># network</span></span>
<span id="cb107-27"><a href="#cb107-27" aria-hidden="true" tabindex="-1"></a>W_1,W_2,W_3 <span class="op">=</span> weights[<span class="dv">0</span>],weights[<span class="dv">1</span>],weights[<span class="dv">2</span>]</span>
<span id="cb107-28"><a href="#cb107-28" aria-hidden="true" tabindex="-1"></a>b_1,b_2,b_3 <span class="op">=</span> biases[<span class="dv">0</span>],biases[<span class="dv">1</span>],biases[<span class="dv">2</span>]</span>
<span id="cb107-29"><a href="#cb107-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-30"><a href="#cb107-30" aria-hidden="true" tabindex="-1"></a><span class="co"># mini-batch params </span></span>
<span id="cb107-31"><a href="#cb107-31" aria-hidden="true" tabindex="-1"></a>num_batches <span class="op">=</span> <span class="bu">int</span>(N<span class="op">/</span>batch_size)</span>
<span id="cb107-32"><a href="#cb107-32" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb107-33"><a href="#cb107-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-34"><a href="#cb107-34" aria-hidden="true" tabindex="-1"></a><span class="co"># various debugging lists</span></span>
<span id="cb107-35"><a href="#cb107-35" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> []</span>
<span id="cb107-36"><a href="#cb107-36" aria-hidden="true" tabindex="-1"></a>grad_norm_1 <span class="op">=</span> []</span>
<span id="cb107-37"><a href="#cb107-37" aria-hidden="true" tabindex="-1"></a>grad_norm_2 <span class="op">=</span> []</span>
<span id="cb107-38"><a href="#cb107-38" aria-hidden="true" tabindex="-1"></a>grad_norm_3 <span class="op">=</span> []</span>
<span id="cb107-39"><a href="#cb107-39" aria-hidden="true" tabindex="-1"></a>norm_1 <span class="op">=</span> []</span>
<span id="cb107-40"><a href="#cb107-40" aria-hidden="true" tabindex="-1"></a>norm_2 <span class="op">=</span> []</span>
<span id="cb107-41"><a href="#cb107-41" aria-hidden="true" tabindex="-1"></a>norm_3 <span class="op">=</span> []</span>
<span id="cb107-42"><a href="#cb107-42" aria-hidden="true" tabindex="-1"></a>grad_errors <span class="op">=</span> []</span>
<span id="cb107-43"><a href="#cb107-43" aria-hidden="true" tabindex="-1"></a>diff_vecs <span class="op">=</span> []</span>
<span id="cb107-44"><a href="#cb107-44" aria-hidden="true" tabindex="-1"></a>grad_vecs <span class="op">=</span> []</span>
<span id="cb107-45"><a href="#cb107-45" aria-hidden="true" tabindex="-1"></a>my_act <span class="op">=</span> []</span>
<span id="cb107-46"><a href="#cb107-46" aria-hidden="true" tabindex="-1"></a>my_w <span class="op">=</span> []</span>
<span id="cb107-47"><a href="#cb107-47" aria-hidden="true" tabindex="-1"></a>my_b <span class="op">=</span> []</span>
<span id="cb107-48"><a href="#cb107-48" aria-hidden="true" tabindex="-1"></a>my_bp <span class="op">=</span> {<span class="st">'x'</span>:[],<span class="st">'y'</span>:[],<span class="st">'act'</span>:[],<span class="st">'delta'</span>:[],<span class="st">'w'</span>:[]}</span>
<span id="cb107-49"><a href="#cb107-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-50"><a href="#cb107-50" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb107-51"><a href="#cb107-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># shuffle the dataset - note we do this by shuffling indices</span></span>
<span id="cb107-52"><a href="#cb107-52" aria-hidden="true" tabindex="-1"></a><span class="co">#     indices = np.random.permutation(N)</span></span>
<span id="cb107-53"><a href="#cb107-53" aria-hidden="true" tabindex="-1"></a><span class="co">#     shuffled_dataset = dataset[indices,:]</span></span>
<span id="cb107-54"><a href="#cb107-54" aria-hidden="true" tabindex="-1"></a><span class="co">#     shuffled_labels = labels[indices,:]</span></span>
<span id="cb107-55"><a href="#cb107-55" aria-hidden="true" tabindex="-1"></a>    shuffled_dataset <span class="op">=</span> dataset</span>
<span id="cb107-56"><a href="#cb107-56" aria-hidden="true" tabindex="-1"></a>    shuffled_labels <span class="op">=</span> labels</span>
<span id="cb107-57"><a href="#cb107-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb107-58"><a href="#cb107-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># work on current batch </span></span>
<span id="cb107-59"><a href="#cb107-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_num <span class="kw">in</span> <span class="bu">range</span>(num_batches):</span>
<span id="cb107-60"><a href="#cb107-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-61"><a href="#cb107-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get current batch</span></span>
<span id="cb107-62"><a href="#cb107-62" aria-hidden="true" tabindex="-1"></a>        batch_start <span class="op">=</span> batch_num<span class="op">*</span>batch_size</span>
<span id="cb107-63"><a href="#cb107-63" aria-hidden="true" tabindex="-1"></a>        batch_end <span class="op">=</span> (batch_num<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>batch_size</span>
<span id="cb107-64"><a href="#cb107-64" aria-hidden="true" tabindex="-1"></a>        batch <span class="op">=</span> shuffled_dataset[batch_start:batch_end,:]</span>
<span id="cb107-65"><a href="#cb107-65" aria-hidden="true" tabindex="-1"></a>        batch_labels <span class="op">=</span> shuffled_labels[batch_start:batch_end,:].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb107-66"><a href="#cb107-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-67"><a href="#cb107-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># feedforward on batch</span></span>
<span id="cb107-68"><a href="#cb107-68" aria-hidden="true" tabindex="-1"></a>        activities, activations <span class="op">=</span> feedforward_batch(batch, weights,biases)</span>
<span id="cb107-69"><a href="#cb107-69" aria-hidden="true" tabindex="-1"></a>        my_act.append(activations[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb107-70"><a href="#cb107-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-71"><a href="#cb107-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># setup matrices to hold gradients</span></span>
<span id="cb107-72"><a href="#cb107-72" aria-hidden="true" tabindex="-1"></a>        grad_W_1 <span class="op">=</span> np.zeros_like(W_1)</span>
<span id="cb107-73"><a href="#cb107-73" aria-hidden="true" tabindex="-1"></a>        grad_b_1 <span class="op">=</span> np.zeros_like(b_1)</span>
<span id="cb107-74"><a href="#cb107-74" aria-hidden="true" tabindex="-1"></a>        grad_W_2 <span class="op">=</span> np.zeros_like(W_2)</span>
<span id="cb107-75"><a href="#cb107-75" aria-hidden="true" tabindex="-1"></a>        grad_b_2 <span class="op">=</span> np.zeros_like(b_2)</span>
<span id="cb107-76"><a href="#cb107-76" aria-hidden="true" tabindex="-1"></a>        grad_W_3 <span class="op">=</span> np.zeros_like(W_3)</span>
<span id="cb107-77"><a href="#cb107-77" aria-hidden="true" tabindex="-1"></a>        grad_b_3 <span class="op">=</span> np.zeros_like(b_3)</span>
<span id="cb107-78"><a href="#cb107-78" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb107-79"><a href="#cb107-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-80"><a href="#cb107-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loop through each example in the batch</span></span>
<span id="cb107-81"><a href="#cb107-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(batch.shape[<span class="dv">0</span>]):</span>
<span id="cb107-82"><a href="#cb107-82" aria-hidden="true" tabindex="-1"></a>            current_sample <span class="op">=</span> batch[idx,:].reshape(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb107-83"><a href="#cb107-83" aria-hidden="true" tabindex="-1"></a>            current_label <span class="op">=</span> batch_labels[idx]</span>
<span id="cb107-84"><a href="#cb107-84" aria-hidden="true" tabindex="-1"></a>            my_bp[<span class="st">'x'</span>].append(current_sample)</span>
<span id="cb107-85"><a href="#cb107-85" aria-hidden="true" tabindex="-1"></a>            my_bp[<span class="st">'y'</span>].append(current_label)</span>
<span id="cb107-86"><a href="#cb107-86" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb107-87"><a href="#cb107-87" aria-hidden="true" tabindex="-1"></a>            curr_act, curr_activ <span class="op">=</span> feedforward_batch(current_sample, weights,biases)</span>
<span id="cb107-88"><a href="#cb107-88" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb107-89"><a href="#cb107-89" aria-hidden="true" tabindex="-1"></a>            <span class="co"># get current weight and bias updates</span></span>
<span id="cb107-90"><a href="#cb107-90" aria-hidden="true" tabindex="-1"></a>            weight_updates, bias_updates <span class="op">=</span> backprop_1_example1(curr_act,curr_activ,</span>
<span id="cb107-91"><a href="#cb107-91" aria-hidden="true" tabindex="-1"></a>                                                              current_label)</span>
<span id="cb107-92"><a href="#cb107-92" aria-hidden="true" tabindex="-1"></a>            my_w.append(weight_updates)</span>
<span id="cb107-93"><a href="#cb107-93" aria-hidden="true" tabindex="-1"></a>            my_b.append(bias_updates)</span>
<span id="cb107-94"><a href="#cb107-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-95"><a href="#cb107-95" aria-hidden="true" tabindex="-1"></a>            <span class="co"># perform gradient cehcking</span></span>
<span id="cb107-96"><a href="#cb107-96" aria-hidden="true" tabindex="-1"></a><span class="co">#             grad_error = gradient_checking(weights,biases,batch,idx,current_label,</span></span>
<span id="cb107-97"><a href="#cb107-97" aria-hidden="true" tabindex="-1"></a><span class="co">#                                            weight_updates,bias_updates,epsilon)</span></span>
<span id="cb107-98"><a href="#cb107-98" aria-hidden="true" tabindex="-1"></a><span class="co">#             grad_errors.append(grad_error)</span></span>
<span id="cb107-99"><a href="#cb107-99" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb107-100"><a href="#cb107-100" aria-hidden="true" tabindex="-1"></a>            <span class="co"># aggregate our updates</span></span>
<span id="cb107-101"><a href="#cb107-101" aria-hidden="true" tabindex="-1"></a>            grad_W_1 <span class="op">=</span> grad_W_1 <span class="op">+</span> weight_updates[<span class="dv">0</span>]</span>
<span id="cb107-102"><a href="#cb107-102" aria-hidden="true" tabindex="-1"></a>            grad_b_1 <span class="op">=</span> grad_b_1 <span class="op">+</span> bias_updates[<span class="dv">0</span>]</span>
<span id="cb107-103"><a href="#cb107-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-104"><a href="#cb107-104" aria-hidden="true" tabindex="-1"></a>            grad_W_2 <span class="op">=</span> grad_W_2 <span class="op">+</span> weight_updates[<span class="dv">1</span>]</span>
<span id="cb107-105"><a href="#cb107-105" aria-hidden="true" tabindex="-1"></a>            grad_b_2 <span class="op">=</span> grad_b_2 <span class="op">+</span> bias_updates[<span class="dv">1</span>]</span>
<span id="cb107-106"><a href="#cb107-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-107"><a href="#cb107-107" aria-hidden="true" tabindex="-1"></a>            grad_W_3 <span class="op">=</span> grad_W_3 <span class="op">+</span> weight_updates[<span class="dv">2</span>]</span>
<span id="cb107-108"><a href="#cb107-108" aria-hidden="true" tabindex="-1"></a>            grad_b_3 <span class="op">=</span> grad_b_3 <span class="op">+</span> bias_updates[<span class="dv">2</span>]</span>
<span id="cb107-109"><a href="#cb107-109" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb107-110"><a href="#cb107-110" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb107-111"><a href="#cb107-111" aria-hidden="true" tabindex="-1"></a>        grad_norm_1.append(np.linalg.norm(grad_W_1))</span>
<span id="cb107-112"><a href="#cb107-112" aria-hidden="true" tabindex="-1"></a>        grad_norm_2.append(np.linalg.norm(grad_W_2))</span>
<span id="cb107-113"><a href="#cb107-113" aria-hidden="true" tabindex="-1"></a>        grad_norm_3.append(np.linalg.norm(grad_W_3))</span>
<span id="cb107-114"><a href="#cb107-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-115"><a href="#cb107-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-116"><a href="#cb107-116" aria-hidden="true" tabindex="-1"></a>        <span class="co"># take your steps:</span></span>
<span id="cb107-117"><a href="#cb107-117" aria-hidden="true" tabindex="-1"></a>        W_1 <span class="op">=</span> W_1 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(grad_W_1)</span>
<span id="cb107-118"><a href="#cb107-118" aria-hidden="true" tabindex="-1"></a>        b_1 <span class="op">=</span> b_1 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(grad_b_1)</span>
<span id="cb107-119"><a href="#cb107-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-120"><a href="#cb107-120" aria-hidden="true" tabindex="-1"></a>        W_2 <span class="op">=</span> W_2 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(grad_W_2)</span>
<span id="cb107-121"><a href="#cb107-121" aria-hidden="true" tabindex="-1"></a>        b_2 <span class="op">=</span> b_2 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(grad_b_2)</span>
<span id="cb107-122"><a href="#cb107-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-123"><a href="#cb107-123" aria-hidden="true" tabindex="-1"></a>        W_3 <span class="op">=</span> W_3 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(grad_W_3)</span>
<span id="cb107-124"><a href="#cb107-124" aria-hidden="true" tabindex="-1"></a>        b_3 <span class="op">=</span> b_3 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(grad_b_3)</span>
<span id="cb107-125"><a href="#cb107-125" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb107-126"><a href="#cb107-126" aria-hidden="true" tabindex="-1"></a>        norm_1.append(np.linalg.norm(W_1))</span>
<span id="cb107-127"><a href="#cb107-127" aria-hidden="true" tabindex="-1"></a>        norm_2.append(np.linalg.norm(W_2))</span>
<span id="cb107-128"><a href="#cb107-128" aria-hidden="true" tabindex="-1"></a>        norm_3.append(np.linalg.norm(W_3))</span>
<span id="cb107-129"><a href="#cb107-129" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb107-130"><a href="#cb107-130" aria-hidden="true" tabindex="-1"></a>        <span class="co"># save new weights and biases to be used in the next feedforward step</span></span>
<span id="cb107-131"><a href="#cb107-131" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> [W_1,W_2,W_3]</span>
<span id="cb107-132"><a href="#cb107-132" aria-hidden="true" tabindex="-1"></a>        biases <span class="op">=</span> [b_1,b_2,b_3]</span>
<span id="cb107-133"><a href="#cb107-133" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb107-134"><a href="#cb107-134" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb107-135"><a href="#cb107-135" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb107-136"><a href="#cb107-136" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch:</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch:1/10
Epoch:2/10
Epoch:3/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch:4/10
Epoch:5/10
Epoch:6/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch:7/10
Epoch:8/10
Epoch:9/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch:10/10</code></pre>
</div>
</div>
<p>Now, lets plot our estimated <strong>gradient errors</strong>:</p>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>plt.plot(grad_errors)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="nn_files/figure-html/cell-82-output-1.png" width="590" height="411"></p>
</div>
</div>
<p>Wow! Look at that y axis! Pretty good! We cal also verify:</p>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>(np.array(grad_errors) <span class="op">&lt;</span> epsilon).<span class="bu">all</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="82">
<pre><code>True</code></pre>
</div>
</div>
<p>Perfect! Our implementation of backprop is now completely:</p>
<center>
<strong>numerically verified‚Ñ¢ üòé</strong>
</center>
</section>
</section>
<section id="compare-to-nielsen-implementation" class="level2">
<h2 class="anchored" data-anchor-id="compare-to-nielsen-implementation">Compare to Nielsen implementation:</h2>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Network(<span class="bu">object</span>):</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, sizes, weights<span class="op">=</span><span class="va">None</span>, biases<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""The list ``sizes`` contains the number of neurons in the</span></span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a><span class="co">        respective layers of the network.  For example, if the list</span></span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a><span class="co">        was [2, 3, 1] then it would be a three-layer network, with the</span></span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a><span class="co">        first layer containing 2 neurons, the second layer 3 neurons,</span></span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a><span class="co">        and the third layer 1 neuron.  The biases and weights for the</span></span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a><span class="co">        network are initialized randomly, using a Gaussian</span></span>
<span id="cb115-10"><a href="#cb115-10" aria-hidden="true" tabindex="-1"></a><span class="co">        distribution with mean 0, and variance 1.  Note that the first</span></span>
<span id="cb115-11"><a href="#cb115-11" aria-hidden="true" tabindex="-1"></a><span class="co">        layer is assumed to be an input layer, and by convention we</span></span>
<span id="cb115-12"><a href="#cb115-12" aria-hidden="true" tabindex="-1"></a><span class="co">        won't set any biases for those neurons, since biases are only</span></span>
<span id="cb115-13"><a href="#cb115-13" aria-hidden="true" tabindex="-1"></a><span class="co">        ever used in computing the outputs from later layers."""</span></span>
<span id="cb115-14"><a href="#cb115-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> <span class="bu">len</span>(sizes)</span>
<span id="cb115-15"><a href="#cb115-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sizes <span class="op">=</span> sizes</span>
<span id="cb115-16"><a href="#cb115-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> weights <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb115-17"><a href="#cb115-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.weights <span class="op">=</span> [np.random.randn(y, x)</span>
<span id="cb115-18"><a href="#cb115-18" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(sizes[:<span class="op">-</span><span class="dv">1</span>], sizes[<span class="dv">1</span>:])]</span>
<span id="cb115-19"><a href="#cb115-19" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"random weights"</span>)</span>
<span id="cb115-20"><a href="#cb115-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb115-21"><a href="#cb115-21" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.weights <span class="op">=</span> weights</span>
<span id="cb115-22"><a href="#cb115-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-23"><a href="#cb115-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> biases <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb115-24"><a href="#cb115-24" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.biases <span class="op">=</span> [np.random.randn(y, <span class="dv">1</span>) <span class="cf">for</span> y <span class="kw">in</span> sizes[<span class="dv">1</span>:]]</span>
<span id="cb115-25"><a href="#cb115-25" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"random biases"</span>)</span>
<span id="cb115-26"><a href="#cb115-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb115-27"><a href="#cb115-27" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.biases <span class="op">=</span> biases</span>
<span id="cb115-28"><a href="#cb115-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-29"><a href="#cb115-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> feedforward(<span class="va">self</span>, a):</span>
<span id="cb115-30"><a href="#cb115-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return the output of the network if ``a`` is input."""</span></span>
<span id="cb115-31"><a href="#cb115-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> b, w <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.biases, <span class="va">self</span>.weights):</span>
<span id="cb115-32"><a href="#cb115-32" aria-hidden="true" tabindex="-1"></a>            a <span class="op">=</span> sigmoid(np.dot(w, a)<span class="op">+</span>b)</span>
<span id="cb115-33"><a href="#cb115-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a</span>
<span id="cb115-34"><a href="#cb115-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-35"><a href="#cb115-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> SGD(<span class="va">self</span>, training_data, epochs, mini_batch_size, eta,</span>
<span id="cb115-36"><a href="#cb115-36" aria-hidden="true" tabindex="-1"></a>            test_data<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb115-37"><a href="#cb115-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Train the neural network using mini-batch stochastic</span></span>
<span id="cb115-38"><a href="#cb115-38" aria-hidden="true" tabindex="-1"></a><span class="co">        gradient descent.  The ``training_data`` is a list of tuples</span></span>
<span id="cb115-39"><a href="#cb115-39" aria-hidden="true" tabindex="-1"></a><span class="co">        ``(x, y)`` representing the training inputs and the desired</span></span>
<span id="cb115-40"><a href="#cb115-40" aria-hidden="true" tabindex="-1"></a><span class="co">        outputs.  The other non-optional parameters are</span></span>
<span id="cb115-41"><a href="#cb115-41" aria-hidden="true" tabindex="-1"></a><span class="co">        self-explanatory.  If ``test_data`` is provided then the</span></span>
<span id="cb115-42"><a href="#cb115-42" aria-hidden="true" tabindex="-1"></a><span class="co">        network will be evaluated against the test data after each</span></span>
<span id="cb115-43"><a href="#cb115-43" aria-hidden="true" tabindex="-1"></a><span class="co">        epoch, and partial progress printed out.  This is useful for</span></span>
<span id="cb115-44"><a href="#cb115-44" aria-hidden="true" tabindex="-1"></a><span class="co">        tracking progress, but slows things down substantially."""</span></span>
<span id="cb115-45"><a href="#cb115-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> test_data: n_test <span class="op">=</span> <span class="bu">len</span>(test_data)</span>
<span id="cb115-46"><a href="#cb115-46" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="bu">len</span>(training_data)</span>
<span id="cb115-47"><a href="#cb115-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb115-48"><a href="#cb115-48" aria-hidden="true" tabindex="-1"></a><span class="co">#             random.shuffle(training_data)</span></span>
<span id="cb115-49"><a href="#cb115-49" aria-hidden="true" tabindex="-1"></a>            mini_batches <span class="op">=</span> [</span>
<span id="cb115-50"><a href="#cb115-50" aria-hidden="true" tabindex="-1"></a>                training_data[k:k<span class="op">+</span>mini_batch_size]</span>
<span id="cb115-51"><a href="#cb115-51" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n, mini_batch_size)]</span>
<span id="cb115-52"><a href="#cb115-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> mini_batch_idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(mini_batches)):</span>
<span id="cb115-53"><a href="#cb115-53" aria-hidden="true" tabindex="-1"></a>                mini_batch <span class="op">=</span> mini_batches[mini_batch_idx]</span>
<span id="cb115-54"><a href="#cb115-54" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.update_mini_batch(mini_batch, eta)</span>
<span id="cb115-55"><a href="#cb115-55" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> test_data:</span>
<span id="cb115-56"><a href="#cb115-56" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">"Epoch </span><span class="sc">{0}</span><span class="st">: </span><span class="sc">{1}</span><span class="st"> / </span><span class="sc">{2}</span><span class="st">"</span>.<span class="bu">format</span>(</span>
<span id="cb115-57"><a href="#cb115-57" aria-hidden="true" tabindex="-1"></a>                    j, <span class="va">self</span>.evaluate(test_data), n_test))</span>
<span id="cb115-58"><a href="#cb115-58" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb115-59"><a href="#cb115-59" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">"Epoch </span><span class="sc">{0}</span><span class="st"> complete"</span>.<span class="bu">format</span>(j))</span>
<span id="cb115-60"><a href="#cb115-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-61"><a href="#cb115-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_mini_batch(<span class="va">self</span>, mini_batch, eta):</span>
<span id="cb115-62"><a href="#cb115-62" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Update the network's weights and biases by applying</span></span>
<span id="cb115-63"><a href="#cb115-63" aria-hidden="true" tabindex="-1"></a><span class="co">        gradient descent using backpropagation to a single mini batch.</span></span>
<span id="cb115-64"><a href="#cb115-64" aria-hidden="true" tabindex="-1"></a><span class="co">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</span></span>
<span id="cb115-65"><a href="#cb115-65" aria-hidden="true" tabindex="-1"></a><span class="co">        is the learning rate."""</span></span>
<span id="cb115-66"><a href="#cb115-66" aria-hidden="true" tabindex="-1"></a>        nabla_b <span class="op">=</span> [np.zeros(b.shape) <span class="cf">for</span> b <span class="kw">in</span> <span class="va">self</span>.biases]</span>
<span id="cb115-67"><a href="#cb115-67" aria-hidden="true" tabindex="-1"></a>        nabla_w <span class="op">=</span> [np.zeros(w.shape) <span class="cf">for</span> w <span class="kw">in</span> <span class="va">self</span>.weights]</span>
<span id="cb115-68"><a href="#cb115-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x, y <span class="kw">in</span> mini_batch:</span>
<span id="cb115-69"><a href="#cb115-69" aria-hidden="true" tabindex="-1"></a>            delta_nabla_b, delta_nabla_w <span class="op">=</span> <span class="va">self</span>.backprop(x, y)</span>
<span id="cb115-70"><a href="#cb115-70" aria-hidden="true" tabindex="-1"></a>            nielsen_w.append(delta_nabla_w)</span>
<span id="cb115-71"><a href="#cb115-71" aria-hidden="true" tabindex="-1"></a>            nielsen_b.append(delta_nabla_b)</span>
<span id="cb115-72"><a href="#cb115-72" aria-hidden="true" tabindex="-1"></a>            nabla_b <span class="op">=</span> [nb<span class="op">+</span>dnb <span class="cf">for</span> nb, dnb <span class="kw">in</span> <span class="bu">zip</span>(nabla_b, delta_nabla_b)]</span>
<span id="cb115-73"><a href="#cb115-73" aria-hidden="true" tabindex="-1"></a>            nabla_w <span class="op">=</span> [nw<span class="op">+</span>dnw <span class="cf">for</span> nw, dnw <span class="kw">in</span> <span class="bu">zip</span>(nabla_w, delta_nabla_w)]</span>
<span id="cb115-74"><a href="#cb115-74" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> [w<span class="op">-</span>(eta<span class="op">/</span><span class="bu">len</span>(mini_batch))<span class="op">*</span>nw</span>
<span id="cb115-75"><a href="#cb115-75" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">for</span> w, nw <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.weights, nabla_w)]</span>
<span id="cb115-76"><a href="#cb115-76" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">=</span> [b<span class="op">-</span>(eta<span class="op">/</span><span class="bu">len</span>(mini_batch))<span class="op">*</span>nb</span>
<span id="cb115-77"><a href="#cb115-77" aria-hidden="true" tabindex="-1"></a>                       <span class="cf">for</span> b, nb <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.biases, nabla_b)]</span>
<span id="cb115-78"><a href="#cb115-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-79"><a href="#cb115-79" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backprop(<span class="va">self</span>, x, y):</span>
<span id="cb115-80"><a href="#cb115-80" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return a tuple ``(nabla_b, nabla_w)`` representing the</span></span>
<span id="cb115-81"><a href="#cb115-81" aria-hidden="true" tabindex="-1"></a><span class="co">        gradient for the cost function C_x.  ``nabla_b`` and</span></span>
<span id="cb115-82"><a href="#cb115-82" aria-hidden="true" tabindex="-1"></a><span class="co">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span>
<span id="cb115-83"><a href="#cb115-83" aria-hidden="true" tabindex="-1"></a><span class="co">        to ``self.biases`` and ``self.weights``."""</span></span>
<span id="cb115-84"><a href="#cb115-84" aria-hidden="true" tabindex="-1"></a>        nabla_b <span class="op">=</span> [np.zeros(b.shape) <span class="cf">for</span> b <span class="kw">in</span> <span class="va">self</span>.biases]</span>
<span id="cb115-85"><a href="#cb115-85" aria-hidden="true" tabindex="-1"></a>        nabla_w <span class="op">=</span> [np.zeros(w.shape) <span class="cf">for</span> w <span class="kw">in</span> <span class="va">self</span>.weights]</span>
<span id="cb115-86"><a href="#cb115-86" aria-hidden="true" tabindex="-1"></a>        nielsen_bp[<span class="st">'x'</span>].append(x)</span>
<span id="cb115-87"><a href="#cb115-87" aria-hidden="true" tabindex="-1"></a>        nielsen_bp[<span class="st">'y'</span>].append(y)</span>
<span id="cb115-88"><a href="#cb115-88" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb115-89"><a href="#cb115-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># feedforward</span></span>
<span id="cb115-90"><a href="#cb115-90" aria-hidden="true" tabindex="-1"></a>        activation <span class="op">=</span> x</span>
<span id="cb115-91"><a href="#cb115-91" aria-hidden="true" tabindex="-1"></a>        activations <span class="op">=</span> [x] <span class="co"># list to store all the activations, layer by layer</span></span>
<span id="cb115-92"><a href="#cb115-92" aria-hidden="true" tabindex="-1"></a>        zs <span class="op">=</span> [] <span class="co"># list to store all the z vectors, layer by layer</span></span>
<span id="cb115-93"><a href="#cb115-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> b, w <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.biases, <span class="va">self</span>.weights):</span>
<span id="cb115-94"><a href="#cb115-94" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> np.dot(w, activation)<span class="op">+</span>b</span>
<span id="cb115-95"><a href="#cb115-95" aria-hidden="true" tabindex="-1"></a>            zs.append(z)</span>
<span id="cb115-96"><a href="#cb115-96" aria-hidden="true" tabindex="-1"></a>            activation <span class="op">=</span> sigmoid(z)</span>
<span id="cb115-97"><a href="#cb115-97" aria-hidden="true" tabindex="-1"></a>            activations.append(activation)</span>
<span id="cb115-98"><a href="#cb115-98" aria-hidden="true" tabindex="-1"></a>        nielsen_bp[<span class="st">'act'</span>].append(activations)</span>
<span id="cb115-99"><a href="#cb115-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backward pass</span></span>
<span id="cb115-100"><a href="#cb115-100" aria-hidden="true" tabindex="-1"></a>        delta <span class="op">=</span> <span class="va">self</span>.cost_derivative(activations[<span class="op">-</span><span class="dv">1</span>], y) <span class="op">*</span> <span class="op">\</span></span>
<span id="cb115-101"><a href="#cb115-101" aria-hidden="true" tabindex="-1"></a>            sigmoid_prime(zs[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb115-102"><a href="#cb115-102" aria-hidden="true" tabindex="-1"></a>        nielsen_bp[<span class="st">'delta'</span>].append(delta)</span>
<span id="cb115-103"><a href="#cb115-103" aria-hidden="true" tabindex="-1"></a>        nabla_b[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> delta</span>
<span id="cb115-104"><a href="#cb115-104" aria-hidden="true" tabindex="-1"></a>        nabla_w[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> np.dot(delta, activations[<span class="op">-</span><span class="dv">2</span>].transpose())</span>
<span id="cb115-105"><a href="#cb115-105" aria-hidden="true" tabindex="-1"></a>        nielsen_bp[<span class="st">'w'</span>].append(nabla_w[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb115-106"><a href="#cb115-106" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note that the variable l in the loop below is used a little</span></span>
<span id="cb115-107"><a href="#cb115-107" aria-hidden="true" tabindex="-1"></a>        <span class="co"># differently to the notation in Chapter 2 of the book.  Here,</span></span>
<span id="cb115-108"><a href="#cb115-108" aria-hidden="true" tabindex="-1"></a>        <span class="co"># l = 1 means the last layer of neurons, l = 2 is the</span></span>
<span id="cb115-109"><a href="#cb115-109" aria-hidden="true" tabindex="-1"></a>        <span class="co"># second-last layer, and so on.  It's a renumbering of the</span></span>
<span id="cb115-110"><a href="#cb115-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># scheme in the book, used here to take advantage of the fact</span></span>
<span id="cb115-111"><a href="#cb115-111" aria-hidden="true" tabindex="-1"></a>        <span class="co"># that Python can use negative indices in lists.</span></span>
<span id="cb115-112"><a href="#cb115-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="va">self</span>.num_layers):</span>
<span id="cb115-113"><a href="#cb115-113" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> zs[<span class="op">-</span>l]</span>
<span id="cb115-114"><a href="#cb115-114" aria-hidden="true" tabindex="-1"></a>            sp <span class="op">=</span> sigmoid_prime(z)</span>
<span id="cb115-115"><a href="#cb115-115" aria-hidden="true" tabindex="-1"></a>            delta <span class="op">=</span> np.dot(<span class="va">self</span>.weights[<span class="op">-</span>l<span class="op">+</span><span class="dv">1</span>].transpose(), delta) <span class="op">*</span> sp</span>
<span id="cb115-116"><a href="#cb115-116" aria-hidden="true" tabindex="-1"></a>            nabla_b[<span class="op">-</span>l] <span class="op">=</span> delta</span>
<span id="cb115-117"><a href="#cb115-117" aria-hidden="true" tabindex="-1"></a>            nielsen_bp[<span class="st">'delta'</span>].append(delta)</span>
<span id="cb115-118"><a href="#cb115-118" aria-hidden="true" tabindex="-1"></a>            nabla_w[<span class="op">-</span>l] <span class="op">=</span> np.dot(delta, activations[<span class="op">-</span>l<span class="op">-</span><span class="dv">1</span>].transpose())</span>
<span id="cb115-119"><a href="#cb115-119" aria-hidden="true" tabindex="-1"></a>            nielsen_bp[<span class="st">'w'</span>].append(nabla_w[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb115-120"><a href="#cb115-120" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (nabla_b, nabla_w)</span>
<span id="cb115-121"><a href="#cb115-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-122"><a href="#cb115-122" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> evaluate(<span class="va">self</span>, test_data):</span>
<span id="cb115-123"><a href="#cb115-123" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return the number of test inputs for which the neural</span></span>
<span id="cb115-124"><a href="#cb115-124" aria-hidden="true" tabindex="-1"></a><span class="co">        network outputs the correct result. Note that the neural</span></span>
<span id="cb115-125"><a href="#cb115-125" aria-hidden="true" tabindex="-1"></a><span class="co">        network's output is assumed to be the index of whichever</span></span>
<span id="cb115-126"><a href="#cb115-126" aria-hidden="true" tabindex="-1"></a><span class="co">        neuron in the final layer has the highest activation."""</span></span>
<span id="cb115-127"><a href="#cb115-127" aria-hidden="true" tabindex="-1"></a>        test_results <span class="op">=</span> [(np.argmax(<span class="va">self</span>.feedforward(x)), y)</span>
<span id="cb115-128"><a href="#cb115-128" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">for</span> (x, y) <span class="kw">in</span> test_data]</span>
<span id="cb115-129"><a href="#cb115-129" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">sum</span>(<span class="bu">int</span>(x <span class="op">==</span> y) <span class="cf">for</span> (x, y) <span class="kw">in</span> test_results)</span>
<span id="cb115-130"><a href="#cb115-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-131"><a href="#cb115-131" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> cost_derivative(<span class="va">self</span>, output_activations, y):</span>
<span id="cb115-132"><a href="#cb115-132" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return the vector of partial derivatives \partial C_x /</span></span>
<span id="cb115-133"><a href="#cb115-133" aria-hidden="true" tabindex="-1"></a><span class="co">        \partial a for the output activations."""</span></span>
<span id="cb115-134"><a href="#cb115-134" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (output_activations<span class="op">-</span>y)</span>
<span id="cb115-135"><a href="#cb115-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-136"><a href="#cb115-136" aria-hidden="true" tabindex="-1"></a><span class="co">#### Miscellaneous functions</span></span>
<span id="cb115-137"><a href="#cb115-137" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb115-138"><a href="#cb115-138" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""The sigmoid function."""</span></span>
<span id="cb115-139"><a href="#cb115-139" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span><span class="op">+</span>np.exp(<span class="op">-</span>z))</span>
<span id="cb115-140"><a href="#cb115-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-141"><a href="#cb115-141" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid_prime(z):</span>
<span id="cb115-142"><a href="#cb115-142" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Derivative of the sigmoid function."""</span></span>
<span id="cb115-143"><a href="#cb115-143" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sigmoid(z)<span class="op">*</span>(<span class="fl">1.0</span><span class="op">-</span>sigmoid(z))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>nielsen_w <span class="op">=</span> []</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>nielsen_b <span class="op">=</span> []</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>nielsen_bp <span class="op">=</span> {<span class="st">'x'</span>:[],<span class="st">'y'</span>:[],<span class="st">'act'</span>:[],<span class="st">'delta'</span>:[],<span class="st">'w'</span>:[]}</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> Network([<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>],weights<span class="op">=</span>[W_1_init, W_2_init, W_3_init],</span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>              biases<span class="op">=</span>[b_1_init, b_2_init, b_3_init])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>y_nielsen <span class="op">=</span> net.feedforward(X.T) </span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>_,act <span class="op">=</span> feedforward_batch(X,weights<span class="op">=</span>[W_1_init, W_2_init, W_3_init],</span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a>                           biases<span class="op">=</span>[b_1_init, b_2_init, b_3_init])</span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a>y_mine <span class="op">=</span> act[<span class="op">-</span><span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>y_nielsen[:,<span class="dv">1</span>:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="86">
<pre><code>array([[0.74194328, 0.74151133, 0.74129913, 0.74195282, 0.74186864,
        0.74195579, 0.74205958, 0.74252934, 0.74216293]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>y_mine[:,<span class="dv">1</span>:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>array([[0.74194328, 0.74151133, 0.74129913, 0.74195282, 0.74186864,
        0.74195579, 0.74205958, 0.74252934, 0.74216293]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>((y_nielsen <span class="op">-</span> y_mine)<span class="op">&lt;</span><span class="fl">0.001</span>).<span class="bu">all</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="88">
<pre><code>True</code></pre>
</div>
</div>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>np.array_equal(y_nielsen,y_mine)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="89">
<pre><code>True</code></pre>
</div>
</div>
<p>They are the same before training with he same weights, so the problem isnt in feedforward.</p>
<div class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>net.SGD([(lx.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),ly.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="cf">for</span> lx,ly <span class="kw">in</span> <span class="bu">zip</span>(X,y)], epochs,batch_size,eta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0 complete
Epoch 1 complete
Epoch 2 complete</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3 complete
Epoch 4 complete
Epoch 5 complete
Epoch 6 complete</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 7 complete</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 8 complete</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 complete</code></pre>
</div>
</div>
<div class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>net.weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="91">
<pre><code>[array([[0.04080823, 0.37448822],
        [0.15191249, 0.71274118],
        [0.59873365, 0.67791594]]),
 array([[0.32794289, 0.65351068, 0.1330368 ],
        [0.58658677, 0.03377164, 0.5889434 ],
        [0.45534099, 0.49025834, 0.24421904]]),
 array([[ 0.84503569, -0.30390145,  0.36283462]])]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="92">
<pre><code>[array([[0.04080823, 0.37448822],
        [0.15191249, 0.71274118],
        [0.59873365, 0.67791594]]),
 array([[0.32794289, 0.65351068, 0.1330368 ],
        [0.58658677, 0.03377164, 0.5889434 ],
        [0.45534099, 0.49025834, 0.24421904]]),
 array([[ 0.84503569, -0.30390145,  0.36283462]])]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>[np.array_equal(i,j) <span class="cf">for</span> i,j <span class="kw">in</span> <span class="bu">zip</span>(weights,net.weights)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="93">
<pre><code>[True, True, True]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>[np.array_equal(i,j) <span class="cf">for</span> i,j <span class="kw">in</span> <span class="bu">zip</span>(biases,net.biases)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="94">
<pre><code>[True, True, True]</code></pre>
</div>
</div>
</section>
<section id="fully-vectorized-implementation" class="level2">
<h2 class="anchored" data-anchor-id="fully-vectorized-implementation">Fully vectorized implementation</h2>
<p>Ok! We‚Äôve done a lot of hard work! Now we‚Äôve come to the final linear algebraic thing we need to do to make this fast üèéüí®.</p>
<p>We want to be able to backprop <strong>across batches</strong>, instead of across individual samples in a batch.</p>
<p>To do so, we will have to introduce <strong>3D matrices!</strong>. In math these are called <strong>tensors</strong>!</p>
<div class="info">
<p>üìñ <strong>Definition</strong>: For our purposes, a <strong>tensor</strong> is defined as a multidimensional matrix.</p>
<p><strong>Note:</strong> In python, these objects are called ‚ÄúN-D arrays‚Äù or just ‚Äúarrays‚Äù. Vectors, and matrices are just 1D and 2D versions of these tensors/arrays.</p>
</div>
<p>Lets create one right now!</p>
<div class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.random.randint(<span class="dv">1</span>,<span class="dv">10</span>,(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>))</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="95">
<pre><code>array([[[9, 7, 3],
        [5, 4, 2],
        [7, 7, 8]],

       [[2, 7, 1],
        [7, 3, 3],
        [7, 9, 3]],

       [[4, 6, 9],
        [5, 4, 2],
        [2, 8, 5]]])</code></pre>
</div>
</div>
<p><font size="20">ü§î</font></p>
<p>This doesn‚Äôt look that special. Thats just because were printing this 3D object on a 2D notebook. Pay special attention to the <code>[</code> and <code>]</code> characters and you‚Äôll notice its almost like its a list of a 2D matrices. Indeed, for right now, we can just think of it like that.</p>
<p>Lets grab the first matrix in the list:</p>
<div class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb142"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">0</span>,:,:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="96">
<pre><code>array([[9, 7, 3],
       [5, 4, 2],
       [7, 7, 8]])</code></pre>
</div>
</div>
<p>And the second:</p>
<div class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">1</span>,:,:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="97">
<pre><code>array([[2, 7, 1],
       [7, 3, 3],
       [7, 9, 3]])</code></pre>
</div>
</div>
<p>And the last:</p>
<div class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb146"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a>a[<span class="op">-</span><span class="dv">1</span>,:,:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="98">
<pre><code>array([[4, 6, 9],
       [5, 4, 2],
       [2, 8, 5]])</code></pre>
</div>
</div>
<p>Ok, so far so good. Lets understand one more thing we need to do with these objects.</p>
<p>We want to be able to take this list/stack of matrices, and multiply the stack with another vector matrix. In other words, if I have a tensor <code>T</code>, which contains four matrices which we will call <code>A</code>, <code>B</code>, <code>C</code>, and <code>D</code> I want to be able to do something like:</p>
<pre><code>T*v = R</code></pre>
<p>where <code>R</code> is now a stack which contains <code>Av</code>,<code>Bv</code>, <code>Cv</code>, <code>Dv</code>.</p>
<p>And then at the end, I can <strong>average across the stack dimension</strong>, to produce one final 2D matrix which is just:</p>
<pre><code>(Av+Bv+Cv+Dv)/4</code></pre>
<p>Ok. We‚Äôve described in words the procedure we want to perform, so lets go about coding it up! To make this clear, lets define our tensor T as we described above:</p>
<div class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb150"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.ones((<span class="dv">3</span>,<span class="dv">3</span>))</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a>A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="99">
<pre><code>array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb152"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.ones((<span class="dv">3</span>,<span class="dv">3</span>))<span class="op">*</span><span class="dv">2</span></span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>B</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="100">
<pre><code>array([[2., 2., 2.],
       [2., 2., 2.],
       [2., 2., 2.]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb154"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> np.ones((<span class="dv">3</span>,<span class="dv">3</span>))<span class="op">*</span><span class="dv">3</span></span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a>C</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="101">
<pre><code>array([[3., 3., 3.],
       [3., 3., 3.],
       [3., 3., 3.]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb156"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.ones((<span class="dv">3</span>,<span class="dv">3</span>))<span class="op">*</span><span class="dv">4</span></span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a>D</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="102">
<pre><code>array([[4., 4., 4.],
       [4., 4., 4.],
       [4., 4., 4.]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="103">
<div class="sourceCode cell-code" id="cb158"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.zeros((<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">3</span>))</span>
<span id="cb158-2"><a href="#cb158-2" aria-hidden="true" tabindex="-1"></a>T[<span class="dv">0</span>,:,:] <span class="op">=</span> A</span>
<span id="cb158-3"><a href="#cb158-3" aria-hidden="true" tabindex="-1"></a>T[<span class="dv">1</span>,:,:] <span class="op">=</span> B</span>
<span id="cb158-4"><a href="#cb158-4" aria-hidden="true" tabindex="-1"></a>T[<span class="dv">2</span>,:,:] <span class="op">=</span> C</span>
<span id="cb158-5"><a href="#cb158-5" aria-hidden="true" tabindex="-1"></a>T[<span class="dv">3</span>,:,:] <span class="op">=</span> D</span>
<span id="cb158-6"><a href="#cb158-6" aria-hidden="true" tabindex="-1"></a>T.shape,T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="103">
<pre><code>((4, 3, 3),
 array([[[1., 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.]],
 
        [[2., 2., 2.],
         [2., 2., 2.],
         [2., 2., 2.]],
 
        [[3., 3., 3.],
         [3., 3., 3.],
         [3., 3., 3.]],
 
        [[4., 4., 4.],
         [4., 4., 4.],
         [4., 4., 4.]]]))</code></pre>
</div>
</div>
<p>Note: we could have also used <code>np.stack</code> to do this for us:</p>
<div class="cell" data-execution_count="104">
<div class="sourceCode cell-code" id="cb160"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a>np.stack((A,B,C,D)).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="104">
<pre><code>(4, 3, 3)</code></pre>
</div>
</div>
<p>And now lets create that vector we wanted:</p>
<div class="cell" data-execution_count="105">
<div class="sourceCode cell-code" id="cb162"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.ones((<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb162-2"><a href="#cb162-2" aria-hidden="true" tabindex="-1"></a>v</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="105">
<pre><code>array([[1.],
       [1.],
       [1.]])</code></pre>
</div>
</div>
<p>and verify how it interacts with each matrix in our stack:</p>
<div class="cell" data-execution_count="106">
<div class="sourceCode cell-code" id="cb164"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a>A.dot(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="106">
<pre><code>array([[3.],
       [3.],
       [3.]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="107">
<div class="sourceCode cell-code" id="cb166"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a>B.dot(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="107">
<pre><code>array([[6.],
       [6.],
       [6.]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="108">
<div class="sourceCode cell-code" id="cb168"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a>C.dot(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="108">
<pre><code>array([[9.],
       [9.],
       [9.]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="109">
<div class="sourceCode cell-code" id="cb170"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a>D.dot(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="109">
<pre><code>array([[12.],
       [12.],
       [12.]])</code></pre>
</div>
</div>
<p>Ok! So now we know what we want <code>R</code> to look like. It should be the stack of those vectors.</p>
<p>We can create this using the <code>matmul</code> function, which stands for <strong>matrix multiply</strong>. We can check its documentation:</p>
<div class="cell" data-execution_count="110">
<div class="sourceCode cell-code" id="cb172"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="co"># np.matmul?</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The line that we are interested in is the following:</p>
<pre><code>If either argument is N-D, N &gt; 2, it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly.</code></pre>
<p>Thats exactly what we want! So lets use it!</p>
<div class="cell" data-execution_count="111">
<div class="sourceCode cell-code" id="cb174"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> np.matmul(T,v)</span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a>R</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="111">
<pre><code>array([[[ 3.],
        [ 3.],
        [ 3.]],

       [[ 6.],
        [ 6.],
        [ 6.]],

       [[ 9.],
        [ 9.],
        [ 9.]],

       [[12.],
        [12.],
        [12.]]])</code></pre>
</div>
</div>
<p>Perfect! Thats what we expected! Lets stop and realize this:</p>
<p>You can think of this one function as having handled the three separate multiplications for us! In one function! And it did so <strong>as fast as possible</strong> because its relying on the underlying numerical libraries!</p>
<p><strong>Hint:</strong> In a few cells from now, we will see this is exactly what we want to do for our batch!</p>
<p>The <code>matmul</code> function is also represented by the handy <code>@</code> symbol, meaning we could also just type:</p>
<div class="cell" data-execution_count="112">
<div class="sourceCode cell-code" id="cb176"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> T<span class="op">@</span>v</span>
<span id="cb176-2"><a href="#cb176-2" aria-hidden="true" tabindex="-1"></a>R</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="112">
<pre><code>array([[[ 3.],
        [ 3.],
        [ 3.]],

       [[ 6.],
        [ 6.],
        [ 6.]],

       [[ 9.],
        [ 9.],
        [ 9.]],

       [[12.],
        [12.],
        [12.]]])</code></pre>
</div>
</div>
<p>Note: <code>matmul/@</code> work as we expect in simpler cases:</p>
<div class="cell" data-execution_count="113">
<div class="sourceCode cell-code" id="cb178"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a>a,b <span class="op">=</span> np.random.randint(<span class="dv">1</span>,<span class="dv">10</span>,(<span class="dv">3</span>,<span class="dv">2</span>)),np.random.randint(<span class="dv">1</span>,<span class="dv">10</span>,(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb178-2"><a href="#cb178-2" aria-hidden="true" tabindex="-1"></a>a,b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="113">
<pre><code>(array([[3, 2],
        [7, 1],
        [2, 3]]),
 array([[6, 8, 5],
        [4, 4, 1]]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="114">
<div class="sourceCode cell-code" id="cb180"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a>a<span class="op">@</span>b, a.dot(b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="114">
<pre><code>(array([[26, 32, 17],
        [46, 60, 36],
        [24, 28, 13]]),
 array([[26, 32, 17],
        [46, 60, 36],
        [24, 28, 13]]))</code></pre>
</div>
</div>
<p>One final thing which might be useful, is the <code>einsum</code> function:</p>
<div class="cell" data-execution_count="115">
<div class="sourceCode cell-code" id="cb182"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb182-1"><a href="#cb182-1" aria-hidden="true" tabindex="-1"></a><span class="co"># np.einsum?</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This is a very powerful function, but we can use it in a straightforward manner to just describe to numpy the shapes of each of our matrices, and the <strong>shape we want</strong> afterwards!</p>
<p>Lets revisit the T/R example above</p>
<div class="cell" data-execution_count="116">
<div class="sourceCode cell-code" id="cb183"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a>np.einsum(<span class="st">'ijk,kl-&gt;ikl'</span>,T,v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="116">
<pre><code>array([[[ 3.],
        [ 3.],
        [ 3.]],

       [[ 6.],
        [ 6.],
        [ 6.]],

       [[ 9.],
        [ 9.],
        [ 9.]],

       [[12.],
        [12.],
        [12.]]])</code></pre>
</div>
</div>
<p><font size="20">ü§Ø</font></p>
<p>What?! What just happened! Well, we just described the operation we wanted directly in terms of sizes using the <strong>Einstein summation convention</strong>.</p>
<p>We said <code>T</code> is of size <code>i x j x k</code>, and that <code>v</code> is of size <code>k x l</code>, and I want to turn this into something that is of size <code>i x k x l</code>. And thats exactly what we got!</p>
<p>The last thing we want to do is ‚Äúcollapse‚Äù across one of our dimensions to form the average. This is easy in numpy, we just tell numpy across what <strong>axis</strong> we want to average:</p>
<div class="cell" data-execution_count="117">
<div class="sourceCode cell-code" id="cb185"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> T<span class="op">@</span>v</span>
<span id="cb185-2"><a href="#cb185-2" aria-hidden="true" tabindex="-1"></a>R</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="117">
<pre><code>array([[[ 3.],
        [ 3.],
        [ 3.]],

       [[ 6.],
        [ 6.],
        [ 6.]],

       [[ 9.],
        [ 9.],
        [ 9.]],

       [[12.],
        [12.],
        [12.]]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="118">
<div class="sourceCode cell-code" id="cb187"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a>R.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="118">
<pre><code>(4, 3, 1)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="119">
<div class="sourceCode cell-code" id="cb189"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a>np.mean(R,axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="119">
<pre><code>array([[7.5],
       [7.5],
       [7.5]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="120">
<div class="sourceCode cell-code" id="cb191"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a>np.mean(R,axis<span class="op">=</span><span class="dv">0</span>).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="120">
<pre><code>(3, 1)</code></pre>
</div>
</div>
<p>It worked! We told numpy to take the average across the first dimension (<code>axis=0</code>), so the result is the <code>3x1</code> matrix/vector which is just the average of all of the members of our stack!</p>
<hr>
<p>Lets point out one more potentially confusing aspect of our implementations. Lets take an activation matrix as our example. The tranpose of this matrix is of size <code>batch_size x nodes in our layer</code>.</p>
<p>We‚Äôve been using this 2D matrix to represent our collection of vectors, implicitly.</p>
<p>We could make this more explicit, by adding on the missing dimension, making it: <code>batch_size x nodes in our layer x 1</code> - giving us a clearer interpretation of this as a <strong>stack of vectors</strong>.</p>
<div class="cell" data-execution_count="121">
<div class="sourceCode cell-code" id="cb193"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.random.randint(<span class="dv">1</span>,<span class="dv">10</span>,(<span class="dv">10</span>,<span class="dv">3</span>))</span>
<span id="cb193-2"><a href="#cb193-2" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="121">
<pre><code>array([[9, 4, 3],
       [6, 1, 5],
       [4, 3, 6],
       [2, 5, 9],
       [1, 6, 2],
       [7, 7, 6],
       [4, 5, 4],
       [3, 6, 9],
       [6, 3, 2],
       [3, 1, 6]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="122">
<div class="sourceCode cell-code" id="cb195"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> a[:,:,np.newaxis]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, <code>a</code> is a <strong>stack of column vectors</strong>! Lets grab the first vector:</p>
<div class="cell" data-execution_count="123">
<div class="sourceCode cell-code" id="cb196"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb196-1"><a href="#cb196-1" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">0</span>,:,:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="123">
<pre><code>array([[9],
       [4],
       [3]])</code></pre>
</div>
</div>
<section id="implementation-6" class="level3">
<h3 class="anchored" data-anchor-id="implementation-6">Implementation</h3>
<p>This is the final piece of the puzzle we need. We are now ready to use this in our backprop implementation! Lets look at our old implementation:</p>
<div class="cell" data-execution_count="124">
<div class="sourceCode cell-code" id="cb198"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb198-1"><a href="#cb198-1" aria-hidden="true" tabindex="-1"></a>activities, activations <span class="op">=</span> feedforward_batch(X,weights,biases)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="125">
<div class="sourceCode cell-code" id="cb199"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a>weight_updates <span class="op">=</span> []</span>
<span id="cb199-2"><a href="#cb199-2" aria-hidden="true" tabindex="-1"></a>bias_updates <span class="op">=</span> []</span>
<span id="cb199-3"><a href="#cb199-3" aria-hidden="true" tabindex="-1"></a>layer_num <span class="op">=</span> <span class="bu">len</span>(activations)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="126">
<div class="sourceCode cell-code" id="cb200"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb200-1"><a href="#cb200-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> activity <span class="kw">in</span> activities:</span>
<span id="cb200-2"><a href="#cb200-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(activity.shape) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(3, 1000)
(3, 1000)
(1, 1000)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="127">
<div class="sourceCode cell-code" id="cb202"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb202-1"><a href="#cb202-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> activation <span class="kw">in</span> activations:</span>
<span id="cb202-2"><a href="#cb202-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(activation.shape) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(2, 1000)
(3, 1000)
(3, 1000)
(1, 1000)</code></pre>
</div>
</div>
<p>This is like having a <code>batch_size</code> of <span class="math inline">1000</span>. Lets continue! We want to move the batch size <strong>to the front</strong> so that its out of the way!</p>
<div class="cell" data-execution_count="128">
<div class="sourceCode cell-code" id="cb204"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb204-1"><a href="#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> activation <span class="kw">in</span> activations:</span>
<span id="cb204-2"><a href="#cb204-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(activation.T.shape) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(1000, 2)
(1000, 3)
(1000, 3)
(1000, 1)</code></pre>
</div>
</div>
<p>Its to these that we want to add our ‚Äúnew dimension‚Äù to clearly make these a stack:</p>
<div class="cell" data-execution_count="129">
<div class="sourceCode cell-code" id="cb206"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb206-1"><a href="#cb206-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> activation <span class="kw">in</span> activations:</span>
<span id="cb206-2"><a href="#cb206-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>((activation.T)[:,:,np.newaxis].shape) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(1000, 2, 1)
(1000, 3, 1)
(1000, 3, 1)
(1000, 1, 1)</code></pre>
</div>
</div>
<p>This is saying: I have a <code>batch_size</code>-many stack, composed of vectors, where each vector is ___ size.</p>
<p>Perfect! Lets get cooking!</p>
<div class="cell" data-execution_count="130">
<div class="sourceCode cell-code" id="cb208"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a><span class="co">########################################</span></span>
<span id="cb208-2"><a href="#cb208-2" aria-hidden="true" tabindex="-1"></a><span class="co">#           Last Level</span></span>
<span id="cb208-3"><a href="#cb208-3" aria-hidden="true" tabindex="-1"></a><span class="co">########################################</span></span>
<span id="cb208-4"><a href="#cb208-4" aria-hidden="true" tabindex="-1"></a><span class="co"># get last layer specific variables</span></span>
<span id="cb208-5"><a href="#cb208-5" aria-hidden="true" tabindex="-1"></a>z_L <span class="op">=</span> (activities[<span class="op">-</span><span class="dv">1</span>].T)[:,:,np.newaxis]</span>
<span id="cb208-6"><a href="#cb208-6" aria-hidden="true" tabindex="-1"></a>a_L <span class="op">=</span> (activations[<span class="op">-</span><span class="dv">1</span>].T)[:,:,np.newaxis]</span>
<span id="cb208-7"><a href="#cb208-7" aria-hidden="true" tabindex="-1"></a>a_L_minus_1 <span class="op">=</span> (activations[<span class="op">-</span><span class="dv">2</span>].T)[:,:,np.newaxis]</span>
<span id="cb208-8"><a href="#cb208-8" aria-hidden="true" tabindex="-1"></a>z_L.shape, a_L.shape,a_L_minus_1.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="130">
<pre><code>((1000, 1, 1), (1000, 1, 1), (1000, 3, 1))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="131">
<div class="sourceCode cell-code" id="cb210"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate delta_L for the last level</span></span>
<span id="cb210-2"><a href="#cb210-2" aria-hidden="true" tabindex="-1"></a>delta_L <span class="op">=</span> (a_L<span class="op">-</span>y[:,:,np.newaxis])<span class="op">*</span>sigmoid_prime(z_L)</span>
<span id="cb210-3"><a href="#cb210-3" aria-hidden="true" tabindex="-1"></a>delta_L.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="131">
<pre><code>(1000, 1, 1)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="132">
<div class="sourceCode cell-code" id="cb212"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb212-1"><a href="#cb212-1" aria-hidden="true" tabindex="-1"></a>deltas <span class="op">=</span> []</span>
<span id="cb212-2"><a href="#cb212-2" aria-hidden="true" tabindex="-1"></a>deltas.append(delta_L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, remember we need to collapse it down across the batch axis!</p>
<div class="cell" data-execution_count="133">
<div class="sourceCode cell-code" id="cb213"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb213-1"><a href="#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate bias update</span></span>
<span id="cb213-2"><a href="#cb213-2" aria-hidden="true" tabindex="-1"></a>dc_db_l <span class="op">=</span> delta_L.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb213-3"><a href="#cb213-3" aria-hidden="true" tabindex="-1"></a>bias_updates <span class="op">=</span> [dc_db_l] <span class="op">+</span> bias_updates</span>
<span id="cb213-4"><a href="#cb213-4" aria-hidden="true" tabindex="-1"></a>dc_db_l.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="133">
<pre><code>(1, 1)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="134">
<div class="sourceCode cell-code" id="cb215"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb215-1"><a href="#cb215-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calcualte weight updates</span></span>
<span id="cb215-2"><a href="#cb215-2" aria-hidden="true" tabindex="-1"></a>dc_dw_l <span class="op">=</span> (delta_L <span class="op">@</span> a_L_minus_1.transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb215-3"><a href="#cb215-3" aria-hidden="true" tabindex="-1"></a>weight_updates <span class="op">=</span> [dc_dw_l] <span class="op">+</span> weight_updates</span>
<span id="cb215-4"><a href="#cb215-4" aria-hidden="true" tabindex="-1"></a>dc_dw_l.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="134">
<pre><code>(1, 3)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="135">
<div class="sourceCode cell-code" id="cb217"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb217-1"><a href="#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="co">########################################</span></span>
<span id="cb217-2"><a href="#cb217-2" aria-hidden="true" tabindex="-1"></a><span class="co"># loop through each layer, from 2 to L-1</span></span>
<span id="cb217-3"><a href="#cb217-3" aria-hidden="true" tabindex="-1"></a><span class="co">######################################## </span></span>
<span id="cb217-4"><a href="#cb217-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,layer_num):</span>
<span id="cb217-5"><a href="#cb217-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># using level as a **negative index**</span></span>
<span id="cb217-6"><a href="#cb217-6" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> <span class="op">-</span>layer</span>
<span id="cb217-7"><a href="#cb217-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb217-8"><a href="#cb217-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># uncomment this print statement </span></span>
<span id="cb217-9"><a href="#cb217-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to help you understand how negative indexing works</span></span>
<span id="cb217-10"><a href="#cb217-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Loop variable: </span><span class="sc">{</span>layer<span class="sc">}</span><span class="ss">, l=</span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">, corresponding to layer: l</span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb217-11"><a href="#cb217-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb217-12"><a href="#cb217-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get layer values</span></span>
<span id="cb217-13"><a href="#cb217-13" aria-hidden="true" tabindex="-1"></a>    z_l <span class="op">=</span> (activities[l].T)[:,:,np.newaxis]</span>
<span id="cb217-14"><a href="#cb217-14" aria-hidden="true" tabindex="-1"></a>    a_l <span class="op">=</span> (activations[l].T)[:,:,np.newaxis]</span>
<span id="cb217-15"><a href="#cb217-15" aria-hidden="true" tabindex="-1"></a>    a_l_minus_1 <span class="op">=</span> (activations[l<span class="op">-</span><span class="dv">1</span>].T)[:,:,np.newaxis]</span>
<span id="cb217-16"><a href="#cb217-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb217-17"><a href="#cb217-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate delta_l for each layer</span></span>
<span id="cb217-18"><a href="#cb217-18" aria-hidden="true" tabindex="-1"></a>    delta_l <span class="op">=</span> (weights[l<span class="op">+</span><span class="dv">1</span>].T <span class="op">@</span> deltas[l<span class="op">+</span><span class="dv">1</span>]) <span class="op">*</span> sigmoid_prime(z_l)</span>
<span id="cb217-19"><a href="#cb217-19" aria-hidden="true" tabindex="-1"></a>    deltas <span class="op">=</span> [delta_l] <span class="op">+</span> deltas</span>
<span id="cb217-20"><a href="#cb217-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb217-21"><a href="#cb217-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate bias update</span></span>
<span id="cb217-22"><a href="#cb217-22" aria-hidden="true" tabindex="-1"></a>    dc_db_l <span class="op">=</span> delta_l.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb217-23"><a href="#cb217-23" aria-hidden="true" tabindex="-1"></a>    bias_updates <span class="op">=</span> [dc_db_l] <span class="op">+</span> bias_updates</span>
<span id="cb217-24"><a href="#cb217-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb217-25"><a href="#cb217-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calcualte weight updates</span></span>
<span id="cb217-26"><a href="#cb217-26" aria-hidden="true" tabindex="-1"></a>    dc_dw_l <span class="op">=</span> (delta_l <span class="op">@</span> a_l_minus_1.transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb217-27"><a href="#cb217-27" aria-hidden="true" tabindex="-1"></a>    weight_updates <span class="op">=</span> [dc_dw_l] <span class="op">+</span> weight_updates</span>
<span id="cb217-28"><a href="#cb217-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb217-29"><a href="#cb217-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'=========Layer:</span><span class="sc">{</span>layer_num<span class="op">+</span>l<span class="sc">}</span><span class="ss">========='</span>)</span>
<span id="cb217-30"><a href="#cb217-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Using negative index: </span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb217-31"><a href="#cb217-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'z_l:</span><span class="sc">{</span>z_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l:</span><span class="sc">{</span>a_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l_minus_1:</span><span class="sc">{</span>a_l_minus_1<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb217-32"><a href="#cb217-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'delta_l:</span><span class="sc">{</span>delta_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, dc_db_l:</span><span class="sc">{</span>dc_db_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, dc_dw_l:</span><span class="sc">{</span>dc_dw_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb217-33"><a href="#cb217-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loop variable: 2, l=-2, corresponding to layer: l-2
=========Layer:2=========
Using negative index: -2
z_l:(1000, 3, 1), a_l:(1000, 3, 1), a_l_minus_1:(1000, 3, 1)
delta_l:(1000, 3, 1), dc_db_l:(3, 1), dc_dw_l:(3, 3)

Loop variable: 3, l=-3, corresponding to layer: l-3
=========Layer:1=========
Using negative index: -3
z_l:(1000, 3, 1), a_l:(1000, 3, 1), a_l_minus_1:(1000, 2, 1)
delta_l:(1000, 3, 1), dc_db_l:(3, 1), dc_dw_l:(3, 2)
</code></pre>
</div>
</div>
<p>Wow! That was easy! We backproped across <strong>every single example in our batch!</strong></p>
<p>We‚Äôre done! Lets define a new function that does the above:</p>
<div class="cell" data-execution_count="136">
<div class="sourceCode cell-code" id="cb219"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb219-1"><a href="#cb219-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backprop_batch(activities,activations,y_batch,quiet<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb219-2"><a href="#cb219-2" aria-hidden="true" tabindex="-1"></a>    weight_updates <span class="op">=</span> []</span>
<span id="cb219-3"><a href="#cb219-3" aria-hidden="true" tabindex="-1"></a>    bias_updates <span class="op">=</span> []</span>
<span id="cb219-4"><a href="#cb219-4" aria-hidden="true" tabindex="-1"></a>    layer_num <span class="op">=</span> <span class="bu">len</span>(activations)</span>
<span id="cb219-5"><a href="#cb219-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb219-6"><a href="#cb219-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb219-7"><a href="#cb219-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">#           Last Level</span></span>
<span id="cb219-8"><a href="#cb219-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb219-9"><a href="#cb219-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get last layer specific variables</span></span>
<span id="cb219-10"><a href="#cb219-10" aria-hidden="true" tabindex="-1"></a>    z_L <span class="op">=</span> (activities[<span class="op">-</span><span class="dv">1</span>].T)[:,:,np.newaxis]</span>
<span id="cb219-11"><a href="#cb219-11" aria-hidden="true" tabindex="-1"></a>    a_L <span class="op">=</span> (activations[<span class="op">-</span><span class="dv">1</span>].T)[:,:,np.newaxis]</span>
<span id="cb219-12"><a href="#cb219-12" aria-hidden="true" tabindex="-1"></a>    a_L_minus_1 <span class="op">=</span> (activations[<span class="op">-</span><span class="dv">2</span>].T)[:,:,np.newaxis]</span>
<span id="cb219-13"><a href="#cb219-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb219-14"><a href="#cb219-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate delta_L for the last level</span></span>
<span id="cb219-15"><a href="#cb219-15" aria-hidden="true" tabindex="-1"></a>    delta_L <span class="op">=</span> (a_L<span class="op">-</span>y_batch[:,:,np.newaxis])<span class="op">*</span>sigmoid_prime(z_L)</span>
<span id="cb219-16"><a href="#cb219-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-17"><a href="#cb219-17" aria-hidden="true" tabindex="-1"></a>    deltas <span class="op">=</span> []</span>
<span id="cb219-18"><a href="#cb219-18" aria-hidden="true" tabindex="-1"></a>    deltas.append(delta_L)</span>
<span id="cb219-19"><a href="#cb219-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb219-20"><a href="#cb219-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate bias update</span></span>
<span id="cb219-21"><a href="#cb219-21" aria-hidden="true" tabindex="-1"></a>    dc_db_L <span class="op">=</span> delta_L.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb219-22"><a href="#cb219-22" aria-hidden="true" tabindex="-1"></a>    bias_updates <span class="op">=</span> [dc_db_L] <span class="op">+</span> bias_updates</span>
<span id="cb219-23"><a href="#cb219-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb219-24"><a href="#cb219-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calcualte weight updates</span></span>
<span id="cb219-25"><a href="#cb219-25" aria-hidden="true" tabindex="-1"></a>    dc_dw_L <span class="op">=</span> (delta_L <span class="op">@</span> a_L_minus_1.transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb219-26"><a href="#cb219-26" aria-hidden="true" tabindex="-1"></a>    weight_updates <span class="op">=</span> [dc_dw_L] <span class="op">+</span> weight_updates</span>
<span id="cb219-27"><a href="#cb219-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb219-28"><a href="#cb219-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-29"><a href="#cb219-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">########################################</span></span>
<span id="cb219-30"><a href="#cb219-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loop through each layer, from 2 to L-1</span></span>
<span id="cb219-31"><a href="#cb219-31" aria-hidden="true" tabindex="-1"></a>    <span class="co">######################################## </span></span>
<span id="cb219-32"><a href="#cb219-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,layer_num):</span>
<span id="cb219-33"><a href="#cb219-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># using level as a **negative index**</span></span>
<span id="cb219-34"><a href="#cb219-34" aria-hidden="true" tabindex="-1"></a>        l <span class="op">=</span> <span class="op">-</span>layer</span>
<span id="cb219-35"><a href="#cb219-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-36"><a href="#cb219-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># uncomment this print statement </span></span>
<span id="cb219-37"><a href="#cb219-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to help you understand how negative indexing works</span></span>
<span id="cb219-38"><a href="#cb219-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> quiet: <span class="bu">print</span>(<span class="ss">f"Loop variable: </span><span class="sc">{</span>layer<span class="sc">}</span><span class="ss">, l=</span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">, corresponding to layer: l</span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb219-39"><a href="#cb219-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-40"><a href="#cb219-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get layer values</span></span>
<span id="cb219-41"><a href="#cb219-41" aria-hidden="true" tabindex="-1"></a>        z_l <span class="op">=</span> (activities[l].T)[:,:,np.newaxis]</span>
<span id="cb219-42"><a href="#cb219-42" aria-hidden="true" tabindex="-1"></a>        a_l <span class="op">=</span> (activations[l].T)[:,:,np.newaxis]</span>
<span id="cb219-43"><a href="#cb219-43" aria-hidden="true" tabindex="-1"></a>        a_l_minus_1 <span class="op">=</span> (activations[l<span class="op">-</span><span class="dv">1</span>].T)[:,:,np.newaxis]</span>
<span id="cb219-44"><a href="#cb219-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-45"><a href="#cb219-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate delta_l for each layer</span></span>
<span id="cb219-46"><a href="#cb219-46" aria-hidden="true" tabindex="-1"></a>        delta_l <span class="op">=</span> (weights[l<span class="op">+</span><span class="dv">1</span>].T <span class="op">@</span> deltas[l<span class="op">+</span><span class="dv">1</span>]) <span class="op">*</span> sigmoid_prime(z_l)</span>
<span id="cb219-47"><a href="#cb219-47" aria-hidden="true" tabindex="-1"></a>        deltas <span class="op">=</span> [delta_l] <span class="op">+</span> deltas</span>
<span id="cb219-48"><a href="#cb219-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-49"><a href="#cb219-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate bias update</span></span>
<span id="cb219-50"><a href="#cb219-50" aria-hidden="true" tabindex="-1"></a>        dc_db_l <span class="op">=</span> delta_l.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb219-51"><a href="#cb219-51" aria-hidden="true" tabindex="-1"></a>        bias_updates <span class="op">=</span> [dc_db_l] <span class="op">+</span> bias_updates</span>
<span id="cb219-52"><a href="#cb219-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-53"><a href="#cb219-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calcualte weight updates</span></span>
<span id="cb219-54"><a href="#cb219-54" aria-hidden="true" tabindex="-1"></a>        dc_dw_l <span class="op">=</span> (delta_l <span class="op">@</span> a_l_minus_1.transpose(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb219-55"><a href="#cb219-55" aria-hidden="true" tabindex="-1"></a>        weight_updates <span class="op">=</span> [dc_dw_l] <span class="op">+</span> weight_updates</span>
<span id="cb219-56"><a href="#cb219-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb219-57"><a href="#cb219-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> quiet:</span>
<span id="cb219-58"><a href="#cb219-58" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'=========Layer:</span><span class="sc">{</span>layer_num<span class="op">+</span>l<span class="sc">}</span><span class="ss">========='</span>)</span>
<span id="cb219-59"><a href="#cb219-59" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'Using negative index: </span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb219-60"><a href="#cb219-60" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'z_l:</span><span class="sc">{</span>z_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l:</span><span class="sc">{</span>a_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, a_l_minus_1:</span><span class="sc">{</span>a_l_minus_1<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb219-61"><a href="#cb219-61" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'delta_l:</span><span class="sc">{</span>delta_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, dc_db_l:</span><span class="sc">{</span>dc_db_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, dc_dw_l:</span><span class="sc">{</span>dc_dw_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb219-62"><a href="#cb219-62" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>()</span>
<span id="cb219-63"><a href="#cb219-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb219-64"><a href="#cb219-64" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weight_updates,bias_updates</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And now lets use it!</p>
<div class="cell" data-execution_count="137">
<div class="sourceCode cell-code" id="cb220"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb220-1"><a href="#cb220-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset</span></span>
<span id="cb220-2"><a href="#cb220-2" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb220-3"><a href="#cb220-3" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb220-4"><a href="#cb220-4" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb220-5"><a href="#cb220-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb220-6"><a href="#cb220-6" aria-hidden="true" tabindex="-1"></a>eta<span class="op">=</span><span class="fl">1e-2</span></span>
<span id="cb220-7"><a href="#cb220-7" aria-hidden="true" tabindex="-1"></a>dataset<span class="op">=</span>X</span>
<span id="cb220-8"><a href="#cb220-8" aria-hidden="true" tabindex="-1"></a>labels<span class="op">=</span>y</span>
<span id="cb220-9"><a href="#cb220-9" aria-hidden="true" tabindex="-1"></a>quiet<span class="op">=</span><span class="va">False</span></span>
<span id="cb220-10"><a href="#cb220-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-11"><a href="#cb220-11" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient checking</span></span>
<span id="cb220-12"><a href="#cb220-12" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb220-13"><a href="#cb220-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-14"><a href="#cb220-14" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weight matrices - notice the dimensions</span></span>
<span id="cb220-15"><a href="#cb220-15" aria-hidden="true" tabindex="-1"></a>W_1_init <span class="op">=</span> np.random.rand(<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb220-16"><a href="#cb220-16" aria-hidden="true" tabindex="-1"></a>W_2_init <span class="op">=</span> np.random.rand(<span class="dv">3</span>,<span class="dv">3</span>)</span>
<span id="cb220-17"><a href="#cb220-17" aria-hidden="true" tabindex="-1"></a>W_3_init <span class="op">=</span> np.random.randn(<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb220-18"><a href="#cb220-18" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> [W_1_init,W_2_init,W_3_init]</span>
<span id="cb220-19"><a href="#cb220-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-20"><a href="#cb220-20" aria-hidden="true" tabindex="-1"></a><span class="co"># and our biases</span></span>
<span id="cb220-21"><a href="#cb220-21" aria-hidden="true" tabindex="-1"></a>b_1_init <span class="op">=</span> np.random.rand(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb220-22"><a href="#cb220-22" aria-hidden="true" tabindex="-1"></a>b_2_init <span class="op">=</span> np.random.rand(<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb220-23"><a href="#cb220-23" aria-hidden="true" tabindex="-1"></a>b_3_init <span class="op">=</span> np.random.rand(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb220-24"><a href="#cb220-24" aria-hidden="true" tabindex="-1"></a>biases <span class="op">=</span> [b_1_init,b_2_init,b_3_init]</span>
<span id="cb220-25"><a href="#cb220-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-26"><a href="#cb220-26" aria-hidden="true" tabindex="-1"></a><span class="co"># network</span></span>
<span id="cb220-27"><a href="#cb220-27" aria-hidden="true" tabindex="-1"></a>W_1,W_2,W_3 <span class="op">=</span> weights[<span class="dv">0</span>],weights[<span class="dv">1</span>],weights[<span class="dv">2</span>]</span>
<span id="cb220-28"><a href="#cb220-28" aria-hidden="true" tabindex="-1"></a>b_1,b_2,b_3 <span class="op">=</span> biases[<span class="dv">0</span>],biases[<span class="dv">1</span>],biases[<span class="dv">2</span>]</span>
<span id="cb220-29"><a href="#cb220-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-30"><a href="#cb220-30" aria-hidden="true" tabindex="-1"></a><span class="co"># mini-batch params </span></span>
<span id="cb220-31"><a href="#cb220-31" aria-hidden="true" tabindex="-1"></a>num_batches <span class="op">=</span> <span class="bu">int</span>(N<span class="op">/</span>batch_size)</span>
<span id="cb220-32"><a href="#cb220-32" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb220-33"><a href="#cb220-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-34"><a href="#cb220-34" aria-hidden="true" tabindex="-1"></a><span class="co"># various debugging lists</span></span>
<span id="cb220-35"><a href="#cb220-35" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> []</span>
<span id="cb220-36"><a href="#cb220-36" aria-hidden="true" tabindex="-1"></a>grad_norm_1 <span class="op">=</span> []</span>
<span id="cb220-37"><a href="#cb220-37" aria-hidden="true" tabindex="-1"></a>grad_norm_2 <span class="op">=</span> []</span>
<span id="cb220-38"><a href="#cb220-38" aria-hidden="true" tabindex="-1"></a>grad_norm_3 <span class="op">=</span> []</span>
<span id="cb220-39"><a href="#cb220-39" aria-hidden="true" tabindex="-1"></a>norm_1 <span class="op">=</span> []</span>
<span id="cb220-40"><a href="#cb220-40" aria-hidden="true" tabindex="-1"></a>norm_2 <span class="op">=</span> []</span>
<span id="cb220-41"><a href="#cb220-41" aria-hidden="true" tabindex="-1"></a>norm_3 <span class="op">=</span> []</span>
<span id="cb220-42"><a href="#cb220-42" aria-hidden="true" tabindex="-1"></a>grad_errors <span class="op">=</span> []</span>
<span id="cb220-43"><a href="#cb220-43" aria-hidden="true" tabindex="-1"></a>diff_vecs <span class="op">=</span> []</span>
<span id="cb220-44"><a href="#cb220-44" aria-hidden="true" tabindex="-1"></a>grad_vecs <span class="op">=</span> []</span>
<span id="cb220-45"><a href="#cb220-45" aria-hidden="true" tabindex="-1"></a>my_act <span class="op">=</span> []</span>
<span id="cb220-46"><a href="#cb220-46" aria-hidden="true" tabindex="-1"></a>my_w <span class="op">=</span> []</span>
<span id="cb220-47"><a href="#cb220-47" aria-hidden="true" tabindex="-1"></a>my_b <span class="op">=</span> []</span>
<span id="cb220-48"><a href="#cb220-48" aria-hidden="true" tabindex="-1"></a>my_bp <span class="op">=</span> {<span class="st">'x'</span>:[],<span class="st">'y'</span>:[],<span class="st">'act'</span>:[],<span class="st">'delta'</span>:[],<span class="st">'w'</span>:[]}</span>
<span id="cb220-49"><a href="#cb220-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-50"><a href="#cb220-50" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb220-51"><a href="#cb220-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># shuffle the dataset - note we do this by shuffling indices</span></span>
<span id="cb220-52"><a href="#cb220-52" aria-hidden="true" tabindex="-1"></a><span class="co">#     indices = np.random.permutation(N)</span></span>
<span id="cb220-53"><a href="#cb220-53" aria-hidden="true" tabindex="-1"></a><span class="co">#     shuffled_dataset = dataset[indices,:]</span></span>
<span id="cb220-54"><a href="#cb220-54" aria-hidden="true" tabindex="-1"></a><span class="co">#     shuffled_labels = labels[indices,:]</span></span>
<span id="cb220-55"><a href="#cb220-55" aria-hidden="true" tabindex="-1"></a>    shuffled_dataset <span class="op">=</span> dataset</span>
<span id="cb220-56"><a href="#cb220-56" aria-hidden="true" tabindex="-1"></a>    shuffled_labels <span class="op">=</span> labels</span>
<span id="cb220-57"><a href="#cb220-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb220-58"><a href="#cb220-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># work on current batch </span></span>
<span id="cb220-59"><a href="#cb220-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_num <span class="kw">in</span> <span class="bu">range</span>(num_batches):</span>
<span id="cb220-60"><a href="#cb220-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-61"><a href="#cb220-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get current batch</span></span>
<span id="cb220-62"><a href="#cb220-62" aria-hidden="true" tabindex="-1"></a>        batch_start <span class="op">=</span> batch_num<span class="op">*</span>batch_size</span>
<span id="cb220-63"><a href="#cb220-63" aria-hidden="true" tabindex="-1"></a>        batch_end <span class="op">=</span> (batch_num<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>batch_size</span>
<span id="cb220-64"><a href="#cb220-64" aria-hidden="true" tabindex="-1"></a>        batch <span class="op">=</span> shuffled_dataset[batch_start:batch_end,:]</span>
<span id="cb220-65"><a href="#cb220-65" aria-hidden="true" tabindex="-1"></a>        batch_labels <span class="op">=</span> shuffled_labels[batch_start:batch_end,:].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb220-66"><a href="#cb220-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-67"><a href="#cb220-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># feedforward on batch</span></span>
<span id="cb220-68"><a href="#cb220-68" aria-hidden="true" tabindex="-1"></a>        activities, activations <span class="op">=</span> feedforward_batch(batch,weights,biases)</span>
<span id="cb220-69"><a href="#cb220-69" aria-hidden="true" tabindex="-1"></a>        my_act.append(activations[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb220-70"><a href="#cb220-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-71"><a href="#cb220-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backprop on batch</span></span>
<span id="cb220-72"><a href="#cb220-72" aria-hidden="true" tabindex="-1"></a>        weight_grads,bias_grads <span class="op">=</span> backprop_batch(activities,activations,batch_labels)</span>
<span id="cb220-73"><a href="#cb220-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-74"><a href="#cb220-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># take your steps:</span></span>
<span id="cb220-75"><a href="#cb220-75" aria-hidden="true" tabindex="-1"></a>        W_1 <span class="op">=</span> W_1 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(weight_grads[<span class="dv">0</span>])</span>
<span id="cb220-76"><a href="#cb220-76" aria-hidden="true" tabindex="-1"></a>        b_1 <span class="op">=</span> b_1 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(bias_grads[<span class="dv">0</span>])</span>
<span id="cb220-77"><a href="#cb220-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-78"><a href="#cb220-78" aria-hidden="true" tabindex="-1"></a>        W_2 <span class="op">=</span> W_2 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(weight_grads[<span class="dv">1</span>])</span>
<span id="cb220-79"><a href="#cb220-79" aria-hidden="true" tabindex="-1"></a>        b_2 <span class="op">=</span> b_2 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(bias_grads[<span class="dv">1</span>])</span>
<span id="cb220-80"><a href="#cb220-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-81"><a href="#cb220-81" aria-hidden="true" tabindex="-1"></a>        W_3 <span class="op">=</span> W_3 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(weight_grads[<span class="dv">2</span>])</span>
<span id="cb220-82"><a href="#cb220-82" aria-hidden="true" tabindex="-1"></a>        b_3 <span class="op">=</span> b_3 <span class="op">-</span> eta<span class="op">/</span>batch_size<span class="op">*</span>(bias_grads[<span class="dv">2</span>])</span>
<span id="cb220-83"><a href="#cb220-83" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb220-84"><a href="#cb220-84" aria-hidden="true" tabindex="-1"></a>        norm_1.append(np.linalg.norm(W_1))</span>
<span id="cb220-85"><a href="#cb220-85" aria-hidden="true" tabindex="-1"></a>        norm_2.append(np.linalg.norm(W_2))</span>
<span id="cb220-86"><a href="#cb220-86" aria-hidden="true" tabindex="-1"></a>        norm_3.append(np.linalg.norm(W_3))</span>
<span id="cb220-87"><a href="#cb220-87" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb220-88"><a href="#cb220-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># save new weights and biases to be used in the next feedforward step</span></span>
<span id="cb220-89"><a href="#cb220-89" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> [W_1,W_2,W_3]</span>
<span id="cb220-90"><a href="#cb220-90" aria-hidden="true" tabindex="-1"></a>        biases <span class="op">=</span> [b_1,b_2,b_3]</span>
<span id="cb220-91"><a href="#cb220-91" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb220-92"><a href="#cb220-92" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb220-93"><a href="#cb220-93" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb220-94"><a href="#cb220-94" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch:</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch:1/10
Epoch:2/10
Epoch:3/10
Epoch:4/10
Epoch:5/10
Epoch:6/10
Epoch:7/10
Epoch:8/10
Epoch:9/10
Epoch:10/10</code></pre>
</div>
</div>
<div class="cell" data-execution_count="138">
<div class="sourceCode cell-code" id="cb222"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb222-1"><a href="#cb222-1" aria-hidden="true" tabindex="-1"></a>weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="138">
<pre><code>[array([[0.1081274 , 0.47951698],
        [0.60154837, 0.12693014],
        [0.87717778, 0.2176614 ]]),
 array([[0.94690038, 0.02751412, 0.60631782],
        [0.22879508, 0.52390474, 0.27270812],
        [0.43900256, 0.60783846, 0.48106349]]),
 array([[ 1.11310853,  1.01175885, -1.22714557]])]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="139">
<div class="sourceCode cell-code" id="cb224"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb224-1"><a href="#cb224-1" aria-hidden="true" tabindex="-1"></a>nielsen_w <span class="op">=</span> []</span>
<span id="cb224-2"><a href="#cb224-2" aria-hidden="true" tabindex="-1"></a>nielsen_b <span class="op">=</span> []</span>
<span id="cb224-3"><a href="#cb224-3" aria-hidden="true" tabindex="-1"></a>nielsen_bp <span class="op">=</span> {<span class="st">'x'</span>:[],<span class="st">'y'</span>:[],<span class="st">'act'</span>:[],<span class="st">'delta'</span>:[],<span class="st">'w'</span>:[]}</span>
<span id="cb224-4"><a href="#cb224-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb224-5"><a href="#cb224-5" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> Network([<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>],weights<span class="op">=</span>[W_1_init, W_2_init, W_3_init],</span>
<span id="cb224-6"><a href="#cb224-6" aria-hidden="true" tabindex="-1"></a>              biases<span class="op">=</span>[b_1_init, b_2_init, b_3_init])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="140">
<div class="sourceCode cell-code" id="cb225"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb225-1"><a href="#cb225-1" aria-hidden="true" tabindex="-1"></a>net.SGD([(lx.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),ly.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="cf">for</span> lx,ly <span class="kw">in</span> <span class="bu">zip</span>(X,y)], epochs,batch_size,eta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0 complete
Epoch 1 complete
Epoch 2 complete</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3 complete
Epoch 4 complete
Epoch 5 complete</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 6 complete
Epoch 7 complete
Epoch 8 complete
Epoch 9 complete</code></pre>
</div>
</div>
<div class="cell" data-execution_count="141">
<div class="sourceCode cell-code" id="cb229"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb229-1"><a href="#cb229-1" aria-hidden="true" tabindex="-1"></a>net.weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="141">
<pre><code>[array([[0.1081274 , 0.47951698],
        [0.60154837, 0.12693014],
        [0.87717778, 0.2176614 ]]),
 array([[0.94690038, 0.02751412, 0.60631782],
        [0.22879508, 0.52390474, 0.27270812],
        [0.43900256, 0.60783846, 0.48106349]]),
 array([[ 1.11310853,  1.01175885, -1.22714557]])]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="142">
<div class="sourceCode cell-code" id="cb231"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb231-1"><a href="#cb231-1" aria-hidden="true" tabindex="-1"></a>weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="142">
<pre><code>[array([[0.1081274 , 0.47951698],
        [0.60154837, 0.12693014],
        [0.87717778, 0.2176614 ]]),
 array([[0.94690038, 0.02751412, 0.60631782],
        [0.22879508, 0.52390474, 0.27270812],
        [0.43900256, 0.60783846, 0.48106349]]),
 array([[ 1.11310853,  1.01175885, -1.22714557]])]</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="additional-resources" class="level1">
<h1>Additional Resources</h1>
<p>This implementation follows the details of: * http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/ * https://www.youtube.com/watch?v=GlcnxUlrtek&amp;list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU&amp;index=4&amp;pbjreload=101 * https://github.com/stephencwelch/Neural-Networks-Demystified/blob/master/Part%202%20Forward%20Propagation.ipynb * https://www.youtube.com/watch?v=7bLEWDZng_M&amp;list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&amp;index=33 * https://www.youtube.com/watch?v=gl3lfL-g5mA * https://news.ycombinator.com/item?id=15782156 * https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>